{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdwYpZKN5b5GEM8bEZx6Kr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathydNU/gen-ai-coursework/blob/main/gan/Exploring_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring GAN\n",
        "\n",
        "In this notebook, we will be building a GAN model that includes the following:\n",
        "- A CNN for the discriminator\n",
        "- A CNN with transposed convolution layers for the generator\n",
        "\n",
        "We will be training the model on the CIFAR-10 dataset to generate images from a arbitary noise vector.\n",
        "\n",
        "The final task would to try and understand if we are able to observe any patterns in the output image change based on changes in the input noise vector. This will sort of give us some guidance towards which pattern in the input vector contributed to forming an image that we see as the output.\n",
        "\n",
        "## Pre requisites\n",
        "\n",
        "As a precursor to understanding GAN, it will be benifitial to understand the concepts involved in building Convolutional Neural Networks as we will be making use of CNNs in building the GAN model. Go throuhg [this notebook](https://github.com/parthasarathydNU/gen-ai-coursework/blob/main/cnn-intro/building_cnn.ipynb) to understand the fundamentals of building CNNs. It will give you a good reference point to start working with this notebook.\n",
        "\n",
        "## What's GAN ?\n",
        "\n",
        "![Gan overview](https://geekflare.com/wp-content/uploads/2022/08/ganworks.png)\n",
        "\n",
        "Generative Adversarial Networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks (Generator and Discriminator) contesting with each other in a zero-sum game framework. This method was introduced by Ian Goodfellow and his colleagues in 2014 and has since been applied to various applications, especially in the generation of visual media. Here’s a breakdown of how GANs function:\n",
        "\n",
        "### Components of a GAN\n",
        "1. **Generator**: This component of the GAN starts with a random noise vector and tries to generate data (like images) that resemble the real data. The goal of the generator is to produce fake data that are indistinguishable from actual data.\n",
        "   \n",
        "2. **Discriminator**: This network’s task is to distinguish between the real data and the fake data produced by the generator. It evaluates each input (real and fake) and attempts to determine if it is genuine or counterfeit.\n",
        "\n",
        "### How GANs Work\n",
        "- **Training Process**: During training, both networks are trained simultaneously. The generator learns to produce more and more realistic data while the discriminator gets better at telling real from fake. This training involves backpropagation and an optimization algorithm, typically a form of gradient descent.\n",
        "- **Adversarial Relationship**: The networks are trained in an adversarial manner. As the generator improves at producing realistic images, the discriminator's job becomes progressively more challenging. This pushes both networks to improve continuously until the generator produces near-perfect renditions of realistic data.\n",
        "\n",
        "### Key Phases in Training\n",
        "1. **Discriminator Update**: Feed the discriminator real data and fake data from the generator. Adjust the discriminator’s weights to minimize its classification errors—the discriminator learns to correctly label real and fake data.\n",
        "   \n",
        "2. **Generator Update**: Adjust the generator’s weights to make the output look more real so that the discriminator is more likely to classify fake data as real. This step uses the gradient from the discriminator's classification to improve the generator.\n",
        "\n",
        "### Unique Features\n",
        "- **Unsupervised Learning**: GANs do not require labeled data (although they can be adapted for semi-supervised tasks), making them powerful tools for generating data in scenarios where labeled data is scarce or expensive to obtain.\n",
        "- **Data Generation**: GANs are powerful for generating new data that mimic the distribution of real data. ***They are widely used for image generation, video creation, and even drug discovery!***\n",
        "\n",
        "### Applications\n",
        "GANs have a wide range of applications, from creating photorealistic images, restoring old films, generating human-like speech, to designing new materials. They have also been used creatively, such as generating art and fashion designs, and practically, such as producing synthetic data for training other machine learning models where data may be limited or privacy-sensitive.\n",
        "\n",
        "By understanding these core concepts and components, you'll be well-prepared to delve deeper into how the various components function within the GAN framework.\n",
        "\n",
        "## Understanding the Generator\n",
        "\n",
        "Understanding the architecture of a generator in a Generative Adversarial Network (GAN) and comparing it to a Convolutional Neural Network (CNN) used in the discriminator can provide deep insights into their functions and differences. Here’s a breakdown of the key aspects:\n",
        "\n",
        "### Generator Architecture in GANs\n",
        "The generator in a GAN essentially performs the opposite task of a CNN. While a CNN acts as a feature extractor, where the input is an image that gets progressively downsampled to a more compact feature representation, a generator starts with a low-dimensional noise vector and upsamples it to construct an image. Key components include:\n",
        "\n",
        "1. **Input Layer**: The generator begins with a dense layer that takes a fixed-length noise vector as input.\n",
        "2. **Upsampling Layers**: The primary architecture uses transposed convolutional layers (sometimes referred to as deconvolutional layers). These layers perform an upsampling operation that increases the spatial dimensions (height and width) of the input tensors.\n",
        "3. **Batch Normalization**: This is often used between layers to stabilize training by normalizing the output of a previous activation layer. [Youtube: What is Batch Norm](https://www.youtube.com/watch?v=dXB-KQYkzNU)\n",
        "4. **Activation Functions**: LeakyReLU is commonly used between layers to introduce non-linearity without blocking gradients during training. The final layer typically uses a tanh or sigmoid activation function depending on the input data scaling (-1 to 1 for tanh if images are scaled likewise).\n",
        "\n",
        "### Discriminator Architecture in GANs\n",
        "The discriminator in a GAN is a typical CNN used for classification tasks. It consists of:\n",
        "\n",
        "1. **Input Layer**: The input is an image, either real from the dataset or fake generated by the generator.\n",
        "2. **Downsampling Layers**: These are convolutional layers that progressively reduce the spatial dimension of the input image while increasing the depth (number of channels), effectively extracting features.\n",
        "3. **Batch Normalization and Activation**: Similar to the generator, these are used between convolutional layers, but the activation function is often LeakyReLU without zeroing out the negative part entirely.\n",
        "4. **Output Layer**: It ends with one or more dense layers culminating in a single neuron with a sigmoid activation function to classify the images as real or fake.\n",
        "\n",
        "### Comparisons and Contrasts:\n",
        "- **Direction of Data Flow**: The generator increases the spatial resolution (upsampling), while the discriminator reduces it (downsampling).\n",
        "- **Purpose**: The generator creates data (images), whereas the discriminator evaluates them.\n",
        "- **End Layer**: The generator uses a tanh or sigmoid to output an image, matching the range of the pixel values of the input images. The discriminator ends with a sigmoid to output a probability (0 to 1), indicating the likelihood of the input being a real image.\n",
        "- **Training Goals**: The generator aims to fool the discriminator by generating realistic images, whereas the discriminator aims to correctly classify real and fake images.\n",
        "\n",
        "### Visualizing the Architecture Differences:\n",
        "To better understand, you might want to visualize both networks. Tools like TensorBoard in TensorFlow or NETRON can help visualize the layer structures and flow of tensors.\n",
        "\n",
        "This contrast not only highlights the distinct roles each plays within the GAN framework but also emphasizes the symmetrical nature of the architecture where both are learning from each other, creating a dynamic learning environment.\n",
        "\n",
        "![](https://developer.ibm.com/developer/default/articles/generative-adversarial-networks-explained/images/GANs.jpg)\n"
      ],
      "metadata": {
        "id": "-jTOx6d63Hu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building and Analyzing the GAN"
      ],
      "metadata": {
        "id": "Mzrtye9fAU29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2eQXDun5ASae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare the CIFAR-10 Dataset\n",
        "\n",
        "The CIFAR-10 dataset will be used for training the discriminator. It contains 60,000 32x32 color images in 10 classes, with 6,000 images per class."
      ],
      "metadata": {
        "id": "UfKSp3dHAeJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(X_train, _), (_, _) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "msbJNgmzBAV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc7b655-1e84-4bb0-b16f-a3aa44a54f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the images to [-1, 1]\n",
        "X_train = X_train.astype(np.float32) / 127.5 - 1"
      ],
      "metadata": {
        "id": "VjS2rf7eBUT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the training data\n",
        "\n",
        "```python\n",
        "X_train = X_train.astype(np.float32) / 127.5 - 1\n",
        "```\n",
        "\n",
        "Here’s what each part of this operation does:\n",
        "\n",
        "1. **`X_train.astype(np.float32)`**: This converts the image pixel values from their original integer type (which are typically stored as integers from 0 to 255 in image datasets) to `float32`. This conversion is necessary for the subsequent mathematical operations, which require floating-point precision and are standard practice when preparing data for neural networks to help in reducing the computational cost during training.\n",
        "\n",
        "2. **`/ 127.5`**: Each pixel value is then divided by 127.5. The reason for dividing by 127.5 is to scale the pixel values to the range [0, 2]. Since the original values range from 0 to 255, dividing by 127.5 adjusts these values so that 0 remains 0, and 255 becomes 2.\n",
        "\n",
        "3. **`- 1`**: After scaling the values to [0, 2], subtracting 1 shifts the range to [-1, 1]. This final range [-1, 1] is a common practice when working with models like GANs. Using this normalization, the mean of the input data approximates zero, which generally helps in faster and more stable convergence during training, and is particularly useful for activation functions like the tanh function used in the output layer of many GANs' generators.\n",
        "\n",
        "Overall, this normalization step is crucial for preparing the input data in a way that optimizes the training process for neural networks, aiding in faster convergence and improving the effectiveness of gradient descent during backpropagation."
      ],
      "metadata": {
        "id": "-NGEK2uIBC7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Descriminator\n",
        "\n",
        "The discriminator is a simple CNN structured to classify images as real or fake."
      ],
      "metadata": {
        "id": "iVeUudGRBkb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[32, 32, 3])) # First conv layer\n",
        "    model.add(layers.LeakyReLU()) # Activation Function\n",
        "    model.add(layers.Dropout(0.3)) # Generalizing the model\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')) # Second conv layer\n",
        "    model.add(layers.LeakyReLU()) # Activation Function\n",
        "    model.add(layers.Dropout(0.3)) # Generalizing the model\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1)) # Predict whether the image is fake or real\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPox8a64BqfK",
        "outputId": "1dee280e-adee-4ec9-9ba2-7e18948a1a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 16, 16, 64)        4864      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 128)         204928    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 217985 (851.50 KB)\n",
            "Trainable params: 217985 (851.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Generator\n",
        "\n",
        "The generator uses a dense layer followed by several transposed convolutional layers to upscale the initial noise vector into a full-fledged image."
      ],
      "metadata": {
        "id": "YjTpzw_ACTqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((8, 8, 256)))\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    return model\n",
        "generator = build_generator()\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMs0QfAOCXl7",
        "outputId": "278f9c1c-3875-4c3d-cb56-7f398b5200f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 16384)             1638400   \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 16384)             65536     \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16384)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         819200    \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        204800    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2D  (None, 32, 32, 3)         4800      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2733504 (10.43 MB)\n",
            "Trainable params: 2700352 (10.30 MB)\n",
            "Non-trainable params: 33152 (129.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why do we have a batch normalization and an activation function at the very beginning of the generator build method ? What are it's functions ?\n",
        "\n",
        "In the generator of a Generative Adversarial Network (GAN), the inclusion of batch normalization and an activation function right after the initial dense layer plays a crucial role in stabilizing and improving the training process. Let's break down their roles and why they are particularly important at the beginning of the generator:\n",
        "\n",
        "### Batch Normalization\n",
        "1. **Stabilizing Training**: Batch normalization (BN) standardizes the inputs to a layer for each mini-batch. This means it stabilizes the learning process by normalizing the input layer by adjusting and scaling activations. This is crucial because, in deep networks, the distribution of input values to each layer can shift as the parameters of the previous layers change, a problem known as internal covariate shift. By addressing this issue, BN helps to speed up the training by allowing higher learning rates.\n",
        "2. **Improving Gradient Flow**: Batch normalization helps maintain a healthy gradient flow throughout the training, which can be crucial in deep networks like GANs. This helps in avoiding the vanishing or exploding gradient problems that are more common in deep networks.\n",
        "3. **Acts as Regularization**: Somewhat counterintuitively, batch normalization also acts as a regularizer, reducing the need for other forms of regularization like dropout. This regularization effect can help prevent the generator from overfitting on the training data.\n",
        "\n",
        "### Leaky ReLU Activation Function\n",
        "1. **Allowing Non-Linearity**: The Leaky ReLU function is an improved version of the rectified linear unit (ReLU). While ReLU outputs zero for any negative input, thereby potentially causing the dying ReLU problem (where neurons permanently output zeros), Leaky ReLU allows a small, non-zero, constant gradient α (usually 0.01) when the unit is inactive and the input is less than zero.\n",
        "2. **Enhancing Network Capability**: By allowing small negative values when the inputs are less than zero, Leaky ReLU increases the range of values that the network can model. In the context of a GAN generator, where generating diverse and complex outputs from a simple noise vector is crucial, having a more expressive activation function helps.\n",
        "3. **Preventing Dead Neurons**: In GANs, maintaining active neurons throughout the network is crucial for generating diverse and high-quality outputs. Leaky ReLU helps prevent neurons from dying during training, unlike traditional ReLU, which can suffer from this issue if a large number of inputs are negative.\n",
        "\n",
        "### Placement at the Beginning of the Generator\n",
        "- **Initial Transformation and Stabilization**: The first layer of the generator receives a noise vector and transforms it into a structure that can be molded into an image through successive layers. Starting the process with batch normalization and Leaky ReLU ensures that this transformation is stable and that the network starts with a healthy distribution of activations. This sets a strong foundation for generating high-quality images as the network processes the input through more layers.\n",
        "\n",
        "Including these elements at the very beginning of the generator's architecture in a GAN is a strategic choice to promote effective training and high-quality output generation, crucial for the success of the generative model."
      ],
      "metadata": {
        "id": "C9yRL97LHoml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the first line of the Generator block\n",
        "\n",
        "The line in question is part of defining the first layer of the generator. Let's break it down for a clearer understanding:\n",
        "\n",
        "```python\n",
        "model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n",
        "```\n",
        "\n",
        "### Explaining `8*8*256`\n",
        "This part of the `Dense` layer specifies the number of neurons or units in the layer. The expression `8*8*256` evaluates to 16,384. This is a strategically chosen value because:\n",
        "\n",
        "- **Structural Planning**: The generator will reshape this layer's output in subsequent steps to form a 3D tensor or feature map. Specifically, this tensor will be of shape `(8, 8, 256)`, where:\n",
        "  - `8, 8` are the spatial dimensions (height and width) of the feature map.\n",
        "  - `256` is the depth of the feature map, or the number of channels/filters at this stage of the generator.\n",
        "\n",
        "This structure is typical in GANs where the generator's output gradually \"expands\" through additional layers to form a full-sized image. Here, the 8x8 feature map is an intermediate representation, which will be upsampled to larger resolutions in subsequent layers (using transposed convolutions) until it matches the desired output dimensions (like 32x32 pixels for CIFAR-10 images).\n",
        "\n",
        "### Explaining `input_shape=(100,)`\n",
        "This parameter defines the shape of the input to the layer, which in the context of GANs, is typically a noise vector:\n",
        "\n",
        "- **Noise Vector**: The generator starts with a noise vector as input. This vector is randomly sampled from a latent space (often a Gaussian distribution), and it serves as the \"seed\" from which the generator produces an image.\n",
        "- **Dimensionality**: The size of this noise vector is 100, meaning each input instance to the generator is a vector of 100 random numbers. The dimensionality of the latent space (100 in this case) is a parameter that can be tuned based on the complexity of the data and the desired diversity of the generated outputs.\n",
        "\n",
        "### Use of `use_bias=False`\n",
        "This argument indicates that the dense layer should not use any bias terms. In neural networks, bias terms are generally added to the output of the weighted sum of inputs to provide additional flexibility. However, in many deep learning models, especially in GANs, it's common to omit the bias when batch normalization will immediately follow. This is because batch normalization adjusts the mean and variance of the output from the dense layer, which can make the role of the bias term redundant. By setting `use_bias=False`, you reduce the number of parameters, simplifying the model slightly and often improving the stability of training.\n",
        "\n",
        "In summary, this line in your generator model establishes the first transformation stage from a compact, high-dimensional latent representation (the noise vector) into a more expansive, structured format that sets the stage for generating a detailed image through subsequent layers."
      ],
      "metadata": {
        "id": "EPVTCJllIWU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observation:\n",
        "\n",
        "Unline the Discriminator network where each convolution layer had a larger number of filters from left to right [64, 128], the generator network, decreases the number of layers from the left to right [128, 64, 3]"
      ],
      "metadata": {
        "id": "RsNkDIjGIukD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-1-13.png)\n",
        "\n",
        "![](https://images.squarespace-cdn.com/content/v1/5c1828d7c258b4d2ab69b7d7/1558277200599-5LP5V7W9V0CACTAJY9SP/Figure+1.jpg)"
      ],
      "metadata": {
        "id": "rA6IcLJ2KCYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Loss and Optimizers\n",
        "\n",
        "GANs require separate optimizers for the generator and discriminator due to their adversarial training dynamics."
      ],
      "metadata": {
        "id": "R0lV-0uuKtKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "fKZ90N9rKxDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the loss function for the generator and the discriminator\n",
        "\n",
        "\n",
        "### Loss Functions\n",
        "In GANs, the generator and discriminator have competing goals, which is why they use different loss functions that are interconnected through the training process.\n",
        "\n",
        "#### Binary Crossentropy Loss\n",
        "```python\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "```\n",
        "- **Binary Crossentropy**: This is a common loss function used in binary classification tasks. It measures the distance between the actual class labels and the predicted class probabilities, aiming to minimize the differences.\n",
        "- **`from_logits=True`**: This parameter indicates that the inputs to the function are raw logits (i.e., unscaled outputs of models). Using logits directly provides numerical stability and is especially important when the output layer does not include a sigmoid activation function.\n",
        "\n",
        "#### Discriminator Loss\n",
        "```python\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "```\n",
        "- **Real Loss**: This is calculated by comparing the discriminator's predictions on real images to an array of ones (since real images should ideally trigger a discriminator output close to 1).\n",
        "- **Fake Loss**: This is calculated by comparing the discriminator's predictions on fake (generated) images to an array of zeros (since fake images should ideally trigger a discriminator output close to 0).\n",
        "- **Total Loss**: The discriminator's total loss is the sum of the real loss and the fake loss. This combined loss function encourages the discriminator to correctly classify real images as real and fake images as fake.\n",
        "\n",
        "#### Generator Loss\n",
        "```python\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "```\n",
        "- **Generator Loss**: The generator's loss is calculated by assessing how well it has tricked the discriminator. Ideally, the generator wants the discriminator to predict the fake images as real. Therefore, this loss is computed by comparing the discriminator's predictions on the fake images to an array of ones.\n",
        "\n",
        "### Optimizers\n",
        "```python\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "```\n",
        "- **Adam Optimizer**: Adam is an optimization algorithm that can handle sparse gradients on noisy problems. It's widely used in training deep neural networks because it combines the advantages of two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp).\n",
        "- **Learning Rate (1e-4)**: The learning rate specified here is `0.0001`. This relatively low learning rate is often chosen in the context of GANs to ensure stable training, as GANs are particularly sensitive to the rate at which they learn.\n",
        "\n",
        "In summary, these loss functions and optimizers are configured to pit the discriminator and generator against each other effectively. The discriminator learns to distinguish between real and fake, while the generator learns to produce increasingly convincing fakes, thus improving through adversarial training."
      ],
      "metadata": {
        "id": "Q7dBRhdPNfnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the training loop\n",
        "\n",
        "The training loop involves alternating between training the discriminator with real and fake images and training the generator to fool the discriminator."
      ],
      "metadata": {
        "id": "PHsSAe0ZPt5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "Gv2oYVZyYBE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed to visualize progress\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "metadata": {
        "id": "vyVsFWCiP1OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint\n",
        "\n",
        "The checkpoint mechanism in TensorFlow is used to save and restore models, which is crucial for long training processes like those typically required for training GANs. This allows training to be paused and resumed without loss of progress, and also provides a way to recover from interruptions or crashes.\n",
        "\n",
        "Here’s how you might define and use a checkpoint in your GAN training code:"
      ],
      "metadata": {
        "id": "OkAEle6cRMOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create a checkpoint directory to store the checkpoints.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                  discriminator_optimizer=discriminator_optimizer,\n",
        "                                  generator=generator,\n",
        "                                  discriminator=discriminator)"
      ],
      "metadata": {
        "id": "oJ3YP45sRQjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    # Notice `training` is set to False.\n",
        "    # This is so all layers run in inference mode (batchnorm).\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(tf.cast(predictions[i, :, :, :] * 127.5 + 127.5, 'uint8'))\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    # plt.show()"
      ],
      "metadata": {
        "id": "WRO9bHbuRbDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "Using the defined train step, train your model over the specified epochs."
      ],
      "metadata": {
        "id": "yBldqnGnQi9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import display, clear_output"
      ],
      "metadata": {
        "id": "SHBqgfMQfF-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch in dataset:\n",
        "            train_step(image_batch)\n",
        "\n",
        "        # Produce images and save it every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            # display.clear_output(wait=True)\n",
        "            generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "        # Save the model every 15 epochs\n",
        "        if (epoch + 1) % 15 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        print(f\"Epoch {epoch+1} completed\")\n",
        "\n",
        "    # Generate after the final epoch\n",
        "    # clear_output(wait=True)\n",
        "    generate_and_save_images(generator, epochs, seed)"
      ],
      "metadata": {
        "id": "-p8OqWq8QnJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(X_train.shape[0]//2).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "U_pr0lWiQ5As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, epochs=500)"
      ],
      "metadata": {
        "id": "S2BS_GK0S6uo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "a6162099-c8db-4635-c89a-c18b50e27c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-aa676e1496c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-7afb7f4b3b7b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Produce images and save it every 100 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Whats Buffer Size ?\n",
        "\n",
        "In the context of training neural networks, including Generative Adversarial Networks (GANs), the `BUFFER_SIZE` parameter is used particularly with TensorFlow's `tf.data.Dataset` API to specify the number of samples from the dataset that are held in a buffer from which the dataset can randomly sample. This is used during the shuffling of the dataset's elements, which is an essential part of data preprocessing in machine learning to ensure that the model does not learn anything specific to the order of the data.\n",
        "\n",
        "### Purpose of Buffer Size\n",
        "- **Random Sampling**: `BUFFER_SIZE` is used to determine how many items are contained in the buffer from which the next batch is sampled. This means that TensorFlow will randomly select the next batch from this subset of your data.\n",
        "- **Shuffling**: This buffer is continuously replenished with new data points as data is read from the dataset, ensuring that the shuffling operation maintains a degree of randomness. The larger the buffer, the better the randomness, and by extension, the independence of the batches.\n",
        "\n",
        "### Example Usage\n",
        "When setting up a data pipeline, you might see something like this:\n",
        "\n",
        "```python\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "```\n",
        "\n",
        "In this line, `BUFFER_SIZE` could be set to a value like the size of the dataset or a fraction thereof. Here's how it's typically determined:\n",
        "\n",
        "- **Full Dataset Shuffling**: If `BUFFER_SIZE` is set equal to the total number of samples in the dataset (`X_train.shape[0]` in the case of CIFAR-10), then the shuffling is perfect—every data point can potentially interact with every other data point in each epoch. This however can be memory intensive.\n",
        "- **Partial Shuffling**: If memory constraints are an issue, `BUFFER_SIZE` can be set to a smaller number. While this won’t provide as effective shuffling, it can still offer a good degree of randomness without the same memory requirements.\n",
        "\n",
        "### Choosing a Buffer Size\n",
        "Choosing an appropriate `BUFFER_SIZE` is a balance between computational efficiency and training effectiveness. Larger buffers provide better randomness but at the cost of increased memory usage and potentially slower performance. In practice, you might start with a size like the number of training samples or a significant fraction of it, and adjust based on the available system resources and specific needs of the training process.\n",
        "\n",
        "It's important to note that the choice of `BUFFER_SIZE` can have a significant impact on the performance of the model, especially for datasets where the order of data might influence the learning process negatively (e.g., time series data where the order is crucial should not be shuffled)."
      ],
      "metadata": {
        "id": "OM3kR03jR4Su"
      }
    }
  ]
}