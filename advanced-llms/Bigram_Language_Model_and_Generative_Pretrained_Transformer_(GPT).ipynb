{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathydNU/gen-ai-coursework/blob/main/advanced-llms/Bigram_Language_Model_and_Generative_Pretrained_Transformer_(GPT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Bigram Language Model and Generative Pretrained Transformer (GPT)\n",
        "\n",
        "\n",
        "The objective of this assignment is to train a simplified transformer model. The primary differences between the implementation:\n",
        "* tokenizer (we use a character level encoder simplicity and compute constraints)\n",
        "* size (we are using 1 consumer grade gpu hosted on colab and a small dataset. in practice, the models are much larger and are trained on much more data)\n",
        "* efficiency\n",
        "\n",
        "\n",
        "Most modern LLMs have multiple training stages, so we won't get a model that is capable of replying to you yet. However, this is the first step towards a model like ChatGPT and Llama.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fYCVVv_1A1kZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F097yaiu7dXQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Bigram MLP for TinyShakespeare (35 points)\n",
        "\n",
        "1a) (1 point). Create a list `chars` that contains all unique characters in `text`\n",
        "\n",
        "1b) (2 points). Implement `encode(s: str) -> list[int]`\n",
        "\n",
        "1c) (2 points). Implement `decode(ids: list[int]) -> str`\n",
        "\n",
        "1d) (5 points). Create two tensors, `inputs_one_hot` and `outputs_one_hot`. Use one hot encoding. Make sure to get every consecutive pair of characters. For example, for the word 'hello', we should create the following input-output pairs\n",
        "```\n",
        "he\n",
        "el\n",
        "ll\n",
        "lo\n",
        "```\n",
        "\n",
        "1e) (10 points). Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token. Specifically, implement the constructor, forward, and generate. The output dimension of the first layer should be 8. Use `torch.optim`. The activation function for the first layer should be `nn.LeakyReLU()`\n",
        "\n",
        "Note: Use the `torch.nn.function.cross_entropy` loss. Read the [docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) about how this loss function works. The logits are the output of a network WITHOUT an activation function applied to the last layer. There are activation functions are applied to every layer except the last.\n",
        "\n",
        "1f) (5 points). Train the BigramOneHotMLP for 1000 steps.\n",
        "\n",
        "1g) (5 points). Create two tensors, `input_ids` and `outputs_one_hot`. These `input_ids` will be used for the embedding layer.\n",
        "\n",
        "1h) (5 points). Implement and train BigramEmbeddingMLP, a 2 layer mlp that predicts the next token. Specifically, implement the constructor, forward, and generate functions. The output dimension of the first layer should be 8. Use `torch.optim`.\n",
        "\n",
        "\n",
        "\n",
        "Note: the output will look like gibberish\n"
      ],
      "metadata": {
        "id": "8qra06Ema_VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iradmn7bZtM",
        "outputId": "626fac42-1dce-4286-c3d7-3e7da3d7e96c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-01 23:38:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-07-01 23:38:06 (131 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the bigram model, let's use the first 1000 characters for the data\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text = text[:5000]"
      ],
      "metadata": {
        "id": "pLoVi294G-T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "NRUazcXhTC_G",
        "outputId": "1c94424c-726d-490b-afb3-29d38c38d459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not maliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was for his country he did it to\\nplease his mother and to be partly proud; which he\\nis, even till the altitude of his virtue.\\n\\nSecond Citizen:\\nWhat he cannot help in his nature, you account a\\nvice in him. You must in no way say he is covetous.\\n\\nFirst Citizen:\\nIf I must not, I need not be barren of accusations;\\nhe hath faults, with surplus, to tire in repetition.\\nWhat shouts are these? The other side o' the city\\nis risen: why stay we prating here? to the Capitol!\\n\\nAll:\\nCome, come.\\n\\nFirst Citizen:\\nSoft! who comes here?\\n\\nSecond Citizen:\\nWorthy Menenius Agrippa; one that hath always loved\\nthe people.\\n\\nFirst Citizen:\\nHe's one honest enough: would all the rest were so!\\n\\nMENENIUS:\\nWhat work's, my countrymen, in hand? where go you\\nWith bats and clubs? The matter? speak, I pray you.\\n\\nFirst Citizen:\\nOur business is not unknown to the senate; they have\\nhad inkling this fortnight what we intend to do,\\nwhich now we'll show 'em in deeds. They say poor\\nsuitors have strong breaths: they shall know we\\nhave strong arms too.\\n\\nMENENIUS:\\nWhy, masters, my good friends, mine honest neighbours,\\nWill you undo yourselves?\\n\\nFirst Citizen:\\nWe cannot, sir, we are undone already.\\n\\nMENENIUS:\\nI tell you, friends, most charitable care\\nHave the patricians of you. For your wants,\\nYour suffering in this dearth, you may as well\\nStrike at the heaven with your staves as lift them\\nAgainst the Roman state, whose course will on\\nThe way it takes, cracking ten thousand curbs\\nOf more strong link asunder than can ever\\nAppear in your impediment. For the dearth,\\nThe gods, not the patricians, make it, and\\nYour knees to them, not arms, must help. Alack,\\nYou are transported by calamity\\nThither where more attends you, and you slander\\nThe helms o' the state, who care for you like fathers,\\nWhen you curse them as enemies.\\n\\nFirst Citizen:\\nCare for us! True, indeed! They ne'er cared for us\\nyet: suffer us to famish, and their store-houses\\ncrammed with grain; make edicts for usury, to\\nsupport usurers; repeal daily any wholesome act\\nestablished against the rich, and provide more\\npiercing statutes daily, to chain up and restrain\\nthe poor. If the wars eat us not up, they will; and\\nthere's all the love they bear us.\\n\\nMENENIUS:\\nEither you must\\nConfess yourselves wondrous malicious,\\nOr be accused of folly. I shall tell you\\nA pretty tale: it may be you have heard it;\\nBut, since it serves my purpose, I will venture\\nTo stale 't a little more.\\n\\nFirst Citizen:\\nWell, I'll hear it, sir: yet you must not think to\\nfob off our disgrace with a tale: but, an 't please\\nyou, deliver.\\n\\nMENENIUS:\\nThere was a time when all the body's members\\nRebell'd against the belly, thus accused it:\\nThat only like a gulf it did remain\\nI' the midst o' the body, idle and unactive,\\nStill cupboarding the viand, never bearing\\nLike labour with the rest, where the other instruments\\nDid see and hear, devise, instruct, walk, feel,\\nAnd, mutually participate, did minister\\nUnto the appetite and affection common\\nOf the whole body. The belly answer'd--\\n\\nFirst Citizen:\\nWell, sir, what answer made the belly?\\n\\nMENENIUS:\\nSir, I shall tell you. With a kind of smile,\\nWhich ne'er came from the lungs, but even thus--\\nFor, look you, I may make the belly smile\\nAs well as speak--it tauntingly replied\\nTo the discontented members, the mutinous parts\\nThat envied his receipt; even so most fitly\\nAs you malign our senators for that\\nThey are not such as you.\\n\\nFirst Citizen:\\nYour belly's answer? What!\\nThe kingly-crowned head, the vigilant eye,\\nThe counsell\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique characters from the text\n",
        "# To have unique values, we can use the set data structure\n",
        "# Reference https://stackoverflow.com/questions/13902805/list-of-all-unique-characters-in-a-string\n",
        "chars = list(set(text))"
      ],
      "metadata": {
        "id": "hoMGZgEOdRjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjjroOrYT8pv",
        "outputId": "1ba12d04-ab7f-4766-f504-33319f080ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We have 46 unique characters"
      ],
      "metadata": {
        "id": "wYWOOhgrT_nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode(string: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Given a string, encode returns a list of integers that represent the characters\n",
        "    in the string.\n",
        "    \"\"\"\n",
        "    encodedChars = []\n",
        "\n",
        "    for s in string:\n",
        "      encodedChars.append(chars.index(s))\n",
        "    return encodedChars\n",
        "\n",
        "def decode(ids: list[int]) -> str:\n",
        "    \"\"\"\n",
        "    Given a list of integers, decode returns the characters in the list as a string.\n",
        "    \"\"\"\n",
        "    decodedChars = [];\n",
        "    for id in ids:\n",
        "      decodedChars.append(chars[id])\n",
        "    return \"\".join(decodedChars)\n",
        "\n",
        "# Testing the encode and decode functions\n",
        "\n",
        "encoded = encode('hello')\n",
        "decoded = decode(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKdvB6FYTr85",
        "outputId": "827d0257-b4f2-4111-8302-728de562616c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14, 21, 3, 3, 29]\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "def create_one_hot_inputs_and_outputs(text: str) -> list[torch.tensor, torch.tensor]:\n",
        "    \"\"\"\n",
        "    For a given word we need to generate all pairs of consecutive characters.\n",
        "    For example, for the word 'hello', we should create the following input-output pairs\n",
        "    he\n",
        "    el\n",
        "    ll\n",
        "    lo\n",
        "\n",
        "    Additionally, we need to conver the input and output to one hot\n",
        "    encoded tensors.\n",
        "\n",
        "    Here we have 46 unique characters, so we take the input of shape (1)\n",
        "    And return an output of shape (1,46) for each character\n",
        "\n",
        "    Say we have the word hello, we need to return the following tensors\n",
        "    [one_hot_encoded_h][one_hot_encoded_e]\n",
        "    [one_hot_encoded_e][one_hot_encoded_l]\n",
        "    [one_hot_encoded_l][one_hot_encoded_l]\n",
        "    [one_hot_encoded_l][one_hot_encoded_o]\n",
        "\n",
        "    All the tensors should be of shape (1,46)\n",
        "    And all the tensors on the left, go into inputs_one_hot\n",
        "    And all the tensors on the right, go into outputs_one_hot\n",
        "    \"\"\"\n",
        "    inputs_one_hot = []\n",
        "    outputs_one_hot = []\n",
        "\n",
        "    # we know which index does each character fall into\n",
        "    # usign the encode method\n",
        "\n",
        "    for i in range(len(text) - 1):\n",
        "      input_char = text[i]\n",
        "      output_char = text[i+1]\n",
        "\n",
        "      # One-hot encoding the input character\n",
        "      input_one_hot = torch.zeros(1, len(chars))\n",
        "      input_one_hot[0][encode(input_char)] = 1\n",
        "      inputs_one_hot.append(input_one_hot)\n",
        "\n",
        "      # One-hot encoding the output character\n",
        "      output_one_hot = torch.zeros(1, len(chars))\n",
        "      output_one_hot[0][encode(output_char)] = 1\n",
        "      outputs_one_hot.append(output_one_hot)\n",
        "\n",
        "    return inputs_one_hot, outputs_one_hot\n",
        "\n",
        "# Example usage\n",
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs(text=\"hi\")\n",
        "print(inputs_one_hot)\n",
        "print(outputs_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taYatNwPULUP",
        "outputId": "4a6491e3-1a01-4b09-df20-77d6f7f84e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n",
            "[tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs(text)"
      ],
      "metadata": {
        "id": "RIa-EYb3kLRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(inputs_one_hot))\n",
        "print(len(outputs_one_hot))\n",
        "print(inputs_one_hot[0].shape)\n",
        "print(outputs_one_hot[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdyulNPQlWuf",
        "outputId": "5518a93b-73b0-421e-97f7-aaa4322a9b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4999\n",
            "4999\n",
            "torch.Size([1, 53])\n",
            "torch.Size([1, 53])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We have one hot encoded both the input and output combinations. And each entry is of shape (1,46)"
      ],
      "metadata": {
        "id": "9TbMifdultnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token.\n",
        "  Specifically, implement the constructor, forward, and generate.\n",
        "  The output dimension of the first layer should be 8. Use torch.optim.\n",
        "  The activation function for the first layer should be nn.LeakyReLU()\n",
        "  Note: Use the torch.nn.function.cross_entropy loss.\n",
        "  Read the docs about how this loss function works.\n",
        "  The logits are the output of a network WITHOUT an activation\n",
        "  function applied to the last layer. There are activation functions\n",
        "  are applied to every layer except the last.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BigramOneHotMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # Calls the init function on the nn.Module class\n",
        "        # 2 Layer mlp that predicts the next token\n",
        "        self.fc1 = nn.Linear(len(chars), 8) # takes inchars. values and outputs 8 values\n",
        "        self.fc2 = nn.Linear(8, len(chars)) # takes in 8 values and outputs 46 values\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [*, 5] batch size: *, 46 dimension (one hot)\n",
        "        out = self.fc1(x) # [*, 8]\n",
        "        out = F.leaky_relu(out) # [*, 8] Activation function of the first layer is leaky relu\n",
        "        out = self.fc2(out) # [*, 46] Final output values, gives a one hot encoded value of the output character\n",
        "        return out\n",
        "\n",
        "bigram_one_hot_mlp = BigramOneHotMLP()"
      ],
      "metadata": {
        "id": "oMbHFp3NlVCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the loss function and optimizer\n",
        "\n",
        "Reference : https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy\n",
        "\n",
        "```python\n",
        "torch.nn.functional.cross_entropy(\n",
        "  input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0\n",
        ")\n",
        "```\n",
        "\n",
        "Parameters\n",
        "- input (Tensor) – Predicted unnormalized logits;\n",
        "- target (Tensor) – Ground truth class indices or class probabilities;\n",
        "- reduction (str, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. **bold text** Default: 'mean'\n",
        "\n",
        "```python\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "```"
      ],
      "metadata": {
        "id": "YyKH6cgK2a2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "chnRG8Zk3_uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to training mode\n",
        "bigram_one_hot_mlp.train()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = torch.optim.SGD(bigram_one_hot_mlp.parameters(), lr=0.01)\n",
        "\n",
        "# Compute the cross entropy loss between input logits and target.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# We already have the inputs and outputs defined as one hot encoded\n",
        "# We just need to convert them to float so that values flow smoothly through the model\n",
        "\n",
        "inputs_one_hot = torch.vstack(inputs_one_hot).float()\n",
        "outputs_one_hot = torch.vstack(outputs_one_hot).float()"
      ],
      "metadata": {
        "id": "lELdqEOX2Eza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR6uU3Zm7Bpk",
        "outputId": "8b3244d0-c3a5-4ba6-97fd-d5c7ed2b0b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4999, 53])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_indices = torch.argmax(outputs_one_hot, dim=1)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(5000):\n",
        "  # Flush the gradients from prev iteration\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # forward pass\n",
        "  outputs = bigram_one_hot_mlp(inputs_one_hot)\n",
        "  loss = loss_fn(outputs, target_indices)\n",
        "\n",
        "  # backward pass and updatin the weights\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(epoch + 1) % 50 == 0:\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "029YpQRv7Ama",
        "outputId": "a406ab91-b854-4b15-fa5f-2180b511ce14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss: 3.289602756500244\n",
            "Epoch 100, Loss: 3.2819199562072754\n",
            "Epoch 150, Loss: 3.2746875286102295\n",
            "Epoch 200, Loss: 3.2678771018981934\n",
            "Epoch 250, Loss: 3.261469602584839\n",
            "Epoch 300, Loss: 3.2554359436035156\n",
            "Epoch 350, Loss: 3.2497506141662598\n",
            "Epoch 400, Loss: 3.2443928718566895\n",
            "Epoch 450, Loss: 3.2393407821655273\n",
            "Epoch 500, Loss: 3.234571933746338\n",
            "Epoch 550, Loss: 3.230064868927002\n",
            "Epoch 600, Loss: 3.225799798965454\n",
            "Epoch 650, Loss: 3.221761465072632\n",
            "Epoch 700, Loss: 3.2179362773895264\n",
            "Epoch 750, Loss: 3.2143027782440186\n",
            "Epoch 800, Loss: 3.2108397483825684\n",
            "Epoch 850, Loss: 3.207523822784424\n",
            "Epoch 900, Loss: 3.2043662071228027\n",
            "Epoch 950, Loss: 3.2013511657714844\n",
            "Epoch 1000, Loss: 3.1984596252441406\n",
            "Epoch 1050, Loss: 3.1956849098205566\n",
            "Epoch 1100, Loss: 3.1930253505706787\n",
            "Epoch 1150, Loss: 3.190463066101074\n",
            "Epoch 1200, Loss: 3.187995672225952\n",
            "Epoch 1250, Loss: 3.1856091022491455\n",
            "Epoch 1300, Loss: 3.1832964420318604\n",
            "Epoch 1350, Loss: 3.1810507774353027\n",
            "Epoch 1400, Loss: 3.1788628101348877\n",
            "Epoch 1450, Loss: 3.176730155944824\n",
            "Epoch 1500, Loss: 3.174649477005005\n",
            "Epoch 1550, Loss: 3.1726152896881104\n",
            "Epoch 1600, Loss: 3.170623779296875\n",
            "Epoch 1650, Loss: 3.1686692237854004\n",
            "Epoch 1700, Loss: 3.166749954223633\n",
            "Epoch 1750, Loss: 3.1648621559143066\n",
            "Epoch 1800, Loss: 3.162994623184204\n",
            "Epoch 1850, Loss: 3.161135196685791\n",
            "Epoch 1900, Loss: 3.15929913520813\n",
            "Epoch 1950, Loss: 3.157482147216797\n",
            "Epoch 2000, Loss: 3.1556835174560547\n",
            "Epoch 2050, Loss: 3.1539063453674316\n",
            "Epoch 2100, Loss: 3.152146577835083\n",
            "Epoch 2150, Loss: 3.150402545928955\n",
            "Epoch 2200, Loss: 3.1486713886260986\n",
            "Epoch 2250, Loss: 3.1469509601593018\n",
            "Epoch 2300, Loss: 3.145240068435669\n",
            "Epoch 2350, Loss: 3.143537998199463\n",
            "Epoch 2400, Loss: 3.141843557357788\n",
            "Epoch 2450, Loss: 3.140155076980591\n",
            "Epoch 2500, Loss: 3.1384713649749756\n",
            "Epoch 2550, Loss: 3.1367881298065186\n",
            "Epoch 2600, Loss: 3.1351091861724854\n",
            "Epoch 2650, Loss: 3.133434295654297\n",
            "Epoch 2700, Loss: 3.1317615509033203\n",
            "Epoch 2750, Loss: 3.130089282989502\n",
            "Epoch 2800, Loss: 3.1284165382385254\n",
            "Epoch 2850, Loss: 3.1267476081848145\n",
            "Epoch 2900, Loss: 3.1250648498535156\n",
            "Epoch 2950, Loss: 3.1233773231506348\n",
            "Epoch 3000, Loss: 3.121689558029175\n",
            "Epoch 3050, Loss: 3.1200008392333984\n",
            "Epoch 3100, Loss: 3.1183111667633057\n",
            "Epoch 3150, Loss: 3.116621732711792\n",
            "Epoch 3200, Loss: 3.114932060241699\n",
            "Epoch 3250, Loss: 3.113241195678711\n",
            "Epoch 3300, Loss: 3.111546516418457\n",
            "Epoch 3350, Loss: 3.109848976135254\n",
            "Epoch 3400, Loss: 3.1081430912017822\n",
            "Epoch 3450, Loss: 3.1064329147338867\n",
            "Epoch 3500, Loss: 3.1047186851501465\n",
            "Epoch 3550, Loss: 3.1030004024505615\n",
            "Epoch 3600, Loss: 3.10127854347229\n",
            "Epoch 3650, Loss: 3.0995519161224365\n",
            "Epoch 3700, Loss: 3.0978243350982666\n",
            "Epoch 3750, Loss: 3.0960917472839355\n",
            "Epoch 3800, Loss: 3.0943543910980225\n",
            "Epoch 3850, Loss: 3.0926120281219482\n",
            "Epoch 3900, Loss: 3.0908639430999756\n",
            "Epoch 3950, Loss: 3.0891098976135254\n",
            "Epoch 4000, Loss: 3.087350606918335\n",
            "Epoch 4050, Loss: 3.085585832595825\n",
            "Epoch 4100, Loss: 3.0838141441345215\n",
            "Epoch 4150, Loss: 3.0820369720458984\n",
            "Epoch 4200, Loss: 3.0802528858184814\n",
            "Epoch 4250, Loss: 3.0784645080566406\n",
            "Epoch 4300, Loss: 3.076672077178955\n",
            "Epoch 4350, Loss: 3.074873208999634\n",
            "Epoch 4400, Loss: 3.073068380355835\n",
            "Epoch 4450, Loss: 3.071258306503296\n",
            "Epoch 4500, Loss: 3.0694427490234375\n",
            "Epoch 4550, Loss: 3.0676207542419434\n",
            "Epoch 4600, Loss: 3.0657920837402344\n",
            "Epoch 4650, Loss: 3.063955545425415\n",
            "Epoch 4700, Loss: 3.0621109008789062\n",
            "Epoch 4750, Loss: 3.0602595806121826\n",
            "Epoch 4800, Loss: 3.058401584625244\n",
            "Epoch 4850, Loss: 3.0565342903137207\n",
            "Epoch 4900, Loss: 3.054659605026245\n",
            "Epoch 4950, Loss: 3.0527777671813965\n",
            "Epoch 5000, Loss: 3.050889253616333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start='a', max_new_tokens=5) -> str:\n",
        "    \"\"\"\n",
        "    Generate text given a starting point\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval() # set the model to eval mode\n",
        "    with torch.no_grad(): # disables gradient calculation\n",
        "\n",
        "      word = start;\n",
        "      currentChar = start;\n",
        "      for _ in range(max_new_tokens):\n",
        "\n",
        "        # We one hot encode the current character\n",
        "        input_one_hot = torch.zeros(1, len(chars))\n",
        "        input_one_hot[0][encode(currentChar)] = 1\n",
        "\n",
        "\n",
        "        # Pass this through the model to get the probablility distribution\n",
        "        # of the next char in the same shape as the input\n",
        "        output = model.forward(input_one_hot) # [1, 46] Eg: [0.06, 0.01, 0.5, .....]\n",
        "\n",
        "        # Get the index that has the max value\n",
        "        # torch.argmax(output): This function is used to find the index of the maximum value in the output tensor.\n",
        "        # .item(): This method is used to get a Python number from a tensor containing a single value.\n",
        "        next_char_id = torch.argmax(output).item()\n",
        "\n",
        "        # Get the character from the set based on the id\n",
        "        next_char = chars[next_char_id]\n",
        "\n",
        "        # Update the word and the current character\n",
        "        currentChar = next_char\n",
        "        word += currentChar\n",
        "\n",
        "      return word"
      ],
      "metadata": {
        "id": "R-yPJk4I-AH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model=bigram_one_hot_mlp, start='r', max_new_tokens=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kgXpzrU49jQ",
        "outputId": "9a54eca2-ba8b-4081-8674-ed9d2944aced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations and thoughts\n",
        "\n",
        "Looks like the one hot encoding input does not help the model that much in predicting the next token.\n",
        "\n",
        "Here are some thoughts regarding one hot encoded vectors vs embeddings:\n",
        "\n",
        "Embeddings provide a powerful alternative to one-hot encoded inputs, especially in models dealing with natural language processing, recommendation systems, and other domains where categorical variables play a crucial role. Here are the primary reasons why embeddings often result in better model performance compared to one-hot encoded inputs:\n",
        "\n",
        "### 1. **Dimensionality Reduction**\n",
        "- **One-hot encoding** produces very high-dimensional vectors where the length is equal to the number of categories in the vocabulary. For example, a vocabulary with 10,000 words results in 10,000-dimensional vectors, with only one non-zero element. This high dimensionality can lead to computational inefficiency and sparsity issues.\n",
        "- **Embeddings**, on the other hand, map these large one-hot vectors into a much smaller dimensional space (e.g., 50, 100, or 300 dimensions). This dense representation reduces the model's memory footprint and accelerates computation.\n",
        "\n",
        "### 2. **Semantic Information Preservation**\n",
        "- **One-hot vectors** are orthogonal and equidistant from each other, implying no relationship between different categories or words. This means one-hot encoding does not capture any form of similarity or semantic relationship between the categories.\n",
        "- **Embedding vectors** are learned during training and can capture complex relationships between categories. Words or items that are used in similar contexts will have embeddings that are closer in the vector space, which helps in capturing semantic meanings and relationships that are not possible with one-hot encoding.\n",
        "\n",
        "### 3. **Better Gradient Flow**\n",
        "- In neural networks, **one-hot inputs** often lead to inefficient gradient flow during backpropagation because only a small part of the weights (those corresponding to the 'hot' part of the vector) are updated at each training step. This inefficiency can slow down the learning process.\n",
        "- **Embeddings** provide a more efficient gradient flow as every dimension of the embedding vector contributes to the learning process and receives updates during training. This generally leads to faster convergence in training neural networks.\n",
        "\n",
        "### 4. **Generalization**\n",
        "- **Embeddings** can help the model generalize better to new, unseen examples. Since embeddings capture semantic relationships, a well-trained model can perform reasonably well even when encountering new words or categories similar to those seen during training.\n",
        "- With **one-hot encoding**, any new category not seen during training would require expanding the encoding scheme, which might not be feasible and does not leverage any learned contextual relationships.\n",
        "\n",
        "### 5. **Network Depth and Complexity**\n",
        "- Using **embeddings** allows deeper network architectures since the input data is more manageable in size and richer in information. Embeddings can be easily integrated into various layers of a neural network, facilitating more complex interactions in the model.\n",
        "- With **one-hot encoded inputs**, adding network depth can be less effective due to the sparsity and high dimensionality, which might not add meaningful information through deeper layers.\n",
        "\n",
        "### Example in NLP:\n",
        "In natural language processing, word embeddings like Word2Vec, GloVe, or those learned by an embedding layer in a deep learning model allow words with similar meanings to have similar representations. This is not achievable with one-hot encoding where each word is isolated with no shared information.\n",
        "\n",
        "In summary, embeddings provide a compact, dense, and semantically rich representation of categorical data, making them more suitable for complex models and large datasets where one-hot encoding would be inefficient both computationally and in terms of learning capability."
      ],
      "metadata": {
        "id": "G3MX-tJSDGfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Bigram MLP"
      ],
      "metadata": {
        "id": "vV7zsgV3Bnul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_inputs_and_outputs() -> list[torch.tensor, torch.tensor]:\n",
        "    \"\"\"\n",
        "    This function retruns input ids and outpits_one_hot_encoded\n",
        "    Here we are trying to perform better than the one_hot_encoded Bigram MLP\n",
        "\n",
        "    In the one hot encoded version we did pairs of\n",
        "    [one_hot_encoded_h][one_hot_encoded_e]\n",
        "    [one_hot_encoded_e][one_hot_encoded_l]\n",
        "\n",
        "    Here we do it in a different way\n",
        "    [id_h][one_hot_encoded_e]\n",
        "    [id_e][one_hot_encoded_l]\n",
        "    [id_l][one_hot_encoded_l]\n",
        "    [id_l][one_hot_encoded_o]\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    outputs_one_hot = []\n",
        "\n",
        "    # We have the chars list that has list of characters\n",
        "    # We have the text, which is the dataset that we have to train on\n",
        "\n",
        "    for i in range(len(text) - 1):\n",
        "      input_char = text[i]\n",
        "      output_char = text[i+1]\n",
        "\n",
        "      # Id of the input character\n",
        "      input_id = chars.index(input_char)\n",
        "      input_ids.append(input_id)\n",
        "\n",
        "      # One-hot encoding the output character\n",
        "      output_one_hot = torch.zeros(1, len(chars))\n",
        "      output_one_hot[0][encode(output_char)] = 1\n",
        "      outputs_one_hot.append(output_one_hot)\n",
        "\n",
        "    return input_ids, outputs_one_hot\n",
        "\n",
        "\n",
        "input_ids, outputs_one_hot = create_embedding_inputs_and_outputs()"
      ],
      "metadata": {
        "id": "PasrfDz-dSqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTZOLUn0K_WF",
        "outputId": "107e8984-6d0f-42a3-b27d-dc732bab3db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "NQwC8qTCLDNw",
        "outputId": "67cca120-b502-4a75-905c-dd6ad71e0a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'F'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_ids))\n",
        "print(len(outputs_one_hot))\n",
        "print(input_ids[0])\n",
        "print(outputs_one_hot[0])\n",
        "\n",
        "print(input_ids[1])\n",
        "print(outputs_one_hot[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajj_jf18LEoo",
        "outputId": "78334724-b009-4e9c-df8b-ef4cd7ca733d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4999\n",
            "4999\n",
            "tensor(21)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "tensor(49)\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting them to tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "outputs_one_hot = torch.vstack(outputs_one_hot)\n",
        "\n",
        "# printing the shape\n",
        "print(input_ids.shape)\n",
        "print(outputs_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svk6k_piLRDq",
        "outputId": "deab88f6-8e34-4526-8940-80305a6af3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4999])\n",
            "torch.Size([4999, 53])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We are training a model to learn how to predict the\n",
        "next character given the previous character.\n",
        "\"\"\"\n",
        "\n",
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(len(chars), 5)\n",
        "        self.fc1 = nn.Linear(5, 8) # takes inchars. values and outputs 8 values\n",
        "        self.fc2 = nn.Linear(8, len(chars)) # takes in 8 values and outputs 46 values\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddedX = self.token_embedding(x)\n",
        "        out = self.fc1(embeddedX)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        self.eval() # set the model to eval mode\n",
        "        with torch.no_grad(): # disables gradient calculation\n",
        "          word = start\n",
        "          current_char = start\n",
        "          # we want to generate max_new_tokens\n",
        "          for _ in range(max_new_tokens):\n",
        "            output = self.forward(torch.tensor([chars.index(current_char)]))\n",
        "            next_char_id = torch.argmax(output).item()\n",
        "            next_char = chars[next_char_id]\n",
        "            word += next_char\n",
        "            current_char = next_char\n",
        "          return word\n",
        "\n",
        "\n",
        "\n",
        "bigram_embedding_mlp = BigramEmbeddingMLP()\n",
        "\n",
        "optimizer = torch.optim.SGD(bigram_embedding_mlp.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "bigram_embedding_mlp.train()\n",
        "\n",
        "# training loop\n",
        "for _ in range(5000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = bigram_embedding_mlp(input_ids)\n",
        "    loss = loss_fn(outputs, outputs_one_hot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if _ % 50 == 0:\n",
        "        print(f'Epoch {_ + 1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CttaGxpIK-wW",
        "outputId": "c85c1731-ede4-4a34-c167-6e7d034c175f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.001214027404785\n",
            "Epoch 51, Loss: 3.9568183422088623\n",
            "Epoch 101, Loss: 3.914968490600586\n",
            "Epoch 151, Loss: 3.8750500679016113\n",
            "Epoch 201, Loss: 3.8365707397460938\n",
            "Epoch 251, Loss: 3.798905372619629\n",
            "Epoch 301, Loss: 3.761927843093872\n",
            "Epoch 351, Loss: 3.725783109664917\n",
            "Epoch 401, Loss: 3.6902477741241455\n",
            "Epoch 451, Loss: 3.6553354263305664\n",
            "Epoch 501, Loss: 3.6211750507354736\n",
            "Epoch 551, Loss: 3.5879199504852295\n",
            "Epoch 601, Loss: 3.5557920932769775\n",
            "Epoch 651, Loss: 3.5250096321105957\n",
            "Epoch 701, Loss: 3.4957902431488037\n",
            "Epoch 751, Loss: 3.4684042930603027\n",
            "Epoch 801, Loss: 3.442491292953491\n",
            "Epoch 851, Loss: 3.4182868003845215\n",
            "Epoch 901, Loss: 3.3958003520965576\n",
            "Epoch 951, Loss: 3.374884843826294\n",
            "Epoch 1001, Loss: 3.355443000793457\n",
            "Epoch 1051, Loss: 3.337252616882324\n",
            "Epoch 1101, Loss: 3.3200581073760986\n",
            "Epoch 1151, Loss: 3.3037796020507812\n",
            "Epoch 1201, Loss: 3.28837251663208\n",
            "Epoch 1251, Loss: 3.2737858295440674\n",
            "Epoch 1301, Loss: 3.2598977088928223\n",
            "Epoch 1351, Loss: 3.2466890811920166\n",
            "Epoch 1401, Loss: 3.2341325283050537\n",
            "Epoch 1451, Loss: 3.222153902053833\n",
            "Epoch 1501, Loss: 3.210742235183716\n",
            "Epoch 1551, Loss: 3.199921131134033\n",
            "Epoch 1601, Loss: 3.189662456512451\n",
            "Epoch 1651, Loss: 3.1798741817474365\n",
            "Epoch 1701, Loss: 3.1704814434051514\n",
            "Epoch 1751, Loss: 3.1615283489227295\n",
            "Epoch 1801, Loss: 3.1529667377471924\n",
            "Epoch 1851, Loss: 3.1447250843048096\n",
            "Epoch 1901, Loss: 3.136784076690674\n",
            "Epoch 1951, Loss: 3.129179000854492\n",
            "Epoch 2001, Loss: 3.121912717819214\n",
            "Epoch 2051, Loss: 3.114903688430786\n",
            "Epoch 2101, Loss: 3.1081137657165527\n",
            "Epoch 2151, Loss: 3.101583242416382\n",
            "Epoch 2201, Loss: 3.095299243927002\n",
            "Epoch 2251, Loss: 3.089245557785034\n",
            "Epoch 2301, Loss: 3.0834105014801025\n",
            "Epoch 2351, Loss: 3.0777788162231445\n",
            "Epoch 2401, Loss: 3.0723400115966797\n",
            "Epoch 2451, Loss: 3.0670740604400635\n",
            "Epoch 2501, Loss: 3.061983823776245\n",
            "Epoch 2551, Loss: 3.057058811187744\n",
            "Epoch 2601, Loss: 3.0522892475128174\n",
            "Epoch 2651, Loss: 3.0476651191711426\n",
            "Epoch 2701, Loss: 3.0431807041168213\n",
            "Epoch 2751, Loss: 3.038825511932373\n",
            "Epoch 2801, Loss: 3.0345957279205322\n",
            "Epoch 2851, Loss: 3.030489206314087\n",
            "Epoch 2901, Loss: 3.0264899730682373\n",
            "Epoch 2951, Loss: 3.022594451904297\n",
            "Epoch 3001, Loss: 3.018796682357788\n",
            "Epoch 3051, Loss: 3.015089511871338\n",
            "Epoch 3101, Loss: 3.011467695236206\n",
            "Epoch 3151, Loss: 3.007925271987915\n",
            "Epoch 3201, Loss: 3.0044679641723633\n",
            "Epoch 3251, Loss: 3.0010807514190674\n",
            "Epoch 3301, Loss: 2.997758150100708\n",
            "Epoch 3351, Loss: 2.9944961071014404\n",
            "Epoch 3401, Loss: 2.991290807723999\n",
            "Epoch 3451, Loss: 2.9881463050842285\n",
            "Epoch 3501, Loss: 2.9850680828094482\n",
            "Epoch 3551, Loss: 2.982038736343384\n",
            "Epoch 3601, Loss: 2.9790549278259277\n",
            "Epoch 3651, Loss: 2.9761147499084473\n",
            "Epoch 3701, Loss: 2.9732179641723633\n",
            "Epoch 3751, Loss: 2.9703636169433594\n",
            "Epoch 3801, Loss: 2.9675490856170654\n",
            "Epoch 3851, Loss: 2.9647746086120605\n",
            "Epoch 3901, Loss: 2.9620492458343506\n",
            "Epoch 3951, Loss: 2.9593427181243896\n",
            "Epoch 4001, Loss: 2.956669807434082\n",
            "Epoch 4051, Loss: 2.9540300369262695\n",
            "Epoch 4101, Loss: 2.9514403343200684\n",
            "Epoch 4151, Loss: 2.9488818645477295\n",
            "Epoch 4201, Loss: 2.9463512897491455\n",
            "Epoch 4251, Loss: 2.9438490867614746\n",
            "Epoch 4301, Loss: 2.941373825073242\n",
            "Epoch 4351, Loss: 2.9389257431030273\n",
            "Epoch 4401, Loss: 2.9365038871765137\n",
            "Epoch 4451, Loss: 2.9341065883636475\n",
            "Epoch 4501, Loss: 2.931731939315796\n",
            "Epoch 4551, Loss: 2.9293811321258545\n",
            "Epoch 4601, Loss: 2.9270529747009277\n",
            "Epoch 4651, Loss: 2.924748420715332\n",
            "Epoch 4701, Loss: 2.9224655628204346\n",
            "Epoch 4751, Loss: 2.9202044010162354\n",
            "Epoch 4801, Loss: 2.91796612739563\n",
            "Epoch 4851, Loss: 2.915745258331299\n",
            "Epoch 4901, Loss: 2.913543939590454\n",
            "Epoch 4951, Loss: 2.911362648010254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigram_embedding_mlp.generate(start='a'))\n",
        "print(bigram_embedding_mlp.generate(start='b'))\n",
        "print(bigram_embedding_mlp.generate(start='c'))\n",
        "print(bigram_embedding_mlp.generate(start='d'))\n",
        "print(bigram_embedding_mlp.generate(start='e'))\n",
        "print(bigram_embedding_mlp.generate(start='f'))\n",
        "print(bigram_embedding_mlp.generate(start='u'))\n",
        "print(bigram_embedding_mlp.generate(start='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VcDLFgdPWTQ",
        "outputId": "d0182d54-cf0e-4d34-ea19-a8b47bdcaa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
            "b t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "ce t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
            "d t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "e t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "f t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "u t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "v t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "Look about the same as the one hot encoded bigram model. The loss has reduced over time.\n",
        "\n",
        "**One hot encoded bigram**:\n",
        "- Epoch 5000, Loss: 3.050889253616333\n",
        "\n",
        "**Embedding bigram**:\n",
        "- Epoch 2650, Loss: 3.0476651191711426"
      ],
      "metadata": {
        "id": "xF8aOMPZPcaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Generative Pretrained Transformer (65 points)\n",
        "\n",
        "For this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU"
      ],
      "metadata": {
        "id": "qplpM8_Cbp0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run nvidia-smi to check gpu usage\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0Oh-3FeFxxnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7045093-b84b-4e81-9057-97458776b601"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul  1 23:38:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the gpt model, let's use the full text\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "rhJAwCAOADP7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a character level tokenization function.\n",
        "\n",
        "1. Create a list of unique characters in the string. (1 points)\n",
        "2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids (1 point)\n",
        "3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string (1 point)\n"
      ],
      "metadata": {
        "id": "z_LZpvZ8AEEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = []\n",
        "\n",
        "# List of unique characters in the string\n",
        "for char in text:\n",
        "    if char not in chars:\n",
        "        chars.append(char)\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Takes in a list of characters and returns a list of ids (ints)\n",
        "    \"\"\"\n",
        "    return [chars.index(char) for char in s]\n",
        "\n",
        "def decode(ids: list[int]) -> str:\n",
        "    \"\"\"\n",
        "    Takes in a list of ids (ints) and returns a string\n",
        "    \"\"\"\n",
        "    return ''.join([chars[id] for id in ids])"
      ],
      "metadata": {
        "id": "rnEOfMj4Dk4Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of characters : {len(chars)}\")\n",
        "print(f\"First 10 characters : {chars[:10]}\")\n",
        "print(f\"Encoded text : {encode('hello')}\")\n",
        "print(f\"Decoded text : {decode(encode('hello'))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BfvXYE1rRkb",
        "outputId": "24e8853c-c91e-4767-8568-f0833e29724b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of characters : 65\n",
            "First 10 characters : ['F', 'i', 'r', 's', 't', ' ', 'C', 'z', 'e', 'n']\n",
            "Encoded text : [22, 8, 28, 28, 14]\n",
            "Decoded text : hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
        "Returns a copy of this object in CUDA memory.\n",
        "If this object is already in CUDA memory and on the correct device,\n",
        "then no copy is performed and the original object is returned."
      ],
      "metadata": {
        "id": "vk0hGPZOr47D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the input data to a tensor\n",
        "encoded_text = encode(text)\n",
        "print(f\"First 10 chars '{text[0:10]}'\")\n",
        "print(f\"encoded text first 10 chars {encoded_text[0:10]}\")\n",
        "print(f\"Total character count {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtIcyVir8Li",
        "outputId": "23a1f25f-663d-4dee-f7b3-1153d6898a47"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 chars 'First Citi'\n",
            "encoded text first 10 chars [0, 1, 2, 3, 4, 5, 6, 1, 4, 1]\n",
            "Total character count 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long).cuda()"
      ],
      "metadata": {
        "id": "1gyOaRF5Dq1P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORgMpJtwshR0",
        "outputId": "06a43556-7326-4637-badc-9a3e85681e0e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1115394])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can think of this as a window of characters that we use as the prefix to predict the next character\n",
        "block_size = 16\n",
        "data[:block_size+1] # first 17 entities in the tensor ( characters )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvWGi8Mk6x1q",
        "outputId": "075cada0-d317-4fa2-e3c0-b5e21307b163"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12,  8],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "GvO4hSK171Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here le'ts just pick the first block of size `block_size` and try to\n",
        "# visuazlie how the transformer learns to predict the next character\n",
        "# We design the system to only learn from tokens that are before the token that has to be predicted\n",
        "\n",
        "x = data[:block_size]\n",
        "y = data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVWxO6Pa70Lh",
        "outputId": "c5008a21-2a3b-4661-9081-c5023ae5c51c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([0], device='cuda:0') the target is: 1\n",
            "when input is tensor([0, 1], device='cuda:0') the target is: 2\n",
            "when input is tensor([0, 1, 2], device='cuda:0') the target is: 3\n",
            "when input is tensor([0, 1, 2, 3], device='cuda:0') the target is: 4\n",
            "when input is tensor([0, 1, 2, 3, 4], device='cuda:0') the target is: 5\n",
            "when input is tensor([0, 1, 2, 3, 4, 5], device='cuda:0') the target is: 6\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0') the target is: 1\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1], device='cuda:0') the target is: 4\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4], device='cuda:0') the target is: 1\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1], device='cuda:0') the target is: 7\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7], device='cuda:0') the target is: 8\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7, 8], device='cuda:0') the target is: 9\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7, 8, 9], device='cuda:0') the target is: 10\n",
            "when input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10],\n",
            "       device='cuda:0') the target is: 11\n",
            "when input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11],\n",
            "       device='cuda:0') the target is: 12\n",
            "when input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12],\n",
            "       device='cuda:0') the target is: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisiting some basics:\n",
        "\n",
        "Terms:\n",
        "- Block Size: The number of characters that the system has been trained to take into consideration while learning to predict the next character"
      ],
      "metadata": {
        "id": "R_tCJuSyt_7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def get_batch():\n",
        "    \"\"\"\n",
        "    This function is responsible for creating a batch of batch_size\n",
        "    For training a GPT model\n",
        "\n",
        "    \"\"\"\n",
        "    # Here we generate a tensor `ix` containing `batch_size` random\n",
        "    # indices within the range `0` to `len(data) - block_size`\n",
        "    # we substract `block_size` from the end so that the last\n",
        "    # selected block stays within the list of available characters\n",
        "    # in the text\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Here we create a stack of tensors (batch size) each of length\n",
        "    # block_size that start from the\n",
        "    # above picked random indices\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "\n",
        "    # creates the target tensor y similarly, but shifted\n",
        "    # one position to the right, representing the next\n",
        "    # character to predict for each position in x.\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "lFYZnm2MuLlt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clarifying some more terms before we proceed to the next step:\n",
        "\n",
        "### Block Size\n",
        "- **Block Size** in a GPT model training code refers to the length of each segment of the input data that each training example consists of. This is directly analogous to what you might think of as \"sequence length\" in other contexts but is specifically termed \"block size\" in training scenarios for models like GPT.\n",
        "- In the context of the Transformer attention head depicted in the image below, each \"sequence\" or input processed through the attention head would be of a fixed length equivalent to the block size. In the image, while not specified as \"block size,\" the dimension that would correspond to this term is the middle dimension of the tensors, which is 32 in your example (`(8, 32, 64)`).\n",
        "\n",
        "### Sequence Length\n",
        "- **Sequence Length** in more general contexts refers to the total length of sequences being processed, which may vary unless specifically pre-processed to be uniform. In models like Transformers (and as seen in the attention head diagram), sequence length is typically fixed to a specific size for each training or inference pass. This fixed length is crucial for attention calculations across the entire sequence uniformly.\n",
        "- In the Transformer attention head diagram, this \"sequence length\" is manifest in each stage of the attention mechanism. It represents the number of positions (or tokens) in each sequence that the model processes simultaneously, marked as the second dimension in the tensors.\n",
        "\n",
        "### How It Relates to the Diagram\n",
        "- In the attention head diagram present below, all tensors maintain a consistent second dimension (32 in this case), reflecting the fixed sequence length or block size used for the calculations. This consistent dimensionality across layers and operations ensures that each token in the sequence can be related to every other token via the attention mechanism, a key feature enabling the model to capture complex dependencies across the input.\n",
        "- The operations like matrix multiplication (`matmul`) between the transposed keys and queries, and subsequent operations like softmax and dropout, all depend on this fixed sequence length to compute the attention scores and ultimately the output sequence. This fixed length, as used in your GPT model training, allows the Transformer to utilize positional relationships effectively.\n",
        "\n",
        "### Summary\n",
        "In summary, in the Transformer model, as depicted by the attention head image below, block size and sequence length can be considered equivalent, referring to the fixed size of the input sequences used for training and inference. This term varies in usage depending on the model architecture but is crucial for models like Transformers that depend on a fixed dimension to compute relationships between all pairs of inputs within a sequence effectively.\n",
        "\n",
        "1. **Batch Size**: This is the number of samples processed before the model is updated. For exaple if we are are dealing with (8, 32, 64), the first dimension \"8\" typically represents the batch size. This means that the model processes 8 samples at a time.\n",
        "\n",
        "2. **Sequence Length**: This is the length of the input sequences each sample in a batch contains. In the example tensor, \"32\" represents the sequence length / block size, indicating each sample consists of 32 sequential elements or **tokens**. ***For instance, in natural language processing, this could represent 32 words in a sentence. In this example since we will be predicting characters, each token represents a character from the list of unique characters available.***\n",
        "\n",
        "3. **Feature Dimension**: This indicates the number of features each element of the sequence holds. The \"64\" in the example tensor suggests that each token or element of the sequence is represented by a vector of 64 features. These could be embeddings that encapsulate the token's meaning in a dense vector.\n",
        "\n",
        "4. **Block Size**: This term isn't explicitly shown in the diagram but is related to how data is structured or processed in blocks during certain operations. For instance, in memory management or in GPU computation, operations might be optimized by processing data in \"blocks.\" In the context of transformers or deep learning, block size might refer to the dimensionality of sub-parts of the model such as in splitting matrices for parallel processing, but typically it's not a term used to describe tensor dimensions directly.\n",
        "\n",
        "Thus, in the below shown transformer model diagram:\n",
        "- Batch size: 8 (number of samples processed together)\n",
        "- Sequence length / block size: 32 (number of tokens, items, or steps per sample)\n",
        "- Feature dimension: 64 (features per token or step in each sequence)"
      ],
      "metadata": {
        "id": "uYYGoQy13BEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Self Attention Head (5 points)\n",
        "![](https://i.ibb.co/GWR1XG0/head.png)"
      ],
      "metadata": {
        "id": "-HmnXJjxtm3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explaining the above artention head set up\n",
        "\n",
        "This diagram depicts the computational flow in a typical attention head of a Transformer neural network, commonly used in models like GPT and BERT. Here’s a breakdown of the operations and their significance:\n",
        "\n",
        "1. **Input Tensor**: The input tensor has the shape (8, 32, 64), where 8 represents the batch size, 32 the sequence length / block size, and 64 the feature dimension of each token in the sequence.\n",
        "\n",
        "2. **Linear Layers**: Three parallel linear transformations are applied to the input tensor. Each layer outputs a tensor of shape (8, 32, 16). These transformations typically generate the queries (Q), keys (K), and values (V) which are used in the attention mechanism.\n",
        "\n",
        "3. **Transpose Operation**: The output of one of the linear layers (presumably representing keys, K) is transposed to change its shape from (8, 32, 16) to (8, 16, 32). This operation is necessary for matrix multiplication with the queries (Q). Here we notice that the transpose is only done across the second and third dimensions as the first dimension only represents the batch size\n",
        "\n",
        "4. **Matrix Multiplication (matmul)**: The output of the transpose operation (K transposed) is matrix-multiplied with another linear output (Q). ***This results in a shape of (8, 32, 32), representing the raw attention scores before they are normalized.***\n",
        "\n",
        "5. **Multiplication (mul)**: This operation might be an element-wise multiplication used as part of scaling the attention scores by the square root of the dimension of the keys to stabilize gradients during training, although the typical square root scaling is not explicitly shown here. More on why we scale the attention values below ...\n",
        "\n",
        "6. **Masked Fill**: This operation is used to apply masks to the attention scores. Masks are often used to ignore (or mask) padding tokens or future tokens during training in sequence models. The operation doesn't change the shape of the tensor. The upper right half of the matrix is made as zeros in this step.\n",
        "\n",
        "7. **Softmax**: The softmax function is applied across the last dimension (32) to normalize the attention scores to a probability distribution.\n",
        "\n",
        "8. **Dropout**: Dropout is a regularization technique where random elements of the tensor are zeroed out during training to prevent overfitting. The shape remains unchanged.\n",
        "\n",
        "9. **Matrix Multiplication (matmul)**: The normalized and possibly masked attention scores are then matrix-multiplied with the third linear output (V, values), resulting in an output tensor of shape (8, 32, 16). This operation computes the weighted sum of the values based on the attention scores.\n",
        "\n",
        "10. **Output Tensor**: The final output tensor is generated with the shape (8, 32, 16), likely to be fed into subsequent layers of the Transformer or processed further depending on the specific architecture and task.\n",
        "\n",
        "This detailed flow illustrates how attention mechanisms selectively focus on different parts of the input sequence, weighting input features by relevance, which is central to the success of Transformer models in handling various sequence-based tasks in natural language processing."
      ],
      "metadata": {
        "id": "2XNdlBRl2VZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The scaling of the attention scores\n",
        "\n",
        "The scaling of the attention scores based on the dimension of the keys in the Transformer architecture addresses a specific challenge in training deep learning models that use softmax to calculate probabilities.\n",
        "\n",
        "### Background on Dot Products and Their Scale\n",
        "The attention mechanism computes the dot products between the query and all keys in the sequence. These dot products are a critical component because they determine the attention scores that indicate how much each part of the input should contribute to the output at each position. However, the magnitude of the dot products depends on the dimensionality of the keys and queries. Here's why:\n",
        "\n",
        "- The dot product of two vectors increases with the number of dimensions. Specifically, if each component of the vectors is drawn from a distribution with a constant variance, the variance of the dot product is proportional to the dimensionality of the vectors.\n",
        "- As the dimension of the keys (and queries, since they are usually of the same dimension) increases, the average value of the dot products becomes larger. This can lead to extremely large values, especially when working with high-dimensional data, which is common in models like Transformers.\n",
        "\n",
        "### Impact on Softmax\n",
        "The softmax function, which is used to convert these dot products into probabilities (or attention scores), is highly sensitive to changes in input values. Specifically:\n",
        "- Large values in the softmax input can lead to a situation where the softmax function's output is close to zero for all inputs except the largest one (a phenomenon often referred to as the softmax function \"saturating\"). This saturation can significantly slow down learning, as it leads to very small gradients during backpropagation — essentially, the network is less able to learn from the input data.\n",
        "\n",
        "### Why Scale by Square Root of Dimension?\n",
        "The scaling factor used, \\(\\sqrt{d_k}\\) (where \\(d_k\\) is the dimension of the keys), helps mitigate these effects:\n",
        "- **Normalization**: By dividing the dot products by \\(\\sqrt{d_k}\\), you effectively normalize them, bringing their variance back to a more manageable scale. This normalization helps maintain a more uniform scale across different model sizes and configurations.\n",
        "- **Gradient Stability**: By keeping the dot products (and thus the inputs to the softmax) at a reasonable magnitude, the scaling prevents gradients under the softmax from becoming too small. This is crucial for efficient learning, as it ensures that each update step during training is informative enough to guide the model towards better performance without being too noisy or too minimal.\n",
        "\n",
        "### Conclusion\n",
        "Scaling by the square root of the dimension of the keys is a practical approach to ensuring that the attention mechanism operates effectively across different settings and model scales, facilitating stable and efficient training. This method is particularly vital in deep learning architectures like Transformers, where models often deal with high-dimensional data and require careful handling of numerical stability during training operations."
      ],
      "metadata": {
        "id": "jjNgfYPJ8VwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Terms relevant to constructing the SelfAttention Head:\n",
        "\n",
        "In the context of Transformer architectures, the **head size** in an attention head refers to the dimension of the vectors used for each of the queries (Q), keys (K), and values (V) within a single attention head. This is a key parameter that defines how much information each attention head can capture.\n",
        "\n",
        "### Definition and Calculation\n",
        "\n",
        "- **Head Size**: The head size is essentially the dimensionality of the Q, K, and V vectors within each specific attention head. It is typically derived by dividing the total dimension of the model's embeddings (\\(d_{\\text{model}}\\)) by the number of attention heads (\\(\\text{num\\_heads}\\)). This allows the model to distribute the embedding information across multiple heads, each focusing on different features or relationships in the data.\n",
        "\n",
        "### Formula\n",
        "The head size for queries and keys (\\(d_k\\) and \\(d_q\\)) is often the same and can be calculated as:\n",
        "\\[ d_k = d_q = \\frac{d_{\\text{model}}}{\\text{num\\_heads}} \\]\n",
        "For values (\\(d_v\\)), it is usually the same as \\(d_k\\) and \\(d_q\\), though this can vary depending on specific model architectures or design choices:\n",
        "\\[ d_v = \\frac{d_{\\text{model}}}{\\text{num\\_heads}} \\]\n",
        "\n",
        "### Example\n",
        "If a Transformer model uses an embedding dimension (\\(d_{\\text{model}}\\)) of 512 and has 8 attention heads:\n",
        "\\[ d_k = d_q = d_v = \\frac{512}{8} = 64 \\]\n",
        "Thus, each head processes vectors of size 64 for queries, keys, and values.\n",
        "\n",
        "### Importance\n",
        "The choice of head size affects how finely the model can focus on different aspects of the input data. Each head can potentially learn to attend to different parts of the sequence or different types of relationships:\n",
        "- **Smaller head sizes** can lead to a more focused and granular attention mechanism, where each head might specialize more distinctly.\n",
        "- **Larger head sizes** provide more capacity to each head, which can be useful for capturing more complex patterns or dependencies, but may reduce the diversity of what different heads can learn.\n",
        "\n",
        "Adjusting the head size is a balance between computational efficiency, capacity, and the diversity of information that the attention heads can capture. It's an important aspect of model tuning, especially in tasks requiring nuanced understanding of context or relationships within the data.\n",
        "\n",
        "### How does this translate to code\n",
        "\n",
        "Following this example: If a Transformer model uses an embedding dimension (\\(d_{\\text{model}}\\)) of 512 and has 8 attention heads:\n",
        "\\[ d_k = d_q = d_v = \\frac{512}{8} = 64 \\]\n",
        "Thus, each head processes vectors of size 64 for queries, keys, and values.\n",
        "\n",
        "Here each layer q, k and v, instead of processing all the tokens that are a part of the embedding, will only process tokens that are passed into this head.\n",
        "\n",
        "Hence the q, k and v linear layers will be of shape batch_size x head_size\n"
      ],
      "metadata": {
        "id": "oc_Nwm3zMuUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Mask\n",
        "\n",
        "The code snippet provided below involves creating a triangular mask and then applying it to an attention matrix in a Transformer model, typically used in tasks like text processing or sequence modeling. Let’s break down the two lines to understand what’s happening:\n",
        "\n",
        "### Line 1: Creating the Mask\n",
        "```python\n",
        "mask = torch.tril(torch.ones(timesteps, timesteps))\n",
        "```\n",
        "- **`torch.ones(timesteps, timesteps)`**: This function creates a 2D tensor (square matrix) filled with the value `1`, where the dimensions of the matrix are both `timesteps`. `timesteps` could be the length of a sequence being processed, such as the number of words in a sentence.\n",
        "- **`torch.tril()`**: This function takes a tensor and returns a lower triangular part of the matrix. It zeroes out all elements above the main diagonal. The main diagonal and the elements below remain as they were, which in this case, are all `1`s due to the `torch.ones()` function. This triangular matrix is typically used in attention mechanisms to ensure that the attention calculation for a given timestep only considers that timestep and the ones before it (i.e., ensuring causality in models like GPT).\n",
        "\n",
        "### Line 2: Applying the Mask to the Attention Matrix\n",
        "```python\n",
        "masked_attention = attention.masked_fill(mask == 0, float('-inf'))\n",
        "```\n",
        "- **`mask == 0`**: This operation compares each element of the `mask` tensor to `0`. Since `mask` is a lower triangular matrix with `1`s in the lower triangle and `0`s elsewhere, this operation generates a Boolean tensor where `True` corresponds to the positions where the mask had `0`s (i.e., the upper triangular part of the matrix) and `False` everywhere else (i.e., the lower triangular part).\n",
        "- **`masked_fill()`**: This method is called on the `attention` tensor. It takes two arguments: a mask and a value to fill. The mask here is the Boolean tensor from `mask == 0`. Wherever the mask is `True`, the `attention` tensor is filled with `float('-inf')`. This effectively applies the mask by setting the attention scores in the upper triangle (those that should not be considered due to causality) to negative infinity.\n",
        "\n",
        "### Why `float('-inf')`?\n",
        "In attention mechanisms, especially when followed by a softmax operation, setting values to negative infinity before softmax ensures that those values have zero probability. When softmax is applied to a vector containing negative infinity, the exponential of negative infinity is zero, hence those positions do not contribute to the output of the softmax.\n",
        "\n",
        "### Summary\n",
        "- The `mask` tensor is used to enforce causality in the attention mechanism by preventing the model from attending to future timesteps in the sequence. This is essential in models like GPT where predictions for a given position should only depend on previous positions.\n",
        "- The `mask == 0` operation identifies positions that should be ignored (in this context, future timesteps), and `masked_fill` applies this by setting such positions in the attention matrix to negative infinity, effectively removing them from consideration during attention normalization (softmax)."
      ],
      "metadata": {
        "id": "dk6--KJhSp6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  \"\"\"\n",
        "  This class implements a single self attention head\n",
        "  For the input dimensions we have batch size , sequence length , feature dimension\n",
        "\n",
        "  First we need to implement the K, Q and V layers\n",
        "  These are three linear layers - we can refer to\n",
        "  https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "\n",
        "  For a\n",
        "  \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    self.k = nn.Linear(batch_size, head_size, bias=False)\n",
        "    self.q = nn.Linear(batch_size, head_size, bias=False)\n",
        "    self.v = nn.Linear(batch_size, head_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The forward step contains the following steps\n",
        "    1. Pass x through the linear layers\n",
        "    2. Perform transpose operation on k\n",
        "    3. Calculate the bidirectional attention value q.k\n",
        "    4. Scaling the attention value\n",
        "    5. Masked fil\n",
        "    6. Softmax\n",
        "    7. Dropout\n",
        "    8. Matrix multiplication with v\n",
        "\n",
        "    Batch, Tokens, Chanels\n",
        "    \"\"\"\n",
        "    B, T, C = x.shape\n",
        "    k = self.k(x)\n",
        "    q = self.q(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    k = k.transpose(1, 2) # Here we transpose dimensions 1 and 2\n",
        "    # k is now of dimension B, C, T\n",
        "\n",
        "    # Here we calculate the bidirectional attention value q.k\n",
        "    attention = torch.matmul(q, k) # B, T, C * B, C, T = B, T, T\n",
        "\n",
        "    # Scaling the attention value by the sq root of the chanels / features\n",
        "    attention = attention * C**-0.5 # this will be of dimension B, T, T\n",
        "\n",
        "    # Masked fill\n",
        "    mask = torch.tril(torch.ones(T, T)) # We create a mask of dimensions T, T\n",
        "    # We apply the mask to the attention matrix\n",
        "    # float('-inf') is applied to all positions where the mask value is 0\n",
        "    masked_attention = attention.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    # Softmax\n",
        "    # the dimension value has to be set to -1 to indicate the last dimension\n",
        "    attention = F.softmax(masked_attention, dim=-1)\n",
        "\n",
        "    # Dropout\n",
        "    attention = F.dropout(attention, p=0.1)\n",
        "\n",
        "    # Matrix multiplication with v\n",
        "    output = torch.matmul(attention, v)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "5SD8Z16R-sfZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Self Attention (5 points)\n",
        "\n",
        "`constructor`\n",
        "\n",
        "- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n",
        "- Create a linear layer with n_embd input dim and n_embd output dim\n",
        "\n",
        "`forward`\n",
        "\n",
        "In the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n",
        "\n",
        "![](https://i.ibb.co/y5SwyZZ/multihead.png)"
      ],
      "metadata": {
        "id": "LWeoHGBiFpWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "gFsPDkpnFs_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP (2 points)\n",
        "Implement a 2 layer MLP\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/C0DtrF5/ff.png)"
      ],
      "metadata": {
        "id": "uH_0ELyZ8YCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # implement\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        # implement\n",
        "        pass"
      ],
      "metadata": {
        "id": "K96Z3kAv7lNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer block (20 points)\n",
        "\n",
        "Layer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n",
        "\n",
        "Dropout is a form of regularization to prevent overfitting.\n",
        "\n",
        "This is the diagram of a transformer block:\n",
        "\n",
        "![](https://i.ibb.co/X85C473/block.png)"
      ],
      "metadata": {
        "id": "bUFxuyf-JIxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "xTDAd66KIvvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT\n",
        "\n",
        "`constructor` (5 points)\n",
        "\n",
        "1. create the token embedding table and the position embedding table\n",
        "2. create variable `self.blocks` that is a series of 4 `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n",
        "3. create a layer norm layer\n",
        "4. create a linear layer for predicting the next token\n",
        "\n",
        "`forward(self, idx, targets=None)`. (5 points)\n",
        "\n",
        "`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n",
        "1. get the token by using the token embedding table created in the constructor\n",
        "2. create the position embeddings\n",
        "3. sum the token and position embeddings to get the model input\n",
        "4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "5. compute the loss\n",
        "\n",
        "`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` (5 points)\n",
        "1. implement top p, top_k, and temperature for sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "SyFQXltDKNti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)"
      ],
      "metadata": {
        "id": "0Xa2bh2XDdKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        pass\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        pass\n",
        "\n",
        "    def generate(self, start_char, max_new_tokens, top_p, top_k, temperature):\n",
        "        pass"
      ],
      "metadata": {
        "id": "8WT4oUN084ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop (15 points)\n",
        "\n",
        "implement training loop"
      ],
      "metadata": {
        "id": "Njzrwwiv-mfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT().to('cuda') # make you are running this on the GPU\n",
        "max_iters = 5000\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    pass"
      ],
      "metadata": {
        "id": "qWtn2uTwYUrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text\n",
        "\n",
        "\n",
        "print some text that your model generates"
      ],
      "metadata": {
        "id": "zy3v8Nv7YVUa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5l4soWviWG5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}