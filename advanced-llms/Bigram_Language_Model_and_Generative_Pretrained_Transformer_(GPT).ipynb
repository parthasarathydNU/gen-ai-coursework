{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathydNU/gen-ai-coursework/blob/main/advanced-llms/Bigram_Language_Model_and_Generative_Pretrained_Transformer_(GPT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Bigram Language Model and Generative Pretrained Transformer (GPT)\n",
        "\n",
        "\n",
        "The objective of this assignment is to train a simplified transformer model. The primary differences between the implementation:\n",
        "* tokenizer (we use a character level encoder simplicity and compute constraints)\n",
        "* size (we are using 1 consumer grade gpu hosted on colab and a small dataset. in practice, the models are much larger and are trained on much more data)\n",
        "* efficiency\n",
        "\n",
        "\n",
        "Most modern LLMs have multiple training stages, so we won't get a model that is capable of replying to you yet. However, this is the first step towards a model like ChatGPT and Llama.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fYCVVv_1A1kZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F097yaiu7dXQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Bigram MLP for TinyShakespeare (35 points)\n",
        "\n",
        "1a) (1 point). Create a list `chars` that contains all unique characters in `text`\n",
        "\n",
        "1b) (2 points). Implement `encode(s: str) -> list[int]`\n",
        "\n",
        "1c) (2 points). Implement `decode(ids: list[int]) -> str`\n",
        "\n",
        "1d) (5 points). Create two tensors, `inputs_one_hot` and `outputs_one_hot`. Use one hot encoding. Make sure to get every consecutive pair of characters. For example, for the word 'hello', we should create the following input-output pairs\n",
        "```\n",
        "he\n",
        "el\n",
        "ll\n",
        "lo\n",
        "```\n",
        "\n",
        "1e) (10 points). Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token. Specifically, implement the constructor, forward, and generate. The output dimension of the first layer should be 8. Use `torch.optim`. The activation function for the first layer should be `nn.LeakyReLU()`\n",
        "\n",
        "Note: Use the `torch.nn.function.cross_entropy` loss. Read the [docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) about how this loss function works. The logits are the output of a network WITHOUT an activation function applied to the last layer. There are activation functions are applied to every layer except the last.\n",
        "\n",
        "1f) (5 points). Train the BigramOneHotMLP for 1000 steps.\n",
        "\n",
        "1g) (5 points). Create two tensors, `input_ids` and `outputs_one_hot`. These `input_ids` will be used for the embedding layer.\n",
        "\n",
        "1h) (5 points). Implement and train BigramEmbeddingMLP, a 2 layer mlp that predicts the next token. Specifically, implement the constructor, forward, and generate functions. The output dimension of the first layer should be 8. Use `torch.optim`.\n",
        "\n",
        "\n",
        "\n",
        "Note: the output will look like gibberish\n"
      ],
      "metadata": {
        "id": "8qra06Ema_VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iradmn7bZtM",
        "outputId": "6cfaacea-b655-40c6-a3ad-b90577a2c21a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-06 23:32:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-06 23:32:10 (29.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the bigram model, let's use the first 1000 characters for the data\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text = text[:5000]"
      ],
      "metadata": {
        "id": "pLoVi294G-T-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "NRUazcXhTC_G",
        "outputId": "4bd71a06-9244-4031-801a-8b79c59798b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not maliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was for his country he did it to\\nplease his mother and to be partly proud; which he\\nis, even till the altitude of his virtue.\\n\\nSecond Citizen:\\nWhat he cannot help in his nature, you account a\\nvice in him. You must in no way say he is covetous.\\n\\nFirst Citizen:\\nIf I must not, I need not be barren of accusations;\\nhe hath faults, with surplus, to tire in repetition.\\nWhat shouts are these? The other side o' the city\\nis risen: why stay we prating here? to the Capitol!\\n\\nAll:\\nCome, come.\\n\\nFirst Citizen:\\nSoft! who comes here?\\n\\nSecond Citizen:\\nWorthy Menenius Agrippa; one that hath always loved\\nthe people.\\n\\nFirst Citizen:\\nHe's one honest enough: would all the rest were so!\\n\\nMENENIUS:\\nWhat work's, my countrymen, in hand? where go you\\nWith bats and clubs? The matter? speak, I pray you.\\n\\nFirst Citizen:\\nOur business is not unknown to the senate; they have\\nhad inkling this fortnight what we intend to do,\\nwhich now we'll show 'em in deeds. They say poor\\nsuitors have strong breaths: they shall know we\\nhave strong arms too.\\n\\nMENENIUS:\\nWhy, masters, my good friends, mine honest neighbours,\\nWill you undo yourselves?\\n\\nFirst Citizen:\\nWe cannot, sir, we are undone already.\\n\\nMENENIUS:\\nI tell you, friends, most charitable care\\nHave the patricians of you. For your wants,\\nYour suffering in this dearth, you may as well\\nStrike at the heaven with your staves as lift them\\nAgainst the Roman state, whose course will on\\nThe way it takes, cracking ten thousand curbs\\nOf more strong link asunder than can ever\\nAppear in your impediment. For the dearth,\\nThe gods, not the patricians, make it, and\\nYour knees to them, not arms, must help. Alack,\\nYou are transported by calamity\\nThither where more attends you, and you slander\\nThe helms o' the state, who care for you like fathers,\\nWhen you curse them as enemies.\\n\\nFirst Citizen:\\nCare for us! True, indeed! They ne'er cared for us\\nyet: suffer us to famish, and their store-houses\\ncrammed with grain; make edicts for usury, to\\nsupport usurers; repeal daily any wholesome act\\nestablished against the rich, and provide more\\npiercing statutes daily, to chain up and restrain\\nthe poor. If the wars eat us not up, they will; and\\nthere's all the love they bear us.\\n\\nMENENIUS:\\nEither you must\\nConfess yourselves wondrous malicious,\\nOr be accused of folly. I shall tell you\\nA pretty tale: it may be you have heard it;\\nBut, since it serves my purpose, I will venture\\nTo stale 't a little more.\\n\\nFirst Citizen:\\nWell, I'll hear it, sir: yet you must not think to\\nfob off our disgrace with a tale: but, an 't please\\nyou, deliver.\\n\\nMENENIUS:\\nThere was a time when all the body's members\\nRebell'd against the belly, thus accused it:\\nThat only like a gulf it did remain\\nI' the midst o' the body, idle and unactive,\\nStill cupboarding the viand, never bearing\\nLike labour with the rest, where the other instruments\\nDid see and hear, devise, instruct, walk, feel,\\nAnd, mutually participate, did minister\\nUnto the appetite and affection common\\nOf the whole body. The belly answer'd--\\n\\nFirst Citizen:\\nWell, sir, what answer made the belly?\\n\\nMENENIUS:\\nSir, I shall tell you. With a kind of smile,\\nWhich ne'er came from the lungs, but even thus--\\nFor, look you, I may make the belly smile\\nAs well as speak--it tauntingly replied\\nTo the discontented members, the mutinous parts\\nThat envied his receipt; even so most fitly\\nAs you malign our senators for that\\nThey are not such as you.\\n\\nFirst Citizen:\\nYour belly's answer? What!\\nThe kingly-crowned head, the vigilant eye,\\nThe counsell\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique characters from the text\n",
        "# To have unique values, we can use the set data structure\n",
        "# Reference https://stackoverflow.com/questions/13902805/list-of-all-unique-characters-in-a-string\n",
        "chars = list(set(text))"
      ],
      "metadata": {
        "id": "hoMGZgEOdRjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjjroOrYT8pv",
        "outputId": "1ba12d04-ab7f-4766-f504-33319f080ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We have 46 unique characters"
      ],
      "metadata": {
        "id": "wYWOOhgrT_nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode(string: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Given a string, encode returns a list of integers that represent the characters\n",
        "    in the string.\n",
        "    \"\"\"\n",
        "    encodedChars = []\n",
        "\n",
        "    for s in string:\n",
        "      encodedChars.append(chars.index(s))\n",
        "    return encodedChars\n",
        "\n",
        "def decode(ids: list[int]) -> str:\n",
        "    \"\"\"\n",
        "    Given a list of integers, decode returns the characters in the list as a string.\n",
        "    \"\"\"\n",
        "    decodedChars = [];\n",
        "    for id in ids:\n",
        "      decodedChars.append(chars[id])\n",
        "    return \"\".join(decodedChars)\n",
        "\n",
        "# Testing the encode and decode functions\n",
        "\n",
        "encoded = encode('hello')\n",
        "decoded = decode(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKdvB6FYTr85",
        "outputId": "827d0257-b4f2-4111-8302-728de562616c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14, 21, 3, 3, 29]\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "def create_one_hot_inputs_and_outputs(text: str) -> list[torch.tensor, torch.tensor]:\n",
        "    \"\"\"\n",
        "    For a given word we need to generate all pairs of consecutive characters.\n",
        "    For example, for the word 'hello', we should create the following input-output pairs\n",
        "    he\n",
        "    el\n",
        "    ll\n",
        "    lo\n",
        "\n",
        "    Additionally, we need to conver the input and output to one hot\n",
        "    encoded tensors.\n",
        "\n",
        "    Here we have 46 unique characters, so we take the input of shape (1)\n",
        "    And return an output of shape (1,46) for each character\n",
        "\n",
        "    Say we have the word hello, we need to return the following tensors\n",
        "    [one_hot_encoded_h][one_hot_encoded_e]\n",
        "    [one_hot_encoded_e][one_hot_encoded_l]\n",
        "    [one_hot_encoded_l][one_hot_encoded_l]\n",
        "    [one_hot_encoded_l][one_hot_encoded_o]\n",
        "\n",
        "    All the tensors should be of shape (1,46)\n",
        "    And all the tensors on the left, go into inputs_one_hot\n",
        "    And all the tensors on the right, go into outputs_one_hot\n",
        "    \"\"\"\n",
        "    inputs_one_hot = []\n",
        "    outputs_one_hot = []\n",
        "\n",
        "    # we know which index does each character fall into\n",
        "    # usign the encode method\n",
        "\n",
        "    for i in range(len(text) - 1):\n",
        "      input_char = text[i]\n",
        "      output_char = text[i+1]\n",
        "\n",
        "      # One-hot encoding the input character\n",
        "      input_one_hot = torch.zeros(1, len(chars))\n",
        "      input_one_hot[0][encode(input_char)] = 1\n",
        "      inputs_one_hot.append(input_one_hot)\n",
        "\n",
        "      # One-hot encoding the output character\n",
        "      output_one_hot = torch.zeros(1, len(chars))\n",
        "      output_one_hot[0][encode(output_char)] = 1\n",
        "      outputs_one_hot.append(output_one_hot)\n",
        "\n",
        "    return inputs_one_hot, outputs_one_hot\n",
        "\n",
        "# Example usage\n",
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs(text=\"hi\")\n",
        "print(inputs_one_hot)\n",
        "print(outputs_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taYatNwPULUP",
        "outputId": "4a6491e3-1a01-4b09-df20-77d6f7f84e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n",
            "[tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs(text)"
      ],
      "metadata": {
        "id": "RIa-EYb3kLRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(inputs_one_hot))\n",
        "print(len(outputs_one_hot))\n",
        "print(inputs_one_hot[0].shape)\n",
        "print(outputs_one_hot[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdyulNPQlWuf",
        "outputId": "5518a93b-73b0-421e-97f7-aaa4322a9b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4999\n",
            "4999\n",
            "torch.Size([1, 53])\n",
            "torch.Size([1, 53])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We have one hot encoded both the input and output combinations. And each entry is of shape (1,46)"
      ],
      "metadata": {
        "id": "9TbMifdultnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token.\n",
        "  Specifically, implement the constructor, forward, and generate.\n",
        "  The output dimension of the first layer should be 8. Use torch.optim.\n",
        "  The activation function for the first layer should be nn.LeakyReLU()\n",
        "  Note: Use the torch.nn.function.cross_entropy loss.\n",
        "  Read the docs about how this loss function works.\n",
        "  The logits are the output of a network WITHOUT an activation\n",
        "  function applied to the last layer. There are activation functions\n",
        "  are applied to every layer except the last.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BigramOneHotMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # Calls the init function on the nn.Module class\n",
        "        # 2 Layer mlp that predicts the next token\n",
        "        self.fc1 = nn.Linear(len(chars), 8) # takes inchars. values and outputs 8 values\n",
        "        self.fc2 = nn.Linear(8, len(chars)) # takes in 8 values and outputs 46 values\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [*, 5] batch size: *, 46 dimension (one hot)\n",
        "        out = self.fc1(x) # [*, 8]\n",
        "        out = F.leaky_relu(out) # [*, 8] Activation function of the first layer is leaky relu\n",
        "        out = self.fc2(out) # [*, 46] Final output values, gives a one hot encoded value of the output character\n",
        "        return out\n",
        "\n",
        "bigram_one_hot_mlp = BigramOneHotMLP()"
      ],
      "metadata": {
        "id": "oMbHFp3NlVCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the loss function and optimizer\n",
        "\n",
        "Reference : https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy\n",
        "\n",
        "```python\n",
        "torch.nn.functional.cross_entropy(\n",
        "  input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0\n",
        ")\n",
        "```\n",
        "\n",
        "Parameters\n",
        "- input (Tensor) – Predicted unnormalized logits;\n",
        "- target (Tensor) – Ground truth class indices or class probabilities;\n",
        "- reduction (str, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. **bold text** Default: 'mean'\n",
        "\n",
        "```python\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "```"
      ],
      "metadata": {
        "id": "YyKH6cgK2a2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "chnRG8Zk3_uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to training mode\n",
        "bigram_one_hot_mlp.train()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = torch.optim.SGD(bigram_one_hot_mlp.parameters(), lr=0.01)\n",
        "\n",
        "# Compute the cross entropy loss between input logits and target.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# We already have the inputs and outputs defined as one hot encoded\n",
        "# We just need to convert them to float so that values flow smoothly through the model\n",
        "\n",
        "inputs_one_hot = torch.vstack(inputs_one_hot).float()\n",
        "outputs_one_hot = torch.vstack(outputs_one_hot).float()"
      ],
      "metadata": {
        "id": "lELdqEOX2Eza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_one_hot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR6uU3Zm7Bpk",
        "outputId": "8b3244d0-c3a5-4ba6-97fd-d5c7ed2b0b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4999, 53])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_indices = torch.argmax(outputs_one_hot, dim=1)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(5000):\n",
        "  # Flush the gradients from prev iteration\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # forward pass\n",
        "  outputs = bigram_one_hot_mlp(inputs_one_hot)\n",
        "  loss = loss_fn(outputs, target_indices)\n",
        "\n",
        "  # backward pass and updatin the weights\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(epoch + 1) % 50 == 0:\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "029YpQRv7Ama",
        "outputId": "a406ab91-b854-4b15-fa5f-2180b511ce14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss: 3.289602756500244\n",
            "Epoch 100, Loss: 3.2819199562072754\n",
            "Epoch 150, Loss: 3.2746875286102295\n",
            "Epoch 200, Loss: 3.2678771018981934\n",
            "Epoch 250, Loss: 3.261469602584839\n",
            "Epoch 300, Loss: 3.2554359436035156\n",
            "Epoch 350, Loss: 3.2497506141662598\n",
            "Epoch 400, Loss: 3.2443928718566895\n",
            "Epoch 450, Loss: 3.2393407821655273\n",
            "Epoch 500, Loss: 3.234571933746338\n",
            "Epoch 550, Loss: 3.230064868927002\n",
            "Epoch 600, Loss: 3.225799798965454\n",
            "Epoch 650, Loss: 3.221761465072632\n",
            "Epoch 700, Loss: 3.2179362773895264\n",
            "Epoch 750, Loss: 3.2143027782440186\n",
            "Epoch 800, Loss: 3.2108397483825684\n",
            "Epoch 850, Loss: 3.207523822784424\n",
            "Epoch 900, Loss: 3.2043662071228027\n",
            "Epoch 950, Loss: 3.2013511657714844\n",
            "Epoch 1000, Loss: 3.1984596252441406\n",
            "Epoch 1050, Loss: 3.1956849098205566\n",
            "Epoch 1100, Loss: 3.1930253505706787\n",
            "Epoch 1150, Loss: 3.190463066101074\n",
            "Epoch 1200, Loss: 3.187995672225952\n",
            "Epoch 1250, Loss: 3.1856091022491455\n",
            "Epoch 1300, Loss: 3.1832964420318604\n",
            "Epoch 1350, Loss: 3.1810507774353027\n",
            "Epoch 1400, Loss: 3.1788628101348877\n",
            "Epoch 1450, Loss: 3.176730155944824\n",
            "Epoch 1500, Loss: 3.174649477005005\n",
            "Epoch 1550, Loss: 3.1726152896881104\n",
            "Epoch 1600, Loss: 3.170623779296875\n",
            "Epoch 1650, Loss: 3.1686692237854004\n",
            "Epoch 1700, Loss: 3.166749954223633\n",
            "Epoch 1750, Loss: 3.1648621559143066\n",
            "Epoch 1800, Loss: 3.162994623184204\n",
            "Epoch 1850, Loss: 3.161135196685791\n",
            "Epoch 1900, Loss: 3.15929913520813\n",
            "Epoch 1950, Loss: 3.157482147216797\n",
            "Epoch 2000, Loss: 3.1556835174560547\n",
            "Epoch 2050, Loss: 3.1539063453674316\n",
            "Epoch 2100, Loss: 3.152146577835083\n",
            "Epoch 2150, Loss: 3.150402545928955\n",
            "Epoch 2200, Loss: 3.1486713886260986\n",
            "Epoch 2250, Loss: 3.1469509601593018\n",
            "Epoch 2300, Loss: 3.145240068435669\n",
            "Epoch 2350, Loss: 3.143537998199463\n",
            "Epoch 2400, Loss: 3.141843557357788\n",
            "Epoch 2450, Loss: 3.140155076980591\n",
            "Epoch 2500, Loss: 3.1384713649749756\n",
            "Epoch 2550, Loss: 3.1367881298065186\n",
            "Epoch 2600, Loss: 3.1351091861724854\n",
            "Epoch 2650, Loss: 3.133434295654297\n",
            "Epoch 2700, Loss: 3.1317615509033203\n",
            "Epoch 2750, Loss: 3.130089282989502\n",
            "Epoch 2800, Loss: 3.1284165382385254\n",
            "Epoch 2850, Loss: 3.1267476081848145\n",
            "Epoch 2900, Loss: 3.1250648498535156\n",
            "Epoch 2950, Loss: 3.1233773231506348\n",
            "Epoch 3000, Loss: 3.121689558029175\n",
            "Epoch 3050, Loss: 3.1200008392333984\n",
            "Epoch 3100, Loss: 3.1183111667633057\n",
            "Epoch 3150, Loss: 3.116621732711792\n",
            "Epoch 3200, Loss: 3.114932060241699\n",
            "Epoch 3250, Loss: 3.113241195678711\n",
            "Epoch 3300, Loss: 3.111546516418457\n",
            "Epoch 3350, Loss: 3.109848976135254\n",
            "Epoch 3400, Loss: 3.1081430912017822\n",
            "Epoch 3450, Loss: 3.1064329147338867\n",
            "Epoch 3500, Loss: 3.1047186851501465\n",
            "Epoch 3550, Loss: 3.1030004024505615\n",
            "Epoch 3600, Loss: 3.10127854347229\n",
            "Epoch 3650, Loss: 3.0995519161224365\n",
            "Epoch 3700, Loss: 3.0978243350982666\n",
            "Epoch 3750, Loss: 3.0960917472839355\n",
            "Epoch 3800, Loss: 3.0943543910980225\n",
            "Epoch 3850, Loss: 3.0926120281219482\n",
            "Epoch 3900, Loss: 3.0908639430999756\n",
            "Epoch 3950, Loss: 3.0891098976135254\n",
            "Epoch 4000, Loss: 3.087350606918335\n",
            "Epoch 4050, Loss: 3.085585832595825\n",
            "Epoch 4100, Loss: 3.0838141441345215\n",
            "Epoch 4150, Loss: 3.0820369720458984\n",
            "Epoch 4200, Loss: 3.0802528858184814\n",
            "Epoch 4250, Loss: 3.0784645080566406\n",
            "Epoch 4300, Loss: 3.076672077178955\n",
            "Epoch 4350, Loss: 3.074873208999634\n",
            "Epoch 4400, Loss: 3.073068380355835\n",
            "Epoch 4450, Loss: 3.071258306503296\n",
            "Epoch 4500, Loss: 3.0694427490234375\n",
            "Epoch 4550, Loss: 3.0676207542419434\n",
            "Epoch 4600, Loss: 3.0657920837402344\n",
            "Epoch 4650, Loss: 3.063955545425415\n",
            "Epoch 4700, Loss: 3.0621109008789062\n",
            "Epoch 4750, Loss: 3.0602595806121826\n",
            "Epoch 4800, Loss: 3.058401584625244\n",
            "Epoch 4850, Loss: 3.0565342903137207\n",
            "Epoch 4900, Loss: 3.054659605026245\n",
            "Epoch 4950, Loss: 3.0527777671813965\n",
            "Epoch 5000, Loss: 3.050889253616333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start='a', max_new_tokens=5) -> str:\n",
        "    \"\"\"\n",
        "    Generate text given a starting point\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval() # set the model to eval mode\n",
        "    with torch.no_grad(): # disables gradient calculation\n",
        "\n",
        "      word = start;\n",
        "      currentChar = start;\n",
        "      for _ in range(max_new_tokens):\n",
        "\n",
        "        # We one hot encode the current character\n",
        "        input_one_hot = torch.zeros(1, len(chars))\n",
        "        input_one_hot[0][encode(currentChar)] = 1\n",
        "\n",
        "\n",
        "        # Pass this through the model to get the probablility distribution\n",
        "        # of the next char in the same shape as the input\n",
        "        output = model.forward(input_one_hot) # [1, 46] Eg: [0.06, 0.01, 0.5, .....]\n",
        "\n",
        "        # Get the index that has the max value\n",
        "        # torch.argmax(output): This function is used to find the index of the maximum value in the output tensor.\n",
        "        # .item(): This method is used to get a Python number from a tensor containing a single value.\n",
        "        next_char_id = torch.argmax(output).item()\n",
        "\n",
        "        # Get the character from the set based on the id\n",
        "        next_char = chars[next_char_id]\n",
        "\n",
        "        # Update the word and the current character\n",
        "        currentChar = next_char\n",
        "        word += currentChar\n",
        "\n",
        "      return word"
      ],
      "metadata": {
        "id": "R-yPJk4I-AH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model=bigram_one_hot_mlp, start='r', max_new_tokens=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kgXpzrU49jQ",
        "outputId": "9a54eca2-ba8b-4081-8674-ed9d2944aced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations and thoughts\n",
        "\n",
        "Looks like the one hot encoding input does not help the model that much in predicting the next token.\n",
        "\n",
        "Here are some thoughts regarding one hot encoded vectors vs embeddings:\n",
        "\n",
        "Embeddings provide a powerful alternative to one-hot encoded inputs, especially in models dealing with natural language processing, recommendation systems, and other domains where categorical variables play a crucial role. Here are the primary reasons why embeddings often result in better model performance compared to one-hot encoded inputs:\n",
        "\n",
        "### 1. **Dimensionality Reduction**\n",
        "- **One-hot encoding** produces very high-dimensional vectors where the length is equal to the number of categories in the vocabulary. For example, a vocabulary with 10,000 words results in 10,000-dimensional vectors, with only one non-zero element. This high dimensionality can lead to computational inefficiency and sparsity issues.\n",
        "- **Embeddings**, on the other hand, map these large one-hot vectors into a much smaller dimensional space (e.g., 50, 100, or 300 dimensions). This dense representation reduces the model's memory footprint and accelerates computation.\n",
        "\n",
        "### 2. **Semantic Information Preservation**\n",
        "- **One-hot vectors** are orthogonal and equidistant from each other, implying no relationship between different categories or words. This means one-hot encoding does not capture any form of similarity or semantic relationship between the categories.\n",
        "- **Embedding vectors** are learned during training and can capture complex relationships between categories. Words or items that are used in similar contexts will have embeddings that are closer in the vector space, which helps in capturing semantic meanings and relationships that are not possible with one-hot encoding.\n",
        "\n",
        "### 3. **Better Gradient Flow**\n",
        "- In neural networks, **one-hot inputs** often lead to inefficient gradient flow during backpropagation because only a small part of the weights (those corresponding to the 'hot' part of the vector) are updated at each training step. This inefficiency can slow down the learning process.\n",
        "- **Embeddings** provide a more efficient gradient flow as every dimension of the embedding vector contributes to the learning process and receives updates during training. This generally leads to faster convergence in training neural networks.\n",
        "\n",
        "### 4. **Generalization**\n",
        "- **Embeddings** can help the model generalize better to new, unseen examples. Since embeddings capture semantic relationships, a well-trained model can perform reasonably well even when encountering new words or categories similar to those seen during training.\n",
        "- With **one-hot encoding**, any new category not seen during training would require expanding the encoding scheme, which might not be feasible and does not leverage any learned contextual relationships.\n",
        "\n",
        "### 5. **Network Depth and Complexity**\n",
        "- Using **embeddings** allows deeper network architectures since the input data is more manageable in size and richer in information. Embeddings can be easily integrated into various layers of a neural network, facilitating more complex interactions in the model.\n",
        "- With **one-hot encoded inputs**, adding network depth can be less effective due to the sparsity and high dimensionality, which might not add meaningful information through deeper layers.\n",
        "\n",
        "### Example in NLP:\n",
        "In natural language processing, word embeddings like Word2Vec, GloVe, or those learned by an embedding layer in a deep learning model allow words with similar meanings to have similar representations. This is not achievable with one-hot encoding where each word is isolated with no shared information.\n",
        "\n",
        "In summary, embeddings provide a compact, dense, and semantically rich representation of categorical data, making them more suitable for complex models and large datasets where one-hot encoding would be inefficient both computationally and in terms of learning capability."
      ],
      "metadata": {
        "id": "G3MX-tJSDGfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Bigram MLP"
      ],
      "metadata": {
        "id": "vV7zsgV3Bnul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_inputs_and_outputs() -> list[torch.tensor, torch.tensor]:\n",
        "    \"\"\"\n",
        "    This function retruns input ids and outpits_one_hot_encoded\n",
        "    Here we are trying to perform better than the one_hot_encoded Bigram MLP\n",
        "\n",
        "    In the one hot encoded version we did pairs of\n",
        "    [one_hot_encoded_h][one_hot_encoded_e]\n",
        "    [one_hot_encoded_e][one_hot_encoded_l]\n",
        "\n",
        "    Here we do it in a different way\n",
        "    [id_h][one_hot_encoded_e]\n",
        "    [id_e][one_hot_encoded_l]\n",
        "    [id_l][one_hot_encoded_l]\n",
        "    [id_l][one_hot_encoded_o]\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    outputs_one_hot = []\n",
        "\n",
        "    # We have the chars list that has list of characters\n",
        "    # We have the text, which is the dataset that we have to train on\n",
        "\n",
        "    for i in range(len(text) - 1):\n",
        "      input_char = text[i]\n",
        "      output_char = text[i+1]\n",
        "\n",
        "      # Id of the input character\n",
        "      input_id = chars.index(input_char)\n",
        "      input_ids.append(input_id)\n",
        "\n",
        "      # One-hot encoding the output character\n",
        "      output_one_hot = torch.zeros(1, len(chars))\n",
        "      output_one_hot[0][encode(output_char)] = 1\n",
        "      outputs_one_hot.append(output_one_hot)\n",
        "\n",
        "    return input_ids, outputs_one_hot\n",
        "\n",
        "\n",
        "input_ids, outputs_one_hot = create_embedding_inputs_and_outputs()"
      ],
      "metadata": {
        "id": "PasrfDz-dSqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTZOLUn0K_WF",
        "outputId": "107e8984-6d0f-42a3-b27d-dc732bab3db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "NQwC8qTCLDNw",
        "outputId": "67cca120-b502-4a75-905c-dd6ad71e0a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'F'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_ids))\n",
        "print(len(outputs_one_hot))\n",
        "print(input_ids[0])\n",
        "print(outputs_one_hot[0])\n",
        "\n",
        "print(input_ids[1])\n",
        "print(outputs_one_hot[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajj_jf18LEoo",
        "outputId": "78334724-b009-4e9c-df8b-ef4cd7ca733d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4999\n",
            "4999\n",
            "tensor(21)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "tensor(49)\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting them to tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "outputs_one_hot = torch.vstack(outputs_one_hot)\n",
        "\n",
        "# printing the shape\n",
        "print(input_ids.shape)\n",
        "print(outputs_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svk6k_piLRDq",
        "outputId": "deab88f6-8e34-4526-8940-80305a6af3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4999])\n",
            "torch.Size([4999, 53])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We are training a model to learn how to predict the\n",
        "next character given the previous character.\n",
        "\"\"\"\n",
        "\n",
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(len(chars), 5)\n",
        "        self.fc1 = nn.Linear(5, 8) # takes inchars. values and outputs 8 values\n",
        "        self.fc2 = nn.Linear(8, len(chars)) # takes in 8 values and outputs 46 values\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddedX = self.token_embedding(x)\n",
        "        out = self.fc1(embeddedX)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        self.eval() # set the model to eval mode\n",
        "        with torch.no_grad(): # disables gradient calculation\n",
        "          word = start\n",
        "          current_char = start\n",
        "          # we want to generate max_new_tokens\n",
        "          for _ in range(max_new_tokens):\n",
        "            output = self.forward(torch.tensor([chars.index(current_char)]))\n",
        "            next_char_id = torch.argmax(output).item()\n",
        "            next_char = chars[next_char_id]\n",
        "            word += next_char\n",
        "            current_char = next_char\n",
        "          return word\n",
        "\n",
        "\n",
        "\n",
        "bigram_embedding_mlp = BigramEmbeddingMLP()\n",
        "\n",
        "optimizer = torch.optim.SGD(bigram_embedding_mlp.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "bigram_embedding_mlp.train()\n",
        "\n",
        "# training loop\n",
        "for _ in range(5000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = bigram_embedding_mlp(input_ids)\n",
        "    loss = loss_fn(outputs, outputs_one_hot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if _ % 50 == 0:\n",
        "        print(f'Epoch {_ + 1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CttaGxpIK-wW",
        "outputId": "c85c1731-ede4-4a34-c167-6e7d034c175f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.001214027404785\n",
            "Epoch 51, Loss: 3.9568183422088623\n",
            "Epoch 101, Loss: 3.914968490600586\n",
            "Epoch 151, Loss: 3.8750500679016113\n",
            "Epoch 201, Loss: 3.8365707397460938\n",
            "Epoch 251, Loss: 3.798905372619629\n",
            "Epoch 301, Loss: 3.761927843093872\n",
            "Epoch 351, Loss: 3.725783109664917\n",
            "Epoch 401, Loss: 3.6902477741241455\n",
            "Epoch 451, Loss: 3.6553354263305664\n",
            "Epoch 501, Loss: 3.6211750507354736\n",
            "Epoch 551, Loss: 3.5879199504852295\n",
            "Epoch 601, Loss: 3.5557920932769775\n",
            "Epoch 651, Loss: 3.5250096321105957\n",
            "Epoch 701, Loss: 3.4957902431488037\n",
            "Epoch 751, Loss: 3.4684042930603027\n",
            "Epoch 801, Loss: 3.442491292953491\n",
            "Epoch 851, Loss: 3.4182868003845215\n",
            "Epoch 901, Loss: 3.3958003520965576\n",
            "Epoch 951, Loss: 3.374884843826294\n",
            "Epoch 1001, Loss: 3.355443000793457\n",
            "Epoch 1051, Loss: 3.337252616882324\n",
            "Epoch 1101, Loss: 3.3200581073760986\n",
            "Epoch 1151, Loss: 3.3037796020507812\n",
            "Epoch 1201, Loss: 3.28837251663208\n",
            "Epoch 1251, Loss: 3.2737858295440674\n",
            "Epoch 1301, Loss: 3.2598977088928223\n",
            "Epoch 1351, Loss: 3.2466890811920166\n",
            "Epoch 1401, Loss: 3.2341325283050537\n",
            "Epoch 1451, Loss: 3.222153902053833\n",
            "Epoch 1501, Loss: 3.210742235183716\n",
            "Epoch 1551, Loss: 3.199921131134033\n",
            "Epoch 1601, Loss: 3.189662456512451\n",
            "Epoch 1651, Loss: 3.1798741817474365\n",
            "Epoch 1701, Loss: 3.1704814434051514\n",
            "Epoch 1751, Loss: 3.1615283489227295\n",
            "Epoch 1801, Loss: 3.1529667377471924\n",
            "Epoch 1851, Loss: 3.1447250843048096\n",
            "Epoch 1901, Loss: 3.136784076690674\n",
            "Epoch 1951, Loss: 3.129179000854492\n",
            "Epoch 2001, Loss: 3.121912717819214\n",
            "Epoch 2051, Loss: 3.114903688430786\n",
            "Epoch 2101, Loss: 3.1081137657165527\n",
            "Epoch 2151, Loss: 3.101583242416382\n",
            "Epoch 2201, Loss: 3.095299243927002\n",
            "Epoch 2251, Loss: 3.089245557785034\n",
            "Epoch 2301, Loss: 3.0834105014801025\n",
            "Epoch 2351, Loss: 3.0777788162231445\n",
            "Epoch 2401, Loss: 3.0723400115966797\n",
            "Epoch 2451, Loss: 3.0670740604400635\n",
            "Epoch 2501, Loss: 3.061983823776245\n",
            "Epoch 2551, Loss: 3.057058811187744\n",
            "Epoch 2601, Loss: 3.0522892475128174\n",
            "Epoch 2651, Loss: 3.0476651191711426\n",
            "Epoch 2701, Loss: 3.0431807041168213\n",
            "Epoch 2751, Loss: 3.038825511932373\n",
            "Epoch 2801, Loss: 3.0345957279205322\n",
            "Epoch 2851, Loss: 3.030489206314087\n",
            "Epoch 2901, Loss: 3.0264899730682373\n",
            "Epoch 2951, Loss: 3.022594451904297\n",
            "Epoch 3001, Loss: 3.018796682357788\n",
            "Epoch 3051, Loss: 3.015089511871338\n",
            "Epoch 3101, Loss: 3.011467695236206\n",
            "Epoch 3151, Loss: 3.007925271987915\n",
            "Epoch 3201, Loss: 3.0044679641723633\n",
            "Epoch 3251, Loss: 3.0010807514190674\n",
            "Epoch 3301, Loss: 2.997758150100708\n",
            "Epoch 3351, Loss: 2.9944961071014404\n",
            "Epoch 3401, Loss: 2.991290807723999\n",
            "Epoch 3451, Loss: 2.9881463050842285\n",
            "Epoch 3501, Loss: 2.9850680828094482\n",
            "Epoch 3551, Loss: 2.982038736343384\n",
            "Epoch 3601, Loss: 2.9790549278259277\n",
            "Epoch 3651, Loss: 2.9761147499084473\n",
            "Epoch 3701, Loss: 2.9732179641723633\n",
            "Epoch 3751, Loss: 2.9703636169433594\n",
            "Epoch 3801, Loss: 2.9675490856170654\n",
            "Epoch 3851, Loss: 2.9647746086120605\n",
            "Epoch 3901, Loss: 2.9620492458343506\n",
            "Epoch 3951, Loss: 2.9593427181243896\n",
            "Epoch 4001, Loss: 2.956669807434082\n",
            "Epoch 4051, Loss: 2.9540300369262695\n",
            "Epoch 4101, Loss: 2.9514403343200684\n",
            "Epoch 4151, Loss: 2.9488818645477295\n",
            "Epoch 4201, Loss: 2.9463512897491455\n",
            "Epoch 4251, Loss: 2.9438490867614746\n",
            "Epoch 4301, Loss: 2.941373825073242\n",
            "Epoch 4351, Loss: 2.9389257431030273\n",
            "Epoch 4401, Loss: 2.9365038871765137\n",
            "Epoch 4451, Loss: 2.9341065883636475\n",
            "Epoch 4501, Loss: 2.931731939315796\n",
            "Epoch 4551, Loss: 2.9293811321258545\n",
            "Epoch 4601, Loss: 2.9270529747009277\n",
            "Epoch 4651, Loss: 2.924748420715332\n",
            "Epoch 4701, Loss: 2.9224655628204346\n",
            "Epoch 4751, Loss: 2.9202044010162354\n",
            "Epoch 4801, Loss: 2.91796612739563\n",
            "Epoch 4851, Loss: 2.915745258331299\n",
            "Epoch 4901, Loss: 2.913543939590454\n",
            "Epoch 4951, Loss: 2.911362648010254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigram_embedding_mlp.generate(start='a'))\n",
        "print(bigram_embedding_mlp.generate(start='b'))\n",
        "print(bigram_embedding_mlp.generate(start='c'))\n",
        "print(bigram_embedding_mlp.generate(start='d'))\n",
        "print(bigram_embedding_mlp.generate(start='e'))\n",
        "print(bigram_embedding_mlp.generate(start='f'))\n",
        "print(bigram_embedding_mlp.generate(start='u'))\n",
        "print(bigram_embedding_mlp.generate(start='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VcDLFgdPWTQ",
        "outputId": "d0182d54-cf0e-4d34-ea19-a8b47bdcaa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
            "b t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "ce t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
            "d t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "e t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "f t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "u t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n",
            "v t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "Look about the same as the one hot encoded bigram model. The loss has reduced over time.\n",
        "\n",
        "**One hot encoded bigram**:\n",
        "- Epoch 5000, Loss: 3.050889253616333\n",
        "\n",
        "**Embedding bigram**:\n",
        "- Epoch 2650, Loss: 3.0476651191711426"
      ],
      "metadata": {
        "id": "xF8aOMPZPcaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Generative Pretrained Transformer (65 points)\n",
        "\n",
        "For this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU"
      ],
      "metadata": {
        "id": "qplpM8_Cbp0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run nvidia-smi to check gpu usage\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0Oh-3FeFxxnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124b3e64-7dbf-499f-cdd7-d34f6a6f9480"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul  6 23:32:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the gpt model, let's use the full text\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "rhJAwCAOADP7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a character level tokenization function.\n",
        "\n",
        "1. Create a list of unique characters in the string. (1 points)\n",
        "2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids (1 point)\n",
        "3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string (1 point)\n"
      ],
      "metadata": {
        "id": "z_LZpvZ8AEEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = []\n",
        "\n",
        "# List of unique characters in the string\n",
        "for char in text:\n",
        "    if char not in chars:\n",
        "        chars.append(char)\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Takes in a list of characters and returns a list of ids (ints)\n",
        "    \"\"\"\n",
        "    return [chars.index(char) for char in s]\n",
        "\n",
        "def decode(ids: list[int]) -> str:\n",
        "    \"\"\"\n",
        "    Takes in a list of ids (ints) and returns a string\n",
        "    \"\"\"\n",
        "    return ''.join([chars[id] for id in ids])"
      ],
      "metadata": {
        "id": "rnEOfMj4Dk4Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of characters : {len(chars)}\")\n",
        "print(f\"First 10 characters : {chars[:10]}\")\n",
        "print(f\"Encoded text : {encode('hello')}\")\n",
        "print(f\"Decoded text : {decode(encode('hello'))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BfvXYE1rRkb",
        "outputId": "24e8853c-c91e-4767-8568-f0833e29724b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of characters : 65\n",
            "First 10 characters : ['F', 'i', 'r', 's', 't', ' ', 'C', 'z', 'e', 'n']\n",
            "Encoded text : [22, 8, 28, 28, 14]\n",
            "Decoded text : hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
        "Returns a copy of this object in CUDA memory.\n",
        "If this object is already in CUDA memory and on the correct device,\n",
        "then no copy is performed and the original object is returned."
      ],
      "metadata": {
        "id": "vk0hGPZOr47D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the input data to a tensor\n",
        "encoded_text = encode(text)\n",
        "print(f\"First 10 chars '{text[0:10]}'\")\n",
        "print(f\"encoded text first 10 chars {encoded_text[0:10]}\")\n",
        "print(f\"Total character count {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtIcyVir8Li",
        "outputId": "23a1f25f-663d-4dee-f7b3-1153d6898a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 chars 'First Citi'\n",
            "encoded text first 10 chars [0, 1, 2, 3, 4, 5, 6, 1, 4, 1]\n",
            "Total character count 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long).cuda()"
      ],
      "metadata": {
        "id": "1gyOaRF5Dq1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORgMpJtwshR0",
        "outputId": "06a43556-7326-4637-badc-9a3e85681e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1115394])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can think of this as a window of characters that we use as the prefix to predict the next character\n",
        "block_size = 16\n",
        "data[:block_size+1] # first 17 entities in the tensor ( characters )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvWGi8Mk6x1q",
        "outputId": "075cada0-d317-4fa2-e3c0-b5e21307b163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12,  8],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "GvO4hSK171Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here le'ts just pick the first block of size `block_size` and try to\n",
        "# visuazlie how the transformer learns to predict the next character\n",
        "# We design the system to only learn from tokens that are before the token that has to be predicted\n",
        "\n",
        "x = data[:block_size]\n",
        "y = data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVWxO6Pa70Lh",
        "outputId": "c5008a21-2a3b-4661-9081-c5023ae5c51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([0], device='cuda:0') the target is: 1\n",
            "when input is tensor([0, 1], device='cuda:0') the target is: 2\n",
            "when input is tensor([0, 1, 2], device='cuda:0') the target is: 3\n",
            "when input is tensor([0, 1, 2, 3], device='cuda:0') the target is: 4\n",
            "when input is tensor([0, 1, 2, 3, 4], device='cuda:0') the target is: 5\n",
            "when input is tensor([0, 1, 2, 3, 4, 5], device='cuda:0') the target is: 6\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0') the target is: 1\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1], device='cuda:0') the target is: 4\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4], device='cuda:0') the target is: 1\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1], device='cuda:0') the target is: 7\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7], device='cuda:0') the target is: 8\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7, 8], device='cuda:0') the target is: 9\n",
            "when input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7, 8, 9], device='cuda:0') the target is: 10\n",
            "when input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10],\n",
            "       device='cuda:0') the target is: 11\n",
            "when input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11],\n",
            "       device='cuda:0') the target is: 12\n",
            "when input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12],\n",
            "       device='cuda:0') the target is: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisiting some basics:\n",
        "\n",
        "Terms:\n",
        "- Block Size: The number of characters that the system has been trained to take into consideration while learning to predict the next character"
      ],
      "metadata": {
        "id": "R_tCJuSyt_7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def get_batch(data, block_size, batch_size):\n",
        "    \"\"\"\n",
        "    This function is responsible for creating a batch of batch_size\n",
        "    For training a GPT model\n",
        "\n",
        "    \"\"\"\n",
        "    # Here we generate a tensor `ix` containing `batch_size` random\n",
        "    # indices within the range `0` to `len(data) - block_size`\n",
        "    # we substract `block_size` from the end so that the last\n",
        "    # selected block stays within the list of available characters\n",
        "    # in the text\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Here we create a stack of tensors (batch size) each of length\n",
        "    # block_size that start from the\n",
        "    # above picked random indices\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "\n",
        "    # creates the target tensor y similarly, but shifted\n",
        "    # one position to the right, representing the next\n",
        "    # character to predict for each position in x.\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "lFYZnm2MuLlt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MdmKEJyJboop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clarifying some more terms before we proceed to the next step:\n",
        "\n",
        "### Block Size\n",
        "- **Block Size** in a GPT model training code refers to the length of each segment of the input data that each training example consists of. This is directly analogous to what you might think of as \"sequence length\" in other contexts but is specifically termed \"block size\" in training scenarios for models like GPT.\n",
        "- In the context of the Transformer attention head depicted in the image below, each \"sequence\" or input processed through the attention head would be of a fixed length equivalent to the block size. In the image, while not specified as \"block size,\" the dimension that would correspond to this term is the middle dimension of the tensors, which is 32 in your example (`(8, 32, 64)`).\n",
        "\n",
        "### Sequence Length\n",
        "- **Sequence Length** in more general contexts refers to the total length of sequences being processed, which may vary unless specifically pre-processed to be uniform. In models like Transformers (and as seen in the attention head diagram), sequence length is typically fixed to a specific size for each training or inference pass. This fixed length is crucial for attention calculations across the entire sequence uniformly.\n",
        "- In the Transformer attention head diagram, this \"sequence length\" is manifest in each stage of the attention mechanism. It represents the number of positions (or tokens) in each sequence that the model processes simultaneously, marked as the second dimension in the tensors.\n",
        "\n",
        "### How It Relates to the Diagram\n",
        "- In the attention head diagram present below, all tensors maintain a consistent second dimension (32 in this case), reflecting the fixed sequence length or block size used for the calculations. This consistent dimensionality across layers and operations ensures that each token in the sequence can be related to every other token via the attention mechanism, a key feature enabling the model to capture complex dependencies across the input.\n",
        "- The operations like matrix multiplication (`matmul`) between the transposed keys and queries, and subsequent operations like softmax and dropout, all depend on this fixed sequence length to compute the attention scores and ultimately the output sequence. This fixed length, as used in your GPT model training, allows the Transformer to utilize positional relationships effectively.\n",
        "\n",
        "### Summary\n",
        "In summary, in the Transformer model, as depicted by the attention head image below, block size and sequence length can be considered equivalent, referring to the fixed size of the input sequences used for training and inference. This term varies in usage depending on the model architecture but is crucial for models like Transformers that depend on a fixed dimension to compute relationships between all pairs of inputs within a sequence effectively.\n",
        "\n",
        "1. **Batch Size**: This is the number of samples processed before the model is updated. For exaple if we are are dealing with (8, 32, 64), the first dimension \"8\" typically represents the batch size. This means that the model processes 8 samples at a time.\n",
        "\n",
        "2. **Sequence Length**: This is the length of the input sequences each sample in a batch contains. In the example tensor, \"32\" represents the sequence length / block size, indicating each sample consists of 32 sequential elements or **tokens**. ***For instance, in natural language processing, this could represent 32 words in a sentence. In this example since we will be predicting characters, each token represents a character from the list of unique characters available.***\n",
        "\n",
        "3. **Feature Dimension**: This indicates the number of features each element of the sequence holds. The \"64\" in the example tensor suggests that each token or element of the sequence is represented by a vector of 64 features. These could be embeddings that encapsulate the token's meaning in a dense vector.\n",
        "\n",
        "4. **Block Size**: This term isn't explicitly shown in the diagram but is related to how data is structured or processed in blocks during certain operations. For instance, in memory management or in GPU computation, operations might be optimized by processing data in \"blocks.\" In the context of transformers or deep learning, block size might refer to the dimensionality of sub-parts of the model such as in splitting matrices for parallel processing, but typically it's not a term used to describe tensor dimensions directly.\n",
        "\n",
        "Thus, in the below shown transformer model diagram:\n",
        "- Batch size: 8 (number of samples processed together)\n",
        "- Sequence length / block size: 32 (number of tokens, items, or steps per sample)\n",
        "- Feature dimension: 64 (features per token or step in each sequence)"
      ],
      "metadata": {
        "id": "uYYGoQy13BEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Self Attention Head (5 points)\n",
        "![](https://i.ibb.co/GWR1XG0/head.png)"
      ],
      "metadata": {
        "id": "-HmnXJjxtm3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explaining the above artention head set up\n",
        "\n",
        "This diagram depicts the computational flow in a typical attention head of a Transformer neural network, commonly used in models like GPT and BERT. Here’s a breakdown of the operations and their significance:\n",
        "\n",
        "1. **Input Tensor**: The input tensor has the shape (8, 32, 64), where 8 represents the batch size, 32 the sequence length / block size, and 64 the feature dimension of each token in the sequence.\n",
        "\n",
        "2. **Linear Layers**: Three parallel linear transformations are applied to the input tensor. Each layer outputs a tensor of shape (8, 32, 16). These transformations typically generate the queries (Q), keys (K), and values (V) which are used in the attention mechanism.\n",
        "\n",
        "3. **Transpose Operation**: The output of one of the linear layers (presumably representing keys, K) is transposed to change its shape from (8, 32, 16) to (8, 16, 32). This operation is necessary for matrix multiplication with the queries (Q). Here we notice that the transpose is only done across the second and third dimensions as the first dimension only represents the batch size\n",
        "\n",
        "4. **Matrix Multiplication (matmul)**: The output of the transpose operation (K transposed) is matrix-multiplied with another linear output (Q). ***This results in a shape of (8, 32, 32), representing the raw attention scores before they are normalized.***\n",
        "\n",
        "5. **Multiplication (mul)**: This operation might be an element-wise multiplication used as part of scaling the attention scores by the square root of the dimension of the keys to stabilize gradients during training, although the typical square root scaling is not explicitly shown here. More on why we scale the attention values below ...\n",
        "\n",
        "6. **Masked Fill**: This operation is used to apply masks to the attention scores. Masks are often used to ignore (or mask) padding tokens or future tokens during training in sequence models. The operation doesn't change the shape of the tensor. The upper right half of the matrix is made as zeros in this step.\n",
        "\n",
        "7. **Softmax**: The softmax function is applied across the last dimension (32) to normalize the attention scores to a probability distribution.\n",
        "\n",
        "8. **Dropout**: Dropout is a regularization technique where random elements of the tensor are zeroed out during training to prevent overfitting. The shape remains unchanged.\n",
        "\n",
        "9. **Matrix Multiplication (matmul)**: The normalized and possibly masked attention scores are then matrix-multiplied with the third linear output (V, values), resulting in an output tensor of shape (8, 32, 16). This operation computes the weighted sum of the values based on the attention scores.\n",
        "\n",
        "10. **Output Tensor**: The final output tensor is generated with the shape (8, 32, 16), likely to be fed into subsequent layers of the Transformer or processed further depending on the specific architecture and task.\n",
        "\n",
        "This detailed flow illustrates how attention mechanisms selectively focus on different parts of the input sequence, weighting input features by relevance, which is central to the success of Transformer models in handling various sequence-based tasks in natural language processing."
      ],
      "metadata": {
        "id": "2XNdlBRl2VZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The scaling of the attention scores\n",
        "\n",
        "The scaling of the attention scores based on the dimension of the keys in the Transformer architecture addresses a specific challenge in training deep learning models that use softmax to calculate probabilities.\n",
        "\n",
        "### Background on Dot Products and Their Scale\n",
        "The attention mechanism computes the dot products between the query and all keys in the sequence. These dot products are a critical component because they determine the attention scores that indicate how much each part of the input should contribute to the output at each position. However, the magnitude of the dot products depends on the dimensionality of the keys and queries. Here's why:\n",
        "\n",
        "- The dot product of two vectors increases with the number of dimensions. Specifically, if each component of the vectors is drawn from a distribution with a constant variance, the variance of the dot product is proportional to the dimensionality of the vectors.\n",
        "- As the dimension of the keys (and queries, since they are usually of the same dimension) increases, the average value of the dot products becomes larger. This can lead to extremely large values, especially when working with high-dimensional data, which is common in models like Transformers.\n",
        "\n",
        "### Impact on Softmax\n",
        "The softmax function, which is used to convert these dot products into probabilities (or attention scores), is highly sensitive to changes in input values. Specifically:\n",
        "- Large values in the softmax input can lead to a situation where the softmax function's output is close to zero for all inputs except the largest one (a phenomenon often referred to as the softmax function \"saturating\"). This saturation can significantly slow down learning, as it leads to very small gradients during backpropagation — essentially, the network is less able to learn from the input data.\n",
        "\n",
        "### Why Scale by Square Root of Dimension?\n",
        "The scaling factor used, \\(\\sqrt{d_k}\\) (where \\(d_k\\) is the dimension of the keys), helps mitigate these effects:\n",
        "- **Normalization**: By dividing the dot products by \\(\\sqrt{d_k}\\), you effectively normalize them, bringing their variance back to a more manageable scale. This normalization helps maintain a more uniform scale across different model sizes and configurations.\n",
        "- **Gradient Stability**: By keeping the dot products (and thus the inputs to the softmax) at a reasonable magnitude, the scaling prevents gradients under the softmax from becoming too small. This is crucial for efficient learning, as it ensures that each update step during training is informative enough to guide the model towards better performance without being too noisy or too minimal.\n",
        "\n",
        "### Conclusion\n",
        "Scaling by the square root of the dimension of the keys is a practical approach to ensuring that the attention mechanism operates effectively across different settings and model scales, facilitating stable and efficient training. This method is particularly vital in deep learning architectures like Transformers, where models often deal with high-dimensional data and require careful handling of numerical stability during training operations."
      ],
      "metadata": {
        "id": "jjNgfYPJ8VwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Terms relevant to constructing the SelfAttention Head:\n",
        "\n",
        "In the context of Transformer architectures, the **head size** in an attention head refers to the dimension of the vectors used for each of the queries (Q), keys (K), and values (V) within a single attention head. This is a key parameter that defines how much information each attention head can capture.\n",
        "\n",
        "### Definition and Calculation\n",
        "\n",
        "- **Head Size**: The head size is essentially the dimensionality of the Q, K, and V vectors within each specific attention head. It is typically derived by dividing the total dimension of the model's embeddings (\\(d_{\\text{model}}\\)) by the number of attention heads (\\(\\text{num\\_heads}\\)). This allows the model to distribute the embedding information across multiple heads, each focusing on different features or relationships in the data.\n",
        "\n",
        "### Formula\n",
        "The head size for queries and keys (\\(d_k\\) and \\(d_q\\)) is often the same and can be calculated as:\n",
        "\\[ d_k = d_q = \\frac{d_{\\text{model}}}{\\text{num\\_heads}} \\]\n",
        "For values (\\(d_v\\)), it is usually the same as \\(d_k\\) and \\(d_q\\), though this can vary depending on specific model architectures or design choices:\n",
        "\\[ d_v = \\frac{d_{\\text{model}}}{\\text{num\\_heads}} \\]\n",
        "\n",
        "### Example\n",
        "If a Transformer model uses an embedding dimension (\\(d_{\\text{model}}\\)) of 512 and has 8 attention heads:\n",
        "\\[ d_k = d_q = d_v = \\frac{512}{8} = 64 \\]\n",
        "Thus, each head processes vectors of size 64 for queries, keys, and values.\n",
        "\n",
        "### Importance\n",
        "The choice of head size affects how finely the model can focus on different aspects of the input data. Each head can potentially learn to attend to different parts of the sequence or different types of relationships:\n",
        "- **Smaller head sizes** can lead to a more focused and granular attention mechanism, where each head might specialize more distinctly.\n",
        "- **Larger head sizes** provide more capacity to each head, which can be useful for capturing more complex patterns or dependencies, but may reduce the diversity of what different heads can learn.\n",
        "\n",
        "Adjusting the head size is a balance between computational efficiency, capacity, and the diversity of information that the attention heads can capture. It's an important aspect of model tuning, especially in tasks requiring nuanced understanding of context or relationships within the data.\n",
        "\n",
        "### How does this translate to code\n",
        "\n",
        "Following this example: If a Transformer model uses an embedding dimension (\\(d_{\\text{model}}\\)) of 512 and has 8 attention heads:\n",
        "\\[ d_k = d_q = d_v = \\frac{512}{8} = 64 \\]\n",
        "Thus, each head processes vectors of size 64 for queries, keys, and values.\n",
        "\n",
        "Here each layer q, k and v, instead of processing all the tokens that are a part of the embedding, will only process tokens that are passed into this head.\n",
        "\n",
        "Hence the q, k and v linear layers will be of shape batch_size x head_size\n"
      ],
      "metadata": {
        "id": "oc_Nwm3zMuUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Mask\n",
        "\n",
        "The code snippet provided below involves creating a triangular mask and then applying it to an attention matrix in a Transformer model, typically used in tasks like text processing or sequence modeling. Let’s break down the two lines to understand what’s happening:\n",
        "\n",
        "### Line 1: Creating the Mask\n",
        "```python\n",
        "mask = torch.tril(torch.ones(timesteps, timesteps))\n",
        "```\n",
        "- **`torch.ones(timesteps, timesteps)`**: This function creates a 2D tensor (square matrix) filled with the value `1`, where the dimensions of the matrix are both `timesteps`. `timesteps` could be the length of a sequence being processed, such as the number of words in a sentence.\n",
        "- **`torch.tril()`**: This function takes a tensor and returns a lower triangular part of the matrix. It zeroes out all elements above the main diagonal. The main diagonal and the elements below remain as they were, which in this case, are all `1`s due to the `torch.ones()` function. This triangular matrix is typically used in attention mechanisms to ensure that the attention calculation for a given timestep only considers that timestep and the ones before it (i.e., ensuring causality in models like GPT).\n",
        "\n",
        "### Line 2: Applying the Mask to the Attention Matrix\n",
        "```python\n",
        "masked_attention = attention.masked_fill(mask == 0, float('-inf'))\n",
        "```\n",
        "- **`mask == 0`**: This operation compares each element of the `mask` tensor to `0`. Since `mask` is a lower triangular matrix with `1`s in the lower triangle and `0`s elsewhere, this operation generates a Boolean tensor where `True` corresponds to the positions where the mask had `0`s (i.e., the upper triangular part of the matrix) and `False` everywhere else (i.e., the lower triangular part).\n",
        "- **`masked_fill()`**: This method is called on the `attention` tensor. It takes two arguments: a mask and a value to fill. The mask here is the Boolean tensor from `mask == 0`. Wherever the mask is `True`, the `attention` tensor is filled with `float('-inf')`. This effectively applies the mask by setting the attention scores in the upper triangle (those that should not be considered due to causality) to negative infinity.\n",
        "\n",
        "### Why `float('-inf')`?\n",
        "In attention mechanisms, especially when followed by a softmax operation, setting values to negative infinity before softmax ensures that those values have zero probability. When softmax is applied to a vector containing negative infinity, the exponential of negative infinity is zero, hence those positions do not contribute to the output of the softmax.\n",
        "\n",
        "### Summary\n",
        "- The `mask` tensor is used to enforce causality in the attention mechanism by preventing the model from attending to future timesteps in the sequence. This is essential in models like GPT where predictions for a given position should only depend on previous positions.\n",
        "- The `mask == 0` operation identifies positions that should be ignored (in this context, future timesteps), and `masked_fill` applies this by setting such positions in the attention matrix to negative infinity, effectively removing them from consideration during attention normalization (softmax)."
      ],
      "metadata": {
        "id": "dk6--KJhSp6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  \"\"\"\n",
        "  This class implements a single self attention head\n",
        "  For the input dimensions we have batch size , sequence length , feature dimension\n",
        "\n",
        "  First we need to implement the K, Q and V layers\n",
        "  These are three linear layers - we can refer to\n",
        "  https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "\n",
        "  For a\n",
        "  \"\"\"\n",
        "  def __init__(self, num_heads, n_embed):\n",
        "    super().__init__()\n",
        "    self.k = nn.Linear(n_embed, n_embed, bias=False)\n",
        "    self.q = nn.Linear(n_embed, n_embed, bias=False)\n",
        "    self.v = nn.Linear(n_embed, n_embed, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The forward step contains the following steps\n",
        "    1. Pass x through the linear layers\n",
        "    2. Perform transpose operation on k\n",
        "    3. Calculate the bidirectional attention value q.k\n",
        "    4. Scaling the attention value\n",
        "    5. Masked fil\n",
        "    6. Softmax\n",
        "    7. Dropout\n",
        "    8. Matrix multiplication with v\n",
        "\n",
        "    Batch, Tokens, Chanels\n",
        "    \"\"\"\n",
        "    print(x.shape)\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    # print shape of k\n",
        "    print(f\"K shape {self.k.weight.shape}\")\n",
        "    # print shape of q\n",
        "    k = self.k(x)\n",
        "    q = self.q(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    print(k.shape)\n",
        "    print(q.shape)\n",
        "    print(v.shape)\n",
        "\n",
        "    # Transpose the dimensions\n",
        "    k = k.transpose(1, 2) # Here we transpose dimensions 1 and 2\n",
        "    # k is now of dimension B, C, T\n",
        "\n",
        "    print(f\"kShape after transpose {k.shape}\")\n",
        "    # Here we calculate the bidirectional attention value q.k\n",
        "    attention = torch.matmul(q, k) # B, T, C * B, C, T = B, T, T\n",
        "\n",
        "    print(f\"Attention shape {attention.shape}\")\n",
        "    # Scaling the attention value by the sq root of the chanels / features\n",
        "    attention = attention * C**-0.5 # this will be of dimension B, T, T\n",
        "\n",
        "\n",
        "    print(f\"Attention shape {attention.shape}\")\n",
        "    # Masked fill\n",
        "    mask = torch.tril(torch.ones(T, T)).to('cuda') # We create a mask of dimensions C, C\n",
        "    # print shape of mask\n",
        "    print(f\"Mask shape {mask.shape}\")\n",
        "\n",
        "    # We apply the mask to the attention matrix\n",
        "    # float('-inf') is applied to all positions where the mask value is 0\n",
        "    masked_attention = attention.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    print(f\"Masked attention shape {masked_attention.shape}\")\n",
        "\n",
        "    # Softmax\n",
        "    # the dimension value has to be set to -1 to indicate the last dimension\n",
        "    attention = F.softmax(masked_attention, dim=-1)\n",
        "\n",
        "    print(f\"Softmax attention shape {attention.shape}\")\n",
        "\n",
        "    # Dropout\n",
        "    attention = F.dropout(attention, p=0.1)\n",
        "\n",
        "    print(f\"Dropout attention shape {attention.shape}\")\n",
        "\n",
        "    # Matrix multiplication with v\n",
        "    output = torch.matmul(attention, v)\n",
        "    print(f\"Attention shape {attention.shape}\")\n",
        "    print(f\"V shape {v.shape}\")\n",
        "    print(f\"Output shape {output.shape}\")\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "5SD8Z16R-sfZ"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Self Attention (5 points)\n",
        "\n",
        "`constructor`\n",
        "\n",
        "- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n",
        "- Create a linear layer with n_embd input dim and n_embd output dim\n",
        "\n",
        "`forward`\n",
        "\n",
        "In the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n",
        "\n",
        "![](https://i.ibb.co/y5SwyZZ/multihead.png)"
      ],
      "metadata": {
        "id": "LWeoHGBiFpWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image and the accompanying task outline depict the implementation steps for a Multihead Self-Attention mechanism, which is a core component of models like the Transformer. Here’s a breakdown of each step in the process:\n",
        "\n",
        "### Constructor\n",
        "1. **SelfAttentionHead Instances**:\n",
        "   - **Purpose**: Each SelfAttentionHead is responsible for capturing different aspects of the input data through separate attention mechanisms. This diversification allows the model to attend to different features or parts of the input simultaneously.\n",
        "   - **Implementation**: Using `nn.ModuleList` to create and manage these instances is efficient because it automatically handles the forward pass for each module within a list, facilitating easier batch processing and gradient updates.\n",
        "   - **Configuration**: You are instructed to create four such heads, which means the input will be divided and processed by these four heads independently in parallel.\n",
        "\n",
        "2. **Linear Layer**:\n",
        "   - **Purpose**: After processing the input through multiple attention heads, the outputs are concatenated. This linear layer then projects the concatenated output back to the desired embedding dimension (`n_embd`). It integrates information from all the attention heads.\n",
        "   - **Specification**: The linear layer should have an input dimension and output dimension equal to `n_embd`, which matches the input's embedding size to ensure dimensional consistency across the network.\n",
        "\n",
        "### Forward Method\n",
        "1. **Pass Input Through Each Head**:\n",
        "   - **Process**: The input tensor `x` is passed through each of the four SelfAttentionHead instances. Since each head may focus on different features or relations within the data, they may produce varying outputs.\n",
        "   - **Output Dimension**: Assuming each head preserves the dimensionality of its output to be smaller or equal to the input dimension divided by the number of heads (for simplicity, let's assume it's divided equally), each head would output a tensor with a shape of `[batch_size, seq_length, n_embd/4]`.\n",
        "\n",
        "2. **Concatenate Outputs**:\n",
        "   - **Function**: The outputs from all heads are concatenated along the feature dimension (last dimension). This step combines the different \"views\" or \"attentions\" the heads have calculated into a single tensor.\n",
        "   - **Resulting Dimension**: After concatenation, the dimension of the output tensor would be `[batch_size, seq_length, n_embd]` because each head's output is concatenated to form the full embedding dimension.\n",
        "\n",
        "3. **Pass Through Linear Layer**:\n",
        "   - **Purpose**: The concatenated tensor is then passed through the linear layer created in the constructor. This layer acts as a transformation that can mix information across the different attention mechanisms, potentially aiding in better information integration.\n",
        "   - **Output**: The output of this linear layer is a tensor of the same shape as the input to the layer, `[batch_size, seq_length, n_embd]`, thus ensuring the output matches the input dimension of the sequence in terms of embedding size.\n",
        "\n",
        "4. **Dropout Application**:\n",
        "   - **Consideration**: Although not detailed in your task outline, typically, a dropout layer would follow the linear layer to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training. This step helps in making the model more robust.\n",
        "\n",
        "### Summary\n",
        "In summary, the multihead self-attention mechanism processes the input through multiple attention heads, each potentially focusing on different features. The outputs of these heads are then integrated via concatenation and further transformed by a linear layer to ensure that the model can leverage information across various heads effectively. This architecture is crucial for complex sequence modeling tasks where different parts of the input sequence carry different types of information relevant to the task."
      ],
      "metadata": {
        "id": "NVuc9V16VjzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embed):\n",
        "        super().__init__()\n",
        "        # Self attention head instances\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(num_heads, n_embed) for _ in range(num_heads)])\n",
        "        # Linear layer\n",
        "        # The linear layer should have an input dimension and output dimension\n",
        "        # equal to n_embd, which matches the input's embedding size to ensure\n",
        "        # dimensional consistency across the network.\n",
        "        self.proj = nn.Linear(head_size * num_heads, head_size * num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward step consists of the following steps\n",
        "        1. Pass x through each head\n",
        "        2. Concatenate all the outputs along the feature dimension\n",
        "        3. Pass the concatenated output through the linear layer\n",
        "        \"\"\"\n",
        "        print(f\"Inside multi head attention\")\n",
        "        # Pass x through each head\n",
        "        heads = [head(x) for head in self.heads] # (B, t, C)[]\n",
        "\n",
        "        # print output dimension of each head\n",
        "        print(f\"Head output dimension {heads[0].shape}\")\n",
        "\n",
        "        print(f\"After passing through heads\")\n",
        "        # Concatenate all the outputs along the feature dimension\n",
        "        x = torch.cat(heads, dim=-1) # B, T, C\n",
        "        # print shape of x\n",
        "        print(f\"X shape {x.shape}\")\n",
        "\n",
        "        # print shape of projection\n",
        "        print(f\"Projection shape {self.proj.weight.shape}\")\n",
        "        print(f\"After concatenating\")\n",
        "        # Pass the concatenated output through the linear layer\n",
        "        x = self.proj(x) # B, T, C\n",
        "\n",
        "        print(f\"After passing through linear projection layer\")\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gFsPDkpnFs_b"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP (2 points)\n",
        "Implement a 2 layer MLP\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/C0DtrF5/ff.png)"
      ],
      "metadata": {
        "id": "uH_0ELyZ8YCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The diagram provided illustrates a sequence of operations that form a typical Feedforward Neural Network (FFN) module, commonly used within the architecture of a Transformer model. This specific sequence of operations represents the \"position-wise feedforward network\" component of the Transformer's architecture. Here’s a breakdown of each step and its function:\n",
        "\n",
        "### 1. **Linear Layer**\n",
        "- **Input**: \\( (8, 32, 64) \\)\n",
        "- **Output**: \\( (8, 32, 256) \\)\n",
        "- **Description**: This layer is a fully connected neural network layer that projects each input feature from a 64-dimensional space to a 256-dimensional space. The input tensor's first dimension typically represents the batch size (8), the second dimension the sequence length (32), and the third dimension the feature size (64). The expansion in feature size allows the network to learn more complex features at each position of the sequence.\n",
        "\n",
        "### 2. **ReLU Activation**\n",
        "- **Input/Output**: \\( (8, 32, 256) \\)\n",
        "- **Description**: The ReLU (Rectified Linear Unit) activation function is applied element-wise. It introduces non-linearity into the model, which is essential for learning complex patterns. ReLU is defined as \\( f(x) = max(0, x) \\), effectively setting all negative values to zero and keeping positive values unchanged. This operation does not alter the dimensions of the data.\n",
        "\n",
        "### 3. **Second Linear Layer**\n",
        "- **Input**: \\( (8, 32, 256) \\)\n",
        "- **Output**: \\( (8, 32, 64) \\)\n",
        "- **Description**: Another fully connected layer that projects the features back from the 256-dimensional space to the original 64-dimensional space. This step is crucial for matching the dimensions of the output with other components in a Transformer model, such as the self-attention outputs, allowing for subsequent operations like residual connections.\n",
        "\n",
        "### 4. **Dropout**\n",
        "- **Input/Output**: \\( (8, 32, 64) \\)\n",
        "- **Description**: Dropout is a regularization technique used to prevent overfitting in neural networks. It randomly sets a fraction of the input units to zero during training at each update step, which helps to make the model robust and less likely to rely on any small set of neurons. The dropout rate (the probability of setting a value to zero) is a hyperparameter that can be tuned. The operation does not change the dimensions of the tensor.\n",
        "\n",
        "### 5. **Output Tensor**\n",
        "- **Dimension**: \\( (8, 32, 64) \\)\n",
        "- **Description**: The final output tensor retains the dimensions of the input tensor to the entire module. This output is typically fed back into the main flow of the Transformer model, often added to the input tensor through a residual connection before being normalized and passed on to the next layer or operation in the model.\n",
        "\n",
        "### Summary\n",
        "This feedforward network within a Transformer model performs crucial transformation and re-projection of features, enhancing the model's ability to capture and manipulate information. The use of non-linearity (ReLU) and regularization (Dropout) helps in learning non-trivial patterns and generalizing better to unseen data. Each component is dimensionally coordinated to maintain consistency throughout the model, allowing for seamless integration with other modules such as multi-head attention."
      ],
      "metadata": {
        "id": "dRhe_OuqdAWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim, scale_up_factor):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim * scale_up_factor, bias=False)\n",
        "        self.fc2 = nn.Linear(embed_dim * scale_up_factor, embed_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        \"\"\"\n",
        "        The forward step of the MLP has the following steps\n",
        "        1. Pass x through the first linear layer\n",
        "        2. Apply ReLU activation\n",
        "        3. Pass the output through the second linear layer\n",
        "        4. Apply dropout\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(x)) # B, T, C*scale_up\n",
        "        x = self.fc2(x) # B, T, C\n",
        "        x = F.dropout(x, p=0.1) # B, T, C\n",
        "        return x"
      ],
      "metadata": {
        "id": "K96Z3kAv7lNt"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer block (20 points)\n",
        "\n",
        "Layer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n",
        "\n",
        "Dropout is a form of regularization to prevent overfitting.\n",
        "\n",
        "This is the diagram of a transformer block:\n",
        "\n",
        "![](https://i.ibb.co/X85C473/block.png)"
      ],
      "metadata": {
        "id": "bUFxuyf-JIxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The transformed block\n",
        "The diagram provided outlines the structure of a typical Transformer block, which is a fundamental component of Transformer models widely used in natural language processing tasks. Here’s a breakdown of each step and component shown in the diagram:\n",
        "\n",
        "### 1. **Input Tensor**\n",
        "- **Shape**: (8, 32, 64)\n",
        "- **Description**: This tensor represents the input to the Transformer block, where 8 could be the batch size, 32 the sequence length, and 64 the dimensionality of each vector in the sequence.\n",
        "\n",
        "### 2. **Layer Normalization**\n",
        "- **Input/Output Shape**: (8, 32, 64)\n",
        "- **Depth**: 1\n",
        "- **Description**: The first operation is layer normalization. Layer normalization is applied within each sample independently and normalizes the data across the feature dimension (across the 64 features of each of the sequence's 32 positions). This helps in stabilizing the learning process by normalizing the inputs to have zero mean and unit variance, which can improve training speed and stability.\n",
        "\n",
        "### 3. **Multi-Head Attention**\n",
        "- **Input/Output Shape**: (8, 32, 64)\n",
        "- **Depth**: 1\n",
        "- **Description**: This component uses multiple sets of attention mechanisms (heads) to process the input. Each head can attend to different parts of the input sequence, allowing the model to capture various aspects of the sequence in parallel. The output of this module is typically the same size as the input, allowing for residual connections.\n",
        "\n",
        "### 4. **Addition (Residual Connection)**\n",
        "- **Input**: 2 x (8, 32, 64)\n",
        "- **Output Shape**: (8, 32, 64)\n",
        "- **Depth**: 1\n",
        "- **Description**: The output from the multi-head attention is added to the original input (prior to layer normalization). This is known as a residual connection and helps in mitigating the vanishing gradient problem by allowing gradients to flow directly through the network.\n",
        "\n",
        "### 5. **Second Layer Normalization**\n",
        "- **Input/Output Shape**: (8, 32, 64)\n",
        "- **Depth**: 1\n",
        "- **Description**: Another layer normalization step is applied after the residual connection and before the feedforward network. This normalizes the data again, preparing it for further processing and maintaining stability in deeper layers.\n",
        "\n",
        "### 6. **FeedForward Network**\n",
        "- **Input/Output Shape**: (8, 32, 64)\n",
        "- **Depth**: 1\n",
        "- **Description**: This is typically a position-wise feedforward neural network, which means it applies the same neural network to each position independently. It usually consists of two linear transformations with a nonlinear activation function in between. This network can expand (and later compress) the internal representation, allowing the model to mix features before passing them to the next layer.\n",
        "\n",
        "### 7. **Addition (Second Residual Connection)**\n",
        "- **Input**: 2 x (8, 32, 64)\n",
        "- **Output Shape**: (8, 32, 64)\n",
        "- **Depth**: 1\n",
        "- **Description**: Similar to the earlier addition, this step adds the output of the feedforward network to the input of the feedforward network (the output from the previous layer normalization). This second residual connection further helps in preserving information throughout layers and aids in training deeper networks.\n",
        "\n",
        "### 8. **Output Tensor**\n",
        "- **Shape**: (8, 32, 64)\n",
        "- **Depth**: 0\n",
        "- **Description**: The final output tensor of the Transformer block maintains the same shape as the input tensor, ensuring that multiple such blocks can be stacked without dimension mismatch.\n",
        "\n",
        "### Summary\n",
        "This diagram presents a classic configuration of a Transformer block, which utilizes normalization, attention mechanisms, residual connections, and feedforward networks to process sequential data effectively. Each component plays a crucial role in ensuring the model learns effectively and generalizes well across different tasks and data distributions. The design of the Transformer block with layer normalization and dropout is specifically tailored to enhance training stability and prevent overfitting, making it highly effective for tasks requiring the handling of complex sequential data."
      ],
      "metadata": {
        "id": "z8vO2lx4gFnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, sequence_length: int):\n",
        "        \"\"\"\n",
        "        Architecture of each transformer block\n",
        "        1. Layer norm\n",
        "        2. Multihead attention\n",
        "        3. Layer norm\n",
        "        4. Feedforward network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_head, sequence_length//n_head, n_embd)\n",
        "        self.mlp = MLP(n_embd, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass of a transformer block consists of the following steps\n",
        "        1. Layer norm\n",
        "        2. Multihead attention\n",
        "        3. Addition (residual connection)\n",
        "        4. Layer norm\n",
        "        5. Feedforward network\n",
        "        6. Addition (second residual connection)\n",
        "        \"\"\"\n",
        "        print(f\"First line of transformer block\")\n",
        "        # Adding layer normalization to each sample individually\n",
        "        norm = self.ln1(x)\n",
        "\n",
        "        print(f\"In transformer block\")\n",
        "        print(f\"After first layer norm\")\n",
        "\n",
        "        # Calculating the attention values\n",
        "        attn = self.attn(norm)\n",
        "\n",
        "        print(f\"After calculating attention\")\n",
        "\n",
        "        # Adding the residual connection\n",
        "        x = x + attn\n",
        "\n",
        "        # Adding layer normalization after the residual connection\n",
        "        norm = self.ln2(x)\n",
        "        print(f\"After second layer norm\")\n",
        "        # Calculating the feedforward network\n",
        "        mlp = self.mlp(norm)\n",
        "        print(f\"After feedforward network\")\n",
        "        # Adding the second residual connection\n",
        "        x = x + mlp\n",
        "        print(f\"After second residual connection\")\n",
        "        # Returning the output of the transformer block\n",
        "        return x"
      ],
      "metadata": {
        "id": "xTDAd66KIvvx"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT\n",
        "\n",
        "`constructor` (5 points)\n",
        "\n",
        "1. create the token embedding table and the position embedding table\n",
        "2. create variable `self.blocks` that is a series of 4 `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n",
        "3. create a layer norm layer\n",
        "4. create a linear layer for predicting the next token\n",
        "\n",
        "`forward(self, idx, targets=None)`. (5 points)\n",
        "\n",
        "`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n",
        "1. get the token by using the token embedding table created in the constructor\n",
        "2. create the position embeddings\n",
        "3. sum the token and position embeddings to get the model input\n",
        "4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "5. compute the loss\n",
        "\n",
        "`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` (5 points)\n",
        "1. implement top p, top_k, and temperature for sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "SyFQXltDKNti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)"
      ],
      "metadata": {
        "id": "0Xa2bh2XDdKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explannation of concepts\n",
        "\n",
        "***Positional Encoding***\n",
        "\n",
        "Positional embeddings are a crucial component in Transformer-based models like GPT, where the architecture lacks any inherent mechanism to track the sequence order of the input tokens. Let's delve into how positional embeddings work and clarify their function.\n",
        "\n",
        "### Purpose of Positional Embeddings\n",
        "Positional embeddings provide the model with information about the relative or absolute position of tokens in the sequence. Since models like GPT process all input tokens simultaneously without recurrence or convolution, they rely on these embeddings to incorporate the order of the sequence into their calculations. This positional information is essential for tasks that depend on the order of words or characters, such as language understanding and generation.\n",
        "\n",
        "### How Positional Embeddings are Implemented\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "1. **Token Embeddings**:\n",
        "   - `token = self.token_embedding(idx)` retrieves embeddings for each token in the batch based on their indices (`idx`). These embeddings capture the semantic meaning of each token.\n",
        "   - Output shape: `[B, T, C]` where `B` is the batch size, `T` is the sequence length, and `C` is the embedding dimension.\n",
        "\n",
        "2. **Generating Positional Indices**:\n",
        "   - `position = torch.arange(0, idx.shape[1])` generates a tensor of positions from `0` to `T-1` (where `T` is the sequence length). This tensor is used to fetch positional embeddings.\n",
        "   - This line creates a tensor that effectively acts as an index for each position in the sequence, where `idx.shape[1]` corresponds to the sequence length `T`.\n",
        "\n",
        "3. **Positional Embeddings Lookup**:\n",
        "   - `position = self.position_embedding(position)` uses the position indices to retrieve the positional embeddings from a defined embedding table (`self.position_embedding`). This table is usually initialized randomly and then learned during training.\n",
        "   - It's important to note that there might be an error in this part of the code because the positional indices need to be expanded for all batches in the input. Typically, you would need to repeat or expand these indices to match the batch size `B`. Here's how you could do it:\n",
        "     ```python\n",
        "     position = self.position_embedding(position.unsqueeze(0).repeat(B, 1, 1))\n",
        "     ```\n",
        "   - After correction, `position` will have the same shape as `token`, i.e., `[B, T, C]`.\n",
        "\n",
        "4. **Combining Token and Positional Embeddings**:\n",
        "   - `x = token + position` sums the token embeddings and positional embeddings element-wise. The addition operation allows the model to consider both the semantic meaning of each token and its position in the sequence simultaneously.\n",
        "   - The resulting tensor `x` is then used as the input for the subsequent layers of the model. This combined embedding retains the shape `[B, T, C]`.\n",
        "\n",
        "### Summary\n",
        "Positional embeddings are critical for providing the necessary context of token order to Transformer models, enabling them to effectively process sequences of data. The combination of token and positional embeddings gives the model a comprehensive understanding of both the meaning of tokens and their positional relationships within sequences."
      ],
      "metadata": {
        "id": "dwySsTf4rCTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, vocab_size, sequence_length):\n",
        "        super().__init__()\n",
        "        # Embedding tables\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(sequence_length, n_embd)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head, sequence_length),\n",
        "            Block(n_embd, n_head, sequence_length),\n",
        "            Block(n_embd, n_head, sequence_length),\n",
        "            Block(n_embd, n_head, sequence_length)\n",
        "        )\n",
        "\n",
        "        # Layer Norm layer\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Linear layer\n",
        "        # This will output values for the possible n number of tokens\n",
        "        self.linear = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        forward takes `idx`,  a batch of context ids as input of size (B, T)\n",
        "        If targets is None, return the logits and None.\n",
        "        Else returns the logits and the loss\n",
        "\n",
        "\n",
        "        1. get the token by using the token embedding table created in the constructor\n",
        "        2. create the position embeddings\n",
        "        3. sum the token and position embeddings to get the model input\n",
        "        4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "        5. compute the loss\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the token by using the token embedding table\n",
        "        token = self.token_embedding(idx) # B, T, C\n",
        "\n",
        "\n",
        "        # Create the position indices\n",
        "        # position = torch.arange(0, idx.size(1), device=idx.device)  # [T]\n",
        "\n",
        "        # Expand position indices to match the batch size\n",
        "        # and retrieve the corresponding positional embeddings\n",
        "        # position = self.position_embedding(position)[None, :, :].repeat(idx.size(0), 1, 1)  # [B, T, C]\n",
        "\n",
        "\n",
        "        # Improved way of calculating position embedding using torch's broadcasting function\n",
        "        position = self.position_embedding(torch.arange(idx.size(1), device=idx.device)).expand(idx.size(0), -1, -1)\n",
        "\n",
        "\n",
        "        # Sum the token and position embeddings to get the model input\n",
        "        x = token + position\n",
        "        print(f\"After encoding\")\n",
        "        # Pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "        x = self.blocks(x)\n",
        "        print(f\"After transformer blocks\")\n",
        "        x = self.ln(x)\n",
        "        print(f\"After layer norm\")\n",
        "\n",
        "        logits = self.linear(x)\n",
        "        print(f\"After linear layer\")\n",
        "\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, start_char, max_new_tokens, top_p, top_k, temperature):\n",
        "        \"\"\"\n",
        "        implement top p, top_k, and temperature for sampling\n",
        "        Generate text starting from start_char with controlled sampling\n",
        "        \"\"\"\n",
        "        start_token_index = encode(start_char)\n",
        "\n",
        "        input_ids = torch.tensor(start_token_index, dtype=torch.long).to(self.token_embedding.weight.device)\n",
        "\n",
        "        self.eval()  # Put the model in evaluation mode\n",
        "        generated_text = [start_char]  # List to collect generated characters\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "          # Extracts the logits form the last batch, last token\n",
        "          # Here since we are working with characters, we only have one entry in a batch with one\n",
        "          # element in the sequence\n",
        "          # So we extract the logits generated for next character based on the input sequence\n",
        "          logits = self.forward(input_ids)[-1, -1, :]\n",
        "\n",
        "          # Apply temperature\n",
        "          logits = logits / temperature\n",
        "\n",
        "          # Filter the logits with top k sampling\n",
        "          top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
        "          logits = torch.zeros_like(logits).scatter_(0, top_k_indices, top_k_values)\n",
        "\n",
        "          # Apply softmax to convert to probabilities\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "          # Filter the logits with top p sampling\n",
        "          sorted_indices = torch.argsort(probs, descending=True)\n",
        "          sorted_probs = torch.sort(probs, descending=True)[0]\n",
        "          cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
        "          sorted_indices_to_remove = cumulative_probs > top_p\n",
        "          # Shift the mask to the right to keep at least one token\n",
        "          sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "          sorted_indices_to_remove[0] = 0\n",
        "\n",
        "          probs[sorted_indices[sorted_indices_to_remove]] = 0\n",
        "\n",
        "          # Renormalize the probabilities\n",
        "          probs /= probs.sum()\n",
        "\n",
        "          # Sample from the modified distribution\n",
        "          next_token_id = torch.multinomial(probs, 1).item()\n",
        "\n",
        "          # Add the sampled token to the input sequence\n",
        "          input_ids = torch.cat([input_ids, torch.tensor([next_token_id], dtype=torch.long).to(input_ids.device)], dim=0)\n",
        "\n",
        "          next_char = decode(next_token_id)\n",
        "          generated_text.append(next_char)\n",
        "\n",
        "        return ''.join(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "8WT4oUN084ts"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop (15 points)\n",
        "\n",
        "implement training loop"
      ],
      "metadata": {
        "id": "Njzrwwiv-mfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "vocab_size = len(chars)  # Adjust based on your dataset\n",
        "n_embd = 768        # Embedding dimension\n",
        "n_head = 4         # Number of attention heads\n",
        "n_block = 4         # Number of Transformer blocks\n",
        "block_size = 128    # Length of the sequence window\n",
        "batch_size = 64     # Number of sequences per batch\n",
        "max_iters = 5000    # Total iterations for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "jDBBxg5fZTSB"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(n_embd, 4, len(chars) ,block_size).to('cuda') # make you are running this on the GPU"
      ],
      "metadata": {
        "id": "oJ0D51wNYyJD"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Setup the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "IrdLrtMtVjNR"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = encode(text)  # Convert entire text to indices\n",
        "data = torch.tensor(data, dtype=torch.long)  #"
      ],
      "metadata": {
        "id": "QNpg11owWm2o"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume 'data' is already a long tensor of encoded text\n",
        "# Randomly split data into training and validation sets\n",
        "train_size = int(0.9 * len(data))\n",
        "train_data, val_data = data[:train_size], data[train_size:]"
      ],
      "metadata": {
        "id": "gOWPEILkay0i"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, val_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYIML2D2c6c6",
        "outputId": "fd092a50-0698-4577-a274-9b0ae8681cf4"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1003854]), torch.Size([111540]))"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AthAqCBmc544"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# TensorBoard for monitoring\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    model.train()\n",
        "    x, y = get_batch(train_data, block_size, batch_size)\n",
        "    print(x.shape, y.shape)\n",
        "    # Forward pass\n",
        "    logits, _ = model(x)\n",
        "    loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Logging\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}: Loss {loss.item()}\")\n",
        "        writer.add_scalar('Training Loss', loss.item(), iter)\n",
        "\n",
        "        # Validation step\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            x_val, y_val = get_batch(val_data, block_size, batch_size)\n",
        "            logits_val, _ = model(x_val)\n",
        "            val_loss = loss_fn(logits_val.view(-1, vocab_size), y_val.view(-1))\n",
        "            print(f\"Validation Loss: {val_loss.item()}\")\n",
        "            writer.add_scalar('Validation Loss', val_loss.item(), iter)\n",
        "\n",
        "writer.close()\n",
        "\n",
        "torch.save(model.state_dict(), 'gpt_model.pth')\n",
        ""
      ],
      "metadata": {
        "id": "qWtn2uTwYUrY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5bf1787-e055-41eb-8770-91f3827e4ce0"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "After encoding\n",
            "First line of transformer block\n",
            "In transformer block\n",
            "After first layer norm\n",
            "Inside multi head attention\n",
            "torch.Size([64, 128, 768])\n",
            "K shape torch.Size([768, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "kShape after transpose torch.Size([64, 768, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Mask shape torch.Size([128, 128])\n",
            "Masked attention shape torch.Size([64, 128, 128])\n",
            "Softmax attention shape torch.Size([64, 128, 128])\n",
            "Dropout attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "V shape torch.Size([64, 128, 768])\n",
            "Output shape torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "K shape torch.Size([768, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "kShape after transpose torch.Size([64, 768, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Mask shape torch.Size([128, 128])\n",
            "Masked attention shape torch.Size([64, 128, 128])\n",
            "Softmax attention shape torch.Size([64, 128, 128])\n",
            "Dropout attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "V shape torch.Size([64, 128, 768])\n",
            "Output shape torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "K shape torch.Size([768, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "kShape after transpose torch.Size([64, 768, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Mask shape torch.Size([128, 128])\n",
            "Masked attention shape torch.Size([64, 128, 128])\n",
            "Softmax attention shape torch.Size([64, 128, 128])\n",
            "Dropout attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "V shape torch.Size([64, 128, 768])\n",
            "Output shape torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "K shape torch.Size([768, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "torch.Size([64, 128, 768])\n",
            "kShape after transpose torch.Size([64, 768, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "Mask shape torch.Size([128, 128])\n",
            "Masked attention shape torch.Size([64, 128, 128])\n",
            "Softmax attention shape torch.Size([64, 128, 128])\n",
            "Dropout attention shape torch.Size([64, 128, 128])\n",
            "Attention shape torch.Size([64, 128, 128])\n",
            "V shape torch.Size([64, 128, 768])\n",
            "Output shape torch.Size([64, 128, 768])\n",
            "Head output dimension torch.Size([64, 128, 768])\n",
            "After passing through heads\n",
            "X shape torch.Size([64, 128, 3072])\n",
            "Projection shape torch.Size([128, 128])\n",
            "After concatenating\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8192x3072 and 128x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-187-ab82bd50f6e8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-c8bcf6e22c4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After encoding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Pass the model through the blocks, the layernorm layer, and the final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After transformer blocks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-179-5ccc6806a7af>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Calculating the attention values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After calculating attention\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-177-28af4131576b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After concatenating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Pass the concatenated output through the linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B, T, C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After passing through linear projection layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8192x3072 and 128x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text\n",
        "\n",
        "\n",
        "print some text that your model generates"
      ],
      "metadata": {
        "id": "zy3v8Nv7YVUa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5l4soWviWG5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}