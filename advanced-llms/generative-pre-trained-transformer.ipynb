{"metadata":{"colab":{"provenance":[{"file_id":"19TZO9ge3WWHlsK3-wEb2yG5coUQdTBsF","timestamp":1717890932543}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{"id":"F097yaiu7dXQ","executionInfo":{"status":"ok","timestamp":1720393233407,"user_tz":240,"elapsed":4283,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T00:59:16.137984Z","iopub.execute_input":"2024-07-08T00:59:16.138337Z","iopub.status.idle":"2024-07-08T00:59:19.817460Z","shell.execute_reply.started":"2024-07-08T00:59:16.138307Z","shell.execute_reply":"2024-07-08T00:59:19.816715Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5iradmn7bZtM","executionInfo":{"status":"ok","timestamp":1720393195661,"user_tz":240,"elapsed":835,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"3e8bc3e0-0030-4707-d7be-bb0ed2962811","execution":{"iopub.status.busy":"2024-07-08T00:59:25.434521Z","iopub.execute_input":"2024-07-08T00:59:25.435459Z","iopub.status.idle":"2024-07-08T00:59:26.666051Z","shell.execute_reply.started":"2024-07-08T00:59:25.435424Z","shell.execute_reply":"2024-07-08T00:59:26.664924Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2024-07-08 00:59:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n\n2024-07-08 00:59:26 (31.0 MB/s) - 'input.txt' saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# For the bigram model, let's use the first 1000 characters for the data\n\nwith open('input.txt', 'r') as f:\n    text = f.read()\ntext = text[:5000]","metadata":{"id":"pLoVi294G-T-","executionInfo":{"status":"ok","timestamp":1720393196836,"user_tz":240,"elapsed":187,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T00:59:29.208666Z","iopub.execute_input":"2024-07-08T00:59:29.209606Z","iopub.status.idle":"2024-07-08T00:59:29.216754Z","shell.execute_reply.started":"2024-07-08T00:59:29.209558Z","shell.execute_reply":"2024-07-08T00:59:29.215850Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"NRUazcXhTC_G","executionInfo":{"status":"ok","timestamp":1720308735335,"user_tz":240,"elapsed":222,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"4bd71a06-9244-4031-801a-8b79c59798b1","execution":{"iopub.status.busy":"2024-07-08T00:59:32.139744Z","iopub.execute_input":"2024-07-08T00:59:32.140095Z","iopub.status.idle":"2024-07-08T00:59:32.149803Z","shell.execute_reply.started":"2024-07-08T00:59:32.140069Z","shell.execute_reply":"2024-07-08T00:59:32.148908Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not maliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was for his country he did it to\\nplease his mother and to be partly proud; which he\\nis, even till the altitude of his virtue.\\n\\nSecond Citizen:\\nWhat he cannot help in his nature, you account a\\nvice in him. You must in no way say he is covetous.\\n\\nFirst Citizen:\\nIf I must not, I need not be barren of accusations;\\nhe hath faults, with surplus, to tire in repetition.\\nWhat shouts are these? The other side o' the city\\nis risen: why stay we prating here? to the Capitol!\\n\\nAll:\\nCome, come.\\n\\nFirst Citizen:\\nSoft! who comes here?\\n\\nSecond Citizen:\\nWorthy Menenius Agrippa; one that hath always loved\\nthe people.\\n\\nFirst Citizen:\\nHe's one honest enough: would all the rest were so!\\n\\nMENENIUS:\\nWhat work's, my countrymen, in hand? where go you\\nWith bats and clubs? The matter? speak, I pray you.\\n\\nFirst Citizen:\\nOur business is not unknown to the senate; they have\\nhad inkling this fortnight what we intend to do,\\nwhich now we'll show 'em in deeds. They say poor\\nsuitors have strong breaths: they shall know we\\nhave strong arms too.\\n\\nMENENIUS:\\nWhy, masters, my good friends, mine honest neighbours,\\nWill you undo yourselves?\\n\\nFirst Citizen:\\nWe cannot, sir, we are undone already.\\n\\nMENENIUS:\\nI tell you, friends, most charitable care\\nHave the patricians of you. For your wants,\\nYour suffering in this dearth, you may as well\\nStrike at the heaven with your staves as lift them\\nAgainst the Roman state, whose course will on\\nThe way it takes, cracking ten thousand curbs\\nOf more strong link asunder than can ever\\nAppear in your impediment. For the dearth,\\nThe gods, not the patricians, make it, and\\nYour knees to them, not arms, must help. Alack,\\nYou are transported by calamity\\nThither where more attends you, and you slander\\nThe helms o' the state, who care for you like fathers,\\nWhen you curse them as enemies.\\n\\nFirst Citizen:\\nCare for us! True, indeed! They ne'er cared for us\\nyet: suffer us to famish, and their store-houses\\ncrammed with grain; make edicts for usury, to\\nsupport usurers; repeal daily any wholesome act\\nestablished against the rich, and provide more\\npiercing statutes daily, to chain up and restrain\\nthe poor. If the wars eat us not up, they will; and\\nthere's all the love they bear us.\\n\\nMENENIUS:\\nEither you must\\nConfess yourselves wondrous malicious,\\nOr be accused of folly. I shall tell you\\nA pretty tale: it may be you have heard it;\\nBut, since it serves my purpose, I will venture\\nTo stale 't a little more.\\n\\nFirst Citizen:\\nWell, I'll hear it, sir: yet you must not think to\\nfob off our disgrace with a tale: but, an 't please\\nyou, deliver.\\n\\nMENENIUS:\\nThere was a time when all the body's members\\nRebell'd against the belly, thus accused it:\\nThat only like a gulf it did remain\\nI' the midst o' the body, idle and unactive,\\nStill cupboarding the viand, never bearing\\nLike labour with the rest, where the other instruments\\nDid see and hear, devise, instruct, walk, feel,\\nAnd, mutually participate, did minister\\nUnto the appetite and affection common\\nOf the whole body. The belly answer'd--\\n\\nFirst Citizen:\\nWell, sir, what answer made the belly?\\n\\nMENENIUS:\\nSir, I shall tell you. With a kind of smile,\\nWhich ne'er came from the lungs, but even thus--\\nFor, look you, I may make the belly smile\\nAs well as speak--it tauntingly replied\\nTo the discontented members, the mutinous parts\\nThat envied his receipt; even so most fitly\\nAs you malign our senators for that\\nThey are not such as you.\\n\\nFirst Citizen:\\nYour belly's answer? What!\\nThe kingly-crowned head, the vigilant eye,\\nThe counsell\""},"metadata":{}}]},{"cell_type":"code","source":"# Get unique characters from the text\n# To have unique values, we can use the set data structure\n# Reference https://stackoverflow.com/questions/13902805/list-of-all-unique-characters-in-a-string\nchars = list(set(text))","metadata":{"id":"hoMGZgEOdRjt","executionInfo":{"status":"ok","timestamp":1720393368915,"user_tz":240,"elapsed":194,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T00:59:34.593622Z","iopub.execute_input":"2024-07-08T00:59:34.594464Z","iopub.status.idle":"2024-07-08T00:59:34.598441Z","shell.execute_reply.started":"2024-07-08T00:59:34.594435Z","shell.execute_reply":"2024-07-08T00:59:34.597486Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(chars)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjjroOrYT8pv","executionInfo":{"status":"ok","timestamp":1720393369711,"user_tz":240,"elapsed":3,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"fe96874a-6ef8-410e-b21f-52b68a0f7c38","execution":{"iopub.status.busy":"2024-07-08T00:59:36.781615Z","iopub.execute_input":"2024-07-08T00:59:36.781987Z","iopub.status.idle":"2024-07-08T00:59:36.788188Z","shell.execute_reply.started":"2024-07-08T00:59:36.781959Z","shell.execute_reply":"2024-07-08T00:59:36.787298Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"53"},"metadata":{}}]},{"cell_type":"code","source":"\ndef encode(string: str) -> list[int]:\n    \"\"\"\n    Given a string, encode returns a list of integers that represent the characters\n    in the string.\n    \"\"\"\n    encodedChars = []\n\n    for s in string:\n      encodedChars.append(chars.index(s))\n    return encodedChars\n\ndef decode(ids: list[int]) -> str:\n    \"\"\"\n    Given a list of integers, decode returns the characters in the list as a string.\n    \"\"\"\n    decodedChars = [];\n    for id in ids:\n      decodedChars.append(chars[id])\n    return \"\".join(decodedChars)\n\n# Testing the encode and decode functions\n\nencoded = encode('hello')\ndecoded = decode(encoded)\nprint(encoded)\nprint(decoded)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKdvB6FYTr85","executionInfo":{"status":"ok","timestamp":1720393214090,"user_tz":240,"elapsed":3,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"1553716f-9197-437f-cc74-6df679461ec9","execution":{"iopub.status.busy":"2024-07-08T00:59:39.106176Z","iopub.execute_input":"2024-07-08T00:59:39.106526Z","iopub.status.idle":"2024-07-08T00:59:39.114330Z","shell.execute_reply.started":"2024-07-08T00:59:39.106501Z","shell.execute_reply":"2024-07-08T00:59:39.113374Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[47, 33, 1, 1, 36]\nhello\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generative Pretrained Transformer\n\nFor this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU","metadata":{"id":"qplpM8_Cbp0s"}},{"cell_type":"code","source":"# Parameters\nvocab_size = len(chars)  # Adjust based on your dataset\nn_embd = 768        # Embedding dimension\nn_head = 1         # Number of attention heads\nn_block = 4         # Number of Transformer blocks\nblock_size = 128    # Length of the sequence window\nbatch_size = 64     # Number of sequences per batch\nmax_iters = 5000    # Total iterations for training\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"id":"jDBBxg5fZTSB","executionInfo":{"status":"ok","timestamp":1720393235569,"user_tz":240,"elapsed":3,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:00:35.596480Z","iopub.execute_input":"2024-07-08T01:00:35.596874Z","iopub.status.idle":"2024-07-08T01:00:35.658810Z","shell.execute_reply.started":"2024-07-08T01:00:35.596846Z","shell.execute_reply":"2024-07-08T01:00:35.657664Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZsdcIXece50","executionInfo":{"status":"ok","timestamp":1720393531428,"user_tz":240,"elapsed":175,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"ff83f21b-24f0-41ba-bf07-ff62aac0596f","execution":{"iopub.status.busy":"2024-07-08T01:00:38.154735Z","iopub.execute_input":"2024-07-08T01:00:38.155657Z","iopub.status.idle":"2024-07-08T01:00:38.160304Z","shell.execute_reply.started":"2024-07-08T01:00:38.155612Z","shell.execute_reply":"2024-07-08T01:00:38.159411Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# run nvidia-smi to check gpu usage\n!nvidia-smi","metadata":{"id":"0Oh-3FeFxxnI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720393238141,"user_tz":240,"elapsed":7,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"29a0a4ba-565c-465e-8e9b-910f3e27e6ac","execution":{"iopub.status.busy":"2024-07-08T01:00:40.818046Z","iopub.execute_input":"2024-07-08T01:00:40.818450Z","iopub.status.idle":"2024-07-08T01:00:41.926896Z","shell.execute_reply.started":"2024-07-08T01:00:40.818416Z","shell.execute_reply":"2024-07-08T01:00:41.925869Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Mon Jul  8 01:00:41 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   43C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# For the gpt model, let's use the full text\n\nwith open('input.txt', 'r') as f:\n    text = f.read()","metadata":{"id":"rhJAwCAOADP7","executionInfo":{"status":"ok","timestamp":1720386881889,"user_tz":240,"elapsed":2,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:01:03.093434Z","iopub.execute_input":"2024-07-08T01:01:03.094380Z","iopub.status.idle":"2024-07-08T01:01:03.100865Z","shell.execute_reply.started":"2024-07-08T01:01:03.094339Z","shell.execute_reply":"2024-07-08T01:01:03.099793Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Implement a character level tokenization function.\n\n1. Create a list of unique characters in the string. (1 points)\n2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids (1 point)\n3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string (1 point)\n","metadata":{"id":"z_LZpvZ8AEEi"}},{"cell_type":"code","source":"chars = []\n\n# List of unique characters in the string\nfor char in text:\n    if char not in chars:\n        chars.append(char)\n\ndef encode(s: str) -> list[int]:\n    \"\"\"\n    Takes in a list of characters and returns a list of ids (ints)\n    \"\"\"\n    return [chars.index(char) for char in s]\n\ndef decode(ids: list[int]) -> str:\n    \"\"\"\n    Takes in a list of ids (ints) and returns a string\n    \"\"\"\n    return ''.join([chars[id] for id in ids])","metadata":{"id":"rnEOfMj4Dk4Y","executionInfo":{"status":"ok","timestamp":1720393278988,"user_tz":240,"elapsed":3,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:01:42.739771Z","iopub.execute_input":"2024-07-08T01:01:42.740144Z","iopub.status.idle":"2024-07-08T01:01:43.324861Z","shell.execute_reply.started":"2024-07-08T01:01:42.740113Z","shell.execute_reply":"2024-07-08T01:01:43.324129Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(f\"Length of characters : {len(chars)}\")\nprint(f\"First 10 characters : {chars[:10]}\")\nprint(f\"Encoded text : {encode('hello')}\")\nprint(f\"Decoded text : {decode(encode('hello'))}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BfvXYE1rRkb","executionInfo":{"status":"ok","timestamp":1720386882674,"user_tz":240,"elapsed":8,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"fc3902b0-0867-4405-b4a5-af2cb4660f85","execution":{"iopub.status.busy":"2024-07-08T01:01:53.430349Z","iopub.execute_input":"2024-07-08T01:01:53.431210Z","iopub.status.idle":"2024-07-08T01:01:53.436570Z","shell.execute_reply.started":"2024-07-08T01:01:53.431178Z","shell.execute_reply":"2024-07-08T01:01:53.435655Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Length of characters : 65\nFirst 10 characters : ['F', 'i', 'r', 's', 't', ' ', 'C', 'z', 'e', 'n']\nEncoded text : [22, 8, 28, 28, 14]\nDecoded text : hello\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\nReturns a copy of this object in CUDA memory.\nIf this object is already in CUDA memory and on the correct device,\nthen no copy is performed and the original object is returned.","metadata":{"id":"vk0hGPZOr47D"}},{"cell_type":"code","source":"# Converting the input data to a tensor\nencoded_text = encode(text)\nprint(f\"First 10 chars '{text[0:10]}'\")\nprint(f\"encoded text first 10 chars {encoded_text[0:10]}\")\nprint(f\"Total character count {len(text)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEtIcyVir8Li","executionInfo":{"status":"ok","timestamp":1720393282695,"user_tz":240,"elapsed":4,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"ecd83938-daa1-40c1-bb37-c645b82a0a10","execution":{"iopub.status.busy":"2024-07-08T01:02:20.363269Z","iopub.execute_input":"2024-07-08T01:02:20.363627Z","iopub.status.idle":"2024-07-08T01:02:20.950820Z","shell.execute_reply.started":"2024-07-08T01:02:20.363579Z","shell.execute_reply":"2024-07-08T01:02:20.949939Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"First 10 chars 'First Citi'\nencoded text first 10 chars [0, 1, 2, 3, 4, 5, 6, 1, 4, 1]\nTotal character count 1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long).to(device)","metadata":{"id":"1gyOaRF5Dq1P","executionInfo":{"status":"ok","timestamp":1720393576793,"user_tz":240,"elapsed":169,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:02:33.327956Z","iopub.execute_input":"2024-07-08T01:02:33.328360Z","iopub.status.idle":"2024-07-08T01:02:34.273443Z","shell.execute_reply.started":"2024-07-08T01:02:33.328332Z","shell.execute_reply":"2024-07-08T01:02:34.272638Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORgMpJtwshR0","executionInfo":{"status":"ok","timestamp":1720393564706,"user_tz":240,"elapsed":189,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"107ca398-b465-4f7f-b96c-66224962774f","execution":{"iopub.status.busy":"2024-07-08T01:02:35.833368Z","iopub.execute_input":"2024-07-08T01:02:35.833740Z","iopub.status.idle":"2024-07-08T01:02:35.839735Z","shell.execute_reply.started":"2024-07-08T01:02:35.833711Z","shell.execute_reply":"2024-07-08T01:02:35.838770Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.Size([1115394])"},"metadata":{}}]},{"cell_type":"code","source":"# We can think of this as a window of characters that we use as the prefix to predict the next character\ndata[:block_size+1] # first 17 entities in the tensor ( characters )","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvWGi8Mk6x1q","executionInfo":{"status":"ok","timestamp":1720386883974,"user_tz":240,"elapsed":10,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"a909278c-af56-4ab8-cdfc-7d9f9b674e0a","execution":{"iopub.status.busy":"2024-07-08T01:03:15.679362Z","iopub.execute_input":"2024-07-08T01:03:15.679998Z","iopub.status.idle":"2024-07-08T01:03:15.708056Z","shell.execute_reply.started":"2024-07-08T01:03:15.679968Z","shell.execute_reply":"2024-07-08T01:03:15.707207Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12,  8, 13,\n        14,  2,  8,  5, 15,  8,  5, 16,  2, 14, 17,  8,  8, 18,  5, 19,  9, 20,\n         5, 13, 21,  2,  4, 22,  8,  2, 23,  5, 22,  8, 19,  2,  5, 24,  8,  5,\n         3, 16,  8, 19, 25, 26, 11, 11, 27, 28, 28, 10, 11, 29, 16,  8, 19, 25,\n        23,  5,  3, 16,  8, 19, 25, 26, 11, 11,  0,  1,  2,  3,  4,  5,  6,  1,\n         4,  1,  7,  8,  9, 10, 11, 30, 14, 21,  5, 19,  2,  8,  5, 19, 28, 28,\n         5,  2,  8,  3, 14, 28, 31,  8, 18,  5,  2, 19,  4, 22,  8,  2,  5,  4,\n        14,  5, 18], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n\n","metadata":{"id":"GvO4hSK171Vu"}},{"cell_type":"code","source":"# Here letss just pick the first block of size `block_size` and try to\n# visuazlie how the transformer learns to predict the next character\n# We design the system to only learn from tokens that are before the token that has to be predicted\ntemp_blick_size = 16\nx = data[:temp_blick_size]\ny = data[1:temp_blick_size+1]\nfor t in range(temp_blick_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target is: {target}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVWxO6Pa70Lh","executionInfo":{"status":"ok","timestamp":1720393581860,"user_tz":240,"elapsed":381,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"31d1068f-da27-49db-9bf0-bb2f68370fbb","execution":{"iopub.status.busy":"2024-07-08T01:03:36.446502Z","iopub.execute_input":"2024-07-08T01:03:36.447228Z","iopub.status.idle":"2024-07-08T01:03:36.459903Z","shell.execute_reply.started":"2024-07-08T01:03:36.447193Z","shell.execute_reply":"2024-07-08T01:03:36.458940Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"when input is tensor([0], device='cuda:0') the target is: 1\nwhen input is tensor([0, 1], device='cuda:0') the target is: 2\nwhen input is tensor([0, 1, 2], device='cuda:0') the target is: 3\nwhen input is tensor([0, 1, 2, 3], device='cuda:0') the target is: 4\nwhen input is tensor([0, 1, 2, 3, 4], device='cuda:0') the target is: 5\nwhen input is tensor([0, 1, 2, 3, 4, 5], device='cuda:0') the target is: 6\nwhen input is tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0') the target is: 1\nwhen input is tensor([0, 1, 2, 3, 4, 5, 6, 1], device='cuda:0') the target is: 4\nwhen input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4], device='cuda:0') the target is: 1\nwhen input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1], device='cuda:0') the target is: 7\nwhen input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7], device='cuda:0') the target is: 8\nwhen input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7, 8], device='cuda:0') the target is: 9\nwhen input is tensor([0, 1, 2, 3, 4, 5, 6, 1, 4, 1, 7, 8, 9], device='cuda:0') the target is: 10\nwhen input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10],\n       device='cuda:0') the target is: 11\nwhen input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11],\n       device='cuda:0') the target is: 12\nwhen input is tensor([ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12],\n       device='cuda:0') the target is: 8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Revisiting some basics:\n\nTerms:\n- Block Size: The number of characters that the system has been trained to take into consideration while learning to predict the next character","metadata":{"id":"R_tCJuSyt_7b"}},{"cell_type":"code","source":"def get_batch(data, block_size, batch_size):\n    \"\"\"\n    This function is responsible for creating a batch of batch_size\n    For training a GPT model\n\n    \"\"\"\n    # Here we generate a tensor `ix` containing `batch_size` random\n    # indices within the range `0` to `len(data) - block_size`\n    # we substract `block_size` from the end so that the last\n    # selected block stays within the list of available characters\n    # in the text\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n\n    # Here we create a stack of tensors (batch size) each of length\n    # block_size that start from the\n    # above picked random indices\n    x = torch.stack([data[i:i+block_size] for i in ix])\n\n    # creates the target tensor y similarly, but shifted\n    # one position to the right, representing the next\n    # character to predict for each position in x.\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n","metadata":{"id":"lFYZnm2MuLlt","executionInfo":{"status":"ok","timestamp":1720393701015,"user_tz":240,"elapsed":187,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:03:49.191162Z","iopub.execute_input":"2024-07-08T01:03:49.191532Z","iopub.status.idle":"2024-07-08T01:03:49.198407Z","shell.execute_reply.started":"2024-07-08T01:03:49.191502Z","shell.execute_reply":"2024-07-08T01:03:49.197434Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Clarifying some more terms before we proceed to the next step:\n\n### Block Size\n- **Block Size** in a GPT model training code refers to the length of each segment of the input data that each training example consists of. This is directly analogous to what you might think of as \"sequence length\" in other contexts but is specifically termed \"block size\" in training scenarios for models like GPT.\n- In the context of the Transformer attention head depicted in the image below, each \"sequence\" or input processed through the attention head would be of a fixed length equivalent to the block size. In the image, while not specified as \"block size,\" the dimension that would correspond to this term is the middle dimension of the tensors, which is 32 in your example (`(8, 32, 64)`).\n\n### Sequence Length\n- **Sequence Length** in more general contexts refers to the total length of sequences being processed, which may vary unless specifically pre-processed to be uniform. In models like Transformers (and as seen in the attention head diagram), sequence length is typically fixed to a specific size for each training or inference pass. This fixed length is crucial for attention calculations across the entire sequence uniformly.\n- In the Transformer attention head diagram, this \"sequence length\" is manifest in each stage of the attention mechanism. It represents the number of positions (or tokens) in each sequence that the model processes simultaneously, marked as the second dimension in the tensors.\n\n### How It Relates to the Diagram\n- In the attention head diagram present below, all tensors maintain a consistent second dimension (32 in this case), reflecting the fixed sequence length or block size used for the calculations. This consistent dimensionality across layers and operations ensures that each token in the sequence can be related to every other token via the attention mechanism, a key feature enabling the model to capture complex dependencies across the input.\n- The operations like matrix multiplication (`matmul`) between the transposed keys and queries, and subsequent operations like softmax and dropout, all depend on this fixed sequence length to compute the attention scores and ultimately the output sequence. This fixed length, as used in your GPT model training, allows the Transformer to utilize positional relationships effectively.\n\n### Summary\nIn summary, in the Transformer model, as depicted by the attention head image below, block size and sequence length can be considered equivalent, referring to the fixed size of the input sequences used for training and inference. This term varies in usage depending on the model architecture but is crucial for models like Transformers that depend on a fixed dimension to compute relationships between all pairs of inputs within a sequence effectively.\n\n1. **Batch Size**: This is the number of samples processed before the model is updated. For exaple if we are are dealing with (8, 32, 64), the first dimension \"8\" typically represents the batch size. This means that the model processes 8 samples at a time.\n\n2. **Sequence Length**: This is the length of the input sequences each sample in a batch contains. In the example tensor, \"32\" represents the sequence length / block size, indicating each sample consists of 32 sequential elements or **tokens**. ***For instance, in natural language processing, this could represent 32 words in a sentence. In this example since we will be predicting characters, each token represents a character from the list of unique characters available.***\n\n3. **Feature Dimension**: This indicates the number of features each element of the sequence holds. The \"64\" in the example tensor suggests that each token or element of the sequence is represented by a vector of 64 features. These could be embeddings that encapsulate the token's meaning in a dense vector.\n\n4. **Block Size**: This term isn't explicitly shown in the diagram but is related to how data is structured or processed in blocks during certain operations. For instance, in memory management or in GPU computation, operations might be optimized by processing data in \"blocks.\" In the context of transformers or deep learning, block size might refer to the dimensionality of sub-parts of the model such as in splitting matrices for parallel processing, but typically it's not a term used to describe tensor dimensions directly.\n\nThus, in the below shown transformer model diagram:\n- Batch size: 8 (number of samples processed together)\n- Sequence length / block size: 32 (number of tokens, items, or steps per sample)\n- Feature dimension: 64 (features per token or step in each sequence)","metadata":{"id":"uYYGoQy13BEM"}},{"cell_type":"markdown","source":"### Single Self Attention Head (5 points)\n![](https://i.ibb.co/GWR1XG0/head.png)","metadata":{"id":"-HmnXJjxtm3N"}},{"cell_type":"markdown","source":"### Explaining the above artention head set up\n\nThis diagram depicts the computational flow in a typical attention head of a Transformer neural network, commonly used in models like GPT and BERT. Here’s a breakdown of the operations and their significance:\n\n1. **Input Tensor**: The input tensor has the shape (8, 32, 64), where 8 represents the batch size, 32 the sequence length / block size, and 64 the feature dimension of each token in the sequence.\n\n2. **Linear Layers**: Three parallel linear transformations are applied to the input tensor. Each layer outputs a tensor of shape (8, 32, 16). These transformations typically generate the queries (Q), keys (K), and values (V) which are used in the attention mechanism.\n\n3. **Transpose Operation**: The output of one of the linear layers (presumably representing keys, K) is transposed to change its shape from (8, 32, 16) to (8, 16, 32). This operation is necessary for matrix multiplication with the queries (Q). Here we notice that the transpose is only done across the second and third dimensions as the first dimension only represents the batch size\n\n4. **Matrix Multiplication (matmul)**: The output of the transpose operation (K transposed) is matrix-multiplied with another linear output (Q). ***This results in a shape of (8, 32, 32), representing the raw attention scores before they are normalized.***\n\n5. **Multiplication (mul)**: This operation might be an element-wise multiplication used as part of scaling the attention scores by the square root of the dimension of the keys to stabilize gradients during training, although the typical square root scaling is not explicitly shown here. More on why we scale the attention values below ...\n\n6. **Masked Fill**: This operation is used to apply masks to the attention scores. Masks are often used to ignore (or mask) padding tokens or future tokens during training in sequence models. The operation doesn't change the shape of the tensor. The upper right half of the matrix is made as zeros in this step.\n\n7. **Softmax**: The softmax function is applied across the last dimension (32) to normalize the attention scores to a probability distribution.\n\n8. **Dropout**: Dropout is a regularization technique where random elements of the tensor are zeroed out during training to prevent overfitting. The shape remains unchanged.\n\n9. **Matrix Multiplication (matmul)**: The normalized and possibly masked attention scores are then matrix-multiplied with the third linear output (V, values), resulting in an output tensor of shape (8, 32, 16). This operation computes the weighted sum of the values based on the attention scores.\n\n10. **Output Tensor**: The final output tensor is generated with the shape (8, 32, 16), likely to be fed into subsequent layers of the Transformer or processed further depending on the specific architecture and task.\n\nThis detailed flow illustrates how attention mechanisms selectively focus on different parts of the input sequence, weighting input features by relevance, which is central to the success of Transformer models in handling various sequence-based tasks in natural language processing.","metadata":{"id":"2XNdlBRl2VZO"}},{"cell_type":"markdown","source":"### The scaling of the attention scores\n\nThe scaling of the attention scores based on the dimension of the keys in the Transformer architecture addresses a specific challenge in training deep learning models that use softmax to calculate probabilities.\n\n### Background on Dot Products and Their Scale\nThe attention mechanism computes the dot products between the query and all keys in the sequence. These dot products are a critical component because they determine the attention scores that indicate how much each part of the input should contribute to the output at each position. However, the magnitude of the dot products depends on the dimensionality of the keys and queries. Here's why:\n\n- The dot product of two vectors increases with the number of dimensions. Specifically, if each component of the vectors is drawn from a distribution with a constant variance, the variance of the dot product is proportional to the dimensionality of the vectors.\n- As the dimension of the keys (and queries, since they are usually of the same dimension) increases, the average value of the dot products becomes larger. This can lead to extremely large values, especially when working with high-dimensional data, which is common in models like Transformers.\n\n### Impact on Softmax\nThe softmax function, which is used to convert these dot products into probabilities (or attention scores), is highly sensitive to changes in input values. Specifically:\n- Large values in the softmax input can lead to a situation where the softmax function's output is close to zero for all inputs except the largest one (a phenomenon often referred to as the softmax function \"saturating\"). This saturation can significantly slow down learning, as it leads to very small gradients during backpropagation — essentially, the network is less able to learn from the input data.\n\n### Why Scale by Square Root of Dimension?\nThe scaling factor used, \\(\\sqrt{d_k}\\) (where \\(d_k\\) is the dimension of the keys), helps mitigate these effects:\n- **Normalization**: By dividing the dot products by \\(\\sqrt{d_k}\\), you effectively normalize them, bringing their variance back to a more manageable scale. This normalization helps maintain a more uniform scale across different model sizes and configurations.\n- **Gradient Stability**: By keeping the dot products (and thus the inputs to the softmax) at a reasonable magnitude, the scaling prevents gradients under the softmax from becoming too small. This is crucial for efficient learning, as it ensures that each update step during training is informative enough to guide the model towards better performance without being too noisy or too minimal.\n\n### Conclusion\nScaling by the square root of the dimension of the keys is a practical approach to ensuring that the attention mechanism operates effectively across different settings and model scales, facilitating stable and efficient training. This method is particularly vital in deep learning architectures like Transformers, where models often deal with high-dimensional data and require careful handling of numerical stability during training operations.","metadata":{"id":"jjNgfYPJ8VwI"}},{"cell_type":"markdown","source":"### Terms relevant to constructing the SelfAttention Head:\n\nIn the context of Transformer architectures, the **head size** in an attention head refers to the dimension of the vectors used for each of the queries (Q), keys (K), and values (V) within a single attention head. This is a key parameter that defines how much information each attention head can capture.\n\n### Definition and Calculation\n\n- **Head Size**: The head size is essentially the dimensionality of the Q, K, and V vectors within each specific attention head. It is typically derived by dividing the total dimension of the model's embeddings (\\(d_{\\text{model}}\\)) by the number of attention heads (\\(\\text{num\\_heads}\\)). This allows the model to distribute the embedding information across multiple heads, each focusing on different features or relationships in the data.\n\n### Formula\nThe head size for queries and keys (\\(d_k\\) and \\(d_q\\)) is often the same and can be calculated as:\n\\[ d_k = d_q = \\frac{d_{\\text{model}}}{\\text{num\\_heads}} \\]\nFor values (\\(d_v\\)), it is usually the same as \\(d_k\\) and \\(d_q\\), though this can vary depending on specific model architectures or design choices:\n\\[ d_v = \\frac{d_{\\text{model}}}{\\text{num\\_heads}} \\]\n\n### Example\nIf a Transformer model uses an embedding dimension (\\(d_{\\text{model}}\\)) of 512 and has 8 attention heads:\n\\[ d_k = d_q = d_v = \\frac{512}{8} = 64 \\]\nThus, each head processes vectors of size 64 for queries, keys, and values.\n\n### Importance\nThe choice of head size affects how finely the model can focus on different aspects of the input data. Each head can potentially learn to attend to different parts of the sequence or different types of relationships:\n- **Smaller head sizes** can lead to a more focused and granular attention mechanism, where each head might specialize more distinctly.\n- **Larger head sizes** provide more capacity to each head, which can be useful for capturing more complex patterns or dependencies, but may reduce the diversity of what different heads can learn.\n\nAdjusting the head size is a balance between computational efficiency, capacity, and the diversity of information that the attention heads can capture. It's an important aspect of model tuning, especially in tasks requiring nuanced understanding of context or relationships within the data.\n\n### How does this translate to code\n\nFollowing this example: If a Transformer model uses an embedding dimension (\\(d_{\\text{model}}\\)) of 512 and has 8 attention heads:\n\\[ d_k = d_q = d_v = \\frac{512}{8} = 64 \\]\nThus, each head processes vectors of size 64 for queries, keys, and values.\n\nHere each layer q, k and v, instead of processing all the tokens that are a part of the embedding, will only process tokens that are passed into this head.\n\nHence the q, k and v linear layers will be of shape batch_size x head_size\n","metadata":{"id":"oc_Nwm3zMuUz"}},{"cell_type":"markdown","source":"### The Mask\n\nThe code snippet provided below involves creating a triangular mask and then applying it to an attention matrix in a Transformer model, typically used in tasks like text processing or sequence modeling. Let’s break down the two lines to understand what’s happening:\n\n### Line 1: Creating the Mask\n```python\nmask = torch.tril(torch.ones(timesteps, timesteps))\n```\n- **`torch.ones(timesteps, timesteps)`**: This function creates a 2D tensor (square matrix) filled with the value `1`, where the dimensions of the matrix are both `timesteps`. `timesteps` could be the length of a sequence being processed, such as the number of words in a sentence.\n- **`torch.tril()`**: This function takes a tensor and returns a lower triangular part of the matrix. It zeroes out all elements above the main diagonal. The main diagonal and the elements below remain as they were, which in this case, are all `1`s due to the `torch.ones()` function. This triangular matrix is typically used in attention mechanisms to ensure that the attention calculation for a given timestep only considers that timestep and the ones before it (i.e., ensuring causality in models like GPT).\n\n### Line 2: Applying the Mask to the Attention Matrix\n```python\nmasked_attention = attention.masked_fill(mask == 0, float('-inf'))\n```\n- **`mask == 0`**: This operation compares each element of the `mask` tensor to `0`. Since `mask` is a lower triangular matrix with `1`s in the lower triangle and `0`s elsewhere, this operation generates a Boolean tensor where `True` corresponds to the positions where the mask had `0`s (i.e., the upper triangular part of the matrix) and `False` everywhere else (i.e., the lower triangular part).\n- **`masked_fill()`**: This method is called on the `attention` tensor. It takes two arguments: a mask and a value to fill. The mask here is the Boolean tensor from `mask == 0`. Wherever the mask is `True`, the `attention` tensor is filled with `float('-inf')`. This effectively applies the mask by setting the attention scores in the upper triangle (those that should not be considered due to causality) to negative infinity.\n\n### Why `float('-inf')`?\nIn attention mechanisms, especially when followed by a softmax operation, setting values to negative infinity before softmax ensures that those values have zero probability. When softmax is applied to a vector containing negative infinity, the exponential of negative infinity is zero, hence those positions do not contribute to the output of the softmax.\n\n### Summary\n- The `mask` tensor is used to enforce causality in the attention mechanism by preventing the model from attending to future timesteps in the sequence. This is essential in models like GPT where predictions for a given position should only depend on previous positions.\n- The `mask == 0` operation identifies positions that should be ignored (in this context, future timesteps), and `masked_fill` applies this by setting such positions in the attention matrix to negative infinity, effectively removing them from consideration during attention normalization (softmax).","metadata":{"id":"dk6--KJhSp6c"}},{"cell_type":"code","source":"class SelfAttentionHead(nn.Module):\n  \"\"\"\n  This class implements a single self attention head\n  For the input dimensions we have batch size , sequence length , feature dimension\n\n  First we need to implement the K, Q and V layers\n  These are three linear layers - we can refer to\n  https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n\n  For a\n  \"\"\"\n  def __init__(self, num_heads, n_embed):\n    super().__init__()\n    self.k = nn.Linear(n_embed, n_embed, bias=False)\n    self.q = nn.Linear(n_embed, n_embed, bias=False)\n    self.v = nn.Linear(n_embed, n_embed, bias=False)\n\n  def forward(self, x):\n    \"\"\"\n    The forward step contains the following steps\n    1. Pass x through the linear layers\n    2. Perform transpose operation on k\n    3. Calculate the bidirectional attention value q.k\n    4. Scaling the attention value\n    5. Masked fil\n    6. Softmax\n    7. Dropout\n    8. Matrix multiplication with v\n\n    Batch, Tokens, Chanels\n    \"\"\"\n    # print(x.shape)\n    B, T, C = x.shape\n\n    # print shape of k\n    # print(f\"K shape {self.k.weight.shape}\")\n    # print shape of q\n    k = self.k(x)\n    q = self.q(x)\n    v = self.v(x)\n\n    # print(k.shape)\n    # print(q.shape)\n    # print(v.shape)\n\n    # Transpose the dimensions\n    k = k.transpose(1, 2) # Here we transpose dimensions 1 and 2\n    # k is now of dimension B, C, T\n\n    # print(f\"kShape after transpose {k.shape}\")\n    # Here we calculate the bidirectional attention value q.k\n    attention = torch.matmul(q, k) # B, T, C * B, C, T = B, T, T\n\n    # print(f\"Attention shape {attention.shape}\")\n    # Scaling the attention value by the sq root of the chanels / features\n    attention = attention * C**-0.5 # this will be of dimension B, T, T\n\n\n    # print(f\"Attention shape {attention.shape}\")\n    # Masked fill\n    mask = torch.tril(torch.ones(T, T)).to(device) # We create a mask of dimensions C, C\n    # print shape of mask\n    # print(f\"Mask shape {mask.shape}\")\n\n    # We apply the mask to the attention matrix\n    # float('-inf') is applied to all positions where the mask value is 0\n    masked_attention = attention.masked_fill(mask == 0, float('-inf'))\n\n    # print(f\"Masked attention shape {masked_attention.shape}\")\n\n    # Softmax\n    # the dimension value has to be set to -1 to indicate the last dimension\n    attention = F.softmax(masked_attention, dim=-1)\n\n    # print(f\"Softmax attention shape {attention.shape}\")\n\n    # Dropout\n    attention = F.dropout(attention, p=0.1)\n\n    # print(f\"Dropout attention shape {attention.shape}\")\n\n    # Matrix multiplication with v\n    output = torch.matmul(attention, v)\n    # print(f\"Attention shape {attention.shape}\")\n    # print(f\"V shape {v.shape}\")\n    # print(f\"Output shape {output.shape}\")\n\n    return output","metadata":{"id":"5SD8Z16R-sfZ","executionInfo":{"status":"ok","timestamp":1720393711192,"user_tz":240,"elapsed":172,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:03:59.586290Z","iopub.execute_input":"2024-07-08T01:03:59.586776Z","iopub.status.idle":"2024-07-08T01:03:59.597915Z","shell.execute_reply.started":"2024-07-08T01:03:59.586745Z","shell.execute_reply":"2024-07-08T01:03:59.596925Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Multihead Self Attention (5 points)\n\n`constructor`\n\n- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n- Create a linear layer with n_embd input dim and n_embd output dim\n\n`forward`\n\nIn the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n\n![](https://i.ibb.co/y5SwyZZ/multihead.png)","metadata":{"id":"LWeoHGBiFpWd"}},{"cell_type":"markdown","source":"The image and the accompanying task outline depict the implementation steps for a Multihead Self-Attention mechanism, which is a core component of models like the Transformer. Here’s a breakdown of each step in the process:\n\n### Constructor\n1. **SelfAttentionHead Instances**:\n   - **Purpose**: Each SelfAttentionHead is responsible for capturing different aspects of the input data through separate attention mechanisms. This diversification allows the model to attend to different features or parts of the input simultaneously.\n   - **Implementation**: Using `nn.ModuleList` to create and manage these instances is efficient because it automatically handles the forward pass for each module within a list, facilitating easier batch processing and gradient updates.\n   - **Configuration**: You are instructed to create four such heads, which means the input will be divided and processed by these four heads independently in parallel.\n\n2. **Linear Layer**:\n   - **Purpose**: After processing the input through multiple attention heads, the outputs are concatenated. This linear layer then projects the concatenated output back to the desired embedding dimension (`n_embd`). It integrates information from all the attention heads.\n   - **Specification**: The linear layer should have an input dimension and output dimension equal to `n_embd`, which matches the input's embedding size to ensure dimensional consistency across the network.\n\n### Forward Method\n1. **Pass Input Through Each Head**:\n   - **Process**: The input tensor `x` is passed through each of the four SelfAttentionHead instances. Since each head may focus on different features or relations within the data, they may produce varying outputs.\n   - **Output Dimension**: Assuming each head preserves the dimensionality of its output to be smaller or equal to the input dimension divided by the number of heads (for simplicity, let's assume it's divided equally), each head would output a tensor with a shape of `[batch_size, seq_length, n_embd/4]`.\n\n2. **Concatenate Outputs**:\n   - **Function**: The outputs from all heads are concatenated along the feature dimension (last dimension). This step combines the different \"views\" or \"attentions\" the heads have calculated into a single tensor.\n   - **Resulting Dimension**: After concatenation, the dimension of the output tensor would be `[batch_size, seq_length, n_embd]` because each head's output is concatenated to form the full embedding dimension.\n\n3. **Pass Through Linear Layer**:\n   - **Purpose**: The concatenated tensor is then passed through the linear layer created in the constructor. This layer acts as a transformation that can mix information across the different attention mechanisms, potentially aiding in better information integration.\n   - **Output**: The output of this linear layer is a tensor of the same shape as the input to the layer, `[batch_size, seq_length, n_embd]`, thus ensuring the output matches the input dimension of the sequence in terms of embedding size.\n\n4. **Dropout Application**:\n   - **Consideration**: Although not detailed in your task outline, typically, a dropout layer would follow the linear layer to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training. This step helps in making the model more robust.\n\n### Summary\nIn summary, the multihead self-attention mechanism processes the input through multiple attention heads, each potentially focusing on different features. The outputs of these heads are then integrated via concatenation and further transformed by a linear layer to ensure that the model can leverage information across various heads effectively. This architecture is crucial for complex sequence modeling tasks where different parts of the input sequence carry different types of information relevant to the task.","metadata":{"id":"NVuc9V16VjzM"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embed):\n        super().__init__()\n        # print(f\"Num heads {num_heads}\")\n        # print(f\"Head size {head_size}\")\n        # print(f\"n_embed {n_embed}\")\n        # Self attention head instances\n        self.heads = nn.ModuleList([SelfAttentionHead(num_heads, n_embed) for _ in range(num_heads)])\n        # Linear layer\n        # The linear layer should have an input dimension and output dimension\n        # equal to n_embd, which matches the input's embedding size to ensure\n        # dimensional consistency across the network.\n        self.proj = nn.Linear(n_embed, n_embed)\n\n    def forward(self, x):\n        \"\"\"\n        The forward step consists of the following steps\n        1. Pass x through each head\n        2. Concatenate all the outputs along the feature dimension\n        3. Pass the concatenated output through the linear layer\n        \"\"\"\n        # print(f\"Inside multi head attention\")\n        # Pass x through each head\n        heads = [head(x) for head in self.heads] # (B, t, C)[]\n\n        # print output dimension of each head\n        # print(f\"Head output dimension {heads[0].shape}\")\n\n        # print(f\"After passing through heads\")\n        # Concatenate all the outputs along the feature dimension\n        x = torch.cat(heads, dim=-1) # B, T, C\n        # print shape of x\n        # print(f\"X shape {x.shape}\")\n\n        # print shape of projection\n        # print(f\"Projection shape {self.proj.weight.shape}\")\n        # print(f\"After concatenating\")\n        # Pass the concatenated output through the linear layer\n        x = self.proj(x) # B, T, C\n\n        # print(f\"After passing through linear projection layer\")\n\n        return x\n\n\n","metadata":{"id":"gFsPDkpnFs_b","executionInfo":{"status":"ok","timestamp":1720393313133,"user_tz":240,"elapsed":241,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:04.469804Z","iopub.execute_input":"2024-07-08T01:04:04.470496Z","iopub.status.idle":"2024-07-08T01:04:04.478260Z","shell.execute_reply.started":"2024-07-08T01:04:04.470463Z","shell.execute_reply":"2024-07-08T01:04:04.477397Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## MLP (2 points)\nImplement a 2 layer MLP\n\n\n![](https://i.ibb.co/C0DtrF5/ff.png)","metadata":{"id":"uH_0ELyZ8YCC"}},{"cell_type":"markdown","source":"The diagram provided illustrates a sequence of operations that form a typical Feedforward Neural Network (FFN) module, commonly used within the architecture of a Transformer model. This specific sequence of operations represents the \"position-wise feedforward network\" component of the Transformer's architecture. Here’s a breakdown of each step and its function:\n\n### 1. **Linear Layer**\n- **Input**: \\( (8, 32, 64) \\)\n- **Output**: \\( (8, 32, 256) \\)\n- **Description**: This layer is a fully connected neural network layer that projects each input feature from a 64-dimensional space to a 256-dimensional space. The input tensor's first dimension typically represents the batch size (8), the second dimension the sequence length (32), and the third dimension the feature size (64). The expansion in feature size allows the network to learn more complex features at each position of the sequence.\n\n### 2. **ReLU Activation**\n- **Input/Output**: \\( (8, 32, 256) \\)\n- **Description**: The ReLU (Rectified Linear Unit) activation function is applied element-wise. It introduces non-linearity into the model, which is essential for learning complex patterns. ReLU is defined as \\( f(x) = max(0, x) \\), effectively setting all negative values to zero and keeping positive values unchanged. This operation does not alter the dimensions of the data.\n\n### 3. **Second Linear Layer**\n- **Input**: \\( (8, 32, 256) \\)\n- **Output**: \\( (8, 32, 64) \\)\n- **Description**: Another fully connected layer that projects the features back from the 256-dimensional space to the original 64-dimensional space. This step is crucial for matching the dimensions of the output with other components in a Transformer model, such as the self-attention outputs, allowing for subsequent operations like residual connections.\n\n### 4. **Dropout**\n- **Input/Output**: \\( (8, 32, 64) \\)\n- **Description**: Dropout is a regularization technique used to prevent overfitting in neural networks. It randomly sets a fraction of the input units to zero during training at each update step, which helps to make the model robust and less likely to rely on any small set of neurons. The dropout rate (the probability of setting a value to zero) is a hyperparameter that can be tuned. The operation does not change the dimensions of the tensor.\n\n### 5. **Output Tensor**\n- **Dimension**: \\( (8, 32, 64) \\)\n- **Description**: The final output tensor retains the dimensions of the input tensor to the entire module. This output is typically fed back into the main flow of the Transformer model, often added to the input tensor through a residual connection before being normalized and passed on to the next layer or operation in the model.\n\n### Summary\nThis feedforward network within a Transformer model performs crucial transformation and re-projection of features, enhancing the model's ability to capture and manipulate information. The use of non-linearity (ReLU) and regularization (Dropout) helps in learning non-trivial patterns and generalizing better to unseen data. Each component is dimensionally coordinated to maintain consistency throughout the model, allowing for seamless integration with other modules such as multi-head attention.","metadata":{"id":"dRhe_OuqdAWH"}},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, embed_dim, scale_up_factor):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, embed_dim * scale_up_factor, bias=False)\n        self.fc2 = nn.Linear(embed_dim * scale_up_factor, embed_dim, bias=False)\n\n    def forward(self, x: torch.tensor) -> torch.tensor:\n        \"\"\"\n        The forward step of the MLP has the following steps\n        1. Pass x through the first linear layer\n        2. Apply ReLU activation\n        3. Pass the output through the second linear layer\n        4. Apply dropout\n        \"\"\"\n        x = F.relu(self.fc1(x)) # B, T, C*scale_up\n        x = self.fc2(x) # B, T, C\n        x = F.dropout(x, p=0.1) # B, T, C\n        return x","metadata":{"id":"K96Z3kAv7lNt","executionInfo":{"status":"ok","timestamp":1720393320141,"user_tz":240,"elapsed":2,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:08.106218Z","iopub.execute_input":"2024-07-08T01:04:08.106622Z","iopub.status.idle":"2024-07-08T01:04:08.113758Z","shell.execute_reply.started":"2024-07-08T01:04:08.106569Z","shell.execute_reply":"2024-07-08T01:04:08.112748Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Transformer block (20 points)\n\nLayer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n\nDropout is a form of regularization to prevent overfitting.\n\nThis is the diagram of a transformer block:\n\n![](https://i.ibb.co/X85C473/block.png)","metadata":{"id":"bUFxuyf-JIxr"}},{"cell_type":"markdown","source":"### The transformed block\nThe diagram provided outlines the structure of a typical Transformer block, which is a fundamental component of Transformer models widely used in natural language processing tasks. Here’s a breakdown of each step and component shown in the diagram:\n\n### 1. **Input Tensor**\n- **Shape**: (8, 32, 64)\n- **Description**: This tensor represents the input to the Transformer block, where 8 could be the batch size, 32 the sequence length, and 64 the dimensionality of each vector in the sequence.\n\n### 2. **Layer Normalization**\n- **Input/Output Shape**: (8, 32, 64)\n- **Depth**: 1\n- **Description**: The first operation is layer normalization. Layer normalization is applied within each sample independently and normalizes the data across the feature dimension (across the 64 features of each of the sequence's 32 positions). This helps in stabilizing the learning process by normalizing the inputs to have zero mean and unit variance, which can improve training speed and stability.\n\n### 3. **Multi-Head Attention**\n- **Input/Output Shape**: (8, 32, 64)\n- **Depth**: 1\n- **Description**: This component uses multiple sets of attention mechanisms (heads) to process the input. Each head can attend to different parts of the input sequence, allowing the model to capture various aspects of the sequence in parallel. The output of this module is typically the same size as the input, allowing for residual connections.\n\n### 4. **Addition (Residual Connection)**\n- **Input**: 2 x (8, 32, 64)\n- **Output Shape**: (8, 32, 64)\n- **Depth**: 1\n- **Description**: The output from the multi-head attention is added to the original input (prior to layer normalization). This is known as a residual connection and helps in mitigating the vanishing gradient problem by allowing gradients to flow directly through the network.\n\n### 5. **Second Layer Normalization**\n- **Input/Output Shape**: (8, 32, 64)\n- **Depth**: 1\n- **Description**: Another layer normalization step is applied after the residual connection and before the feedforward network. This normalizes the data again, preparing it for further processing and maintaining stability in deeper layers.\n\n### 6. **FeedForward Network**\n- **Input/Output Shape**: (8, 32, 64)\n- **Depth**: 1\n- **Description**: This is typically a position-wise feedforward neural network, which means it applies the same neural network to each position independently. It usually consists of two linear transformations with a nonlinear activation function in between. This network can expand (and later compress) the internal representation, allowing the model to mix features before passing them to the next layer.\n\n### 7. **Addition (Second Residual Connection)**\n- **Input**: 2 x (8, 32, 64)\n- **Output Shape**: (8, 32, 64)\n- **Depth**: 1\n- **Description**: Similar to the earlier addition, this step adds the output of the feedforward network to the input of the feedforward network (the output from the previous layer normalization). This second residual connection further helps in preserving information throughout layers and aids in training deeper networks.\n\n### 8. **Output Tensor**\n- **Shape**: (8, 32, 64)\n- **Depth**: 0\n- **Description**: The final output tensor of the Transformer block maintains the same shape as the input tensor, ensuring that multiple such blocks can be stacked without dimension mismatch.\n\n### Summary\nThis diagram presents a classic configuration of a Transformer block, which utilizes normalization, attention mechanisms, residual connections, and feedforward networks to process sequential data effectively. Each component plays a crucial role in ensuring the model learns effectively and generalizes well across different tasks and data distributions. The design of the Transformer block with layer normalization and dropout is specifically tailored to enhance training stability and prevent overfitting, making it highly effective for tasks requiring the handling of complex sequential data.","metadata":{"id":"z8vO2lx4gFnO"}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, n_embd: int, n_head: int, sequence_length: int):\n        \"\"\"\n        Architecture of each transformer block\n        1. Layer norm\n        2. Multihead attention\n        3. Layer norm\n        4. Feedforward network\n        \"\"\"\n        super().__init__()\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n        self.attn = MultiHeadAttention(n_head, sequence_length//n_head, n_embd)\n        self.mlp = MLP(n_embd, 4)\n\n    def forward(self, x):\n        \"\"\"\n        The forward pass of a transformer block consists of the following steps\n        1. Layer norm\n        2. Multihead attention\n        3. Addition (residual connection)\n        4. Layer norm\n        5. Feedforward network\n        6. Addition (second residual connection)\n        \"\"\"\n        # print(f\"First line of transformer block\")\n        # Adding layer normalization to each sample individually\n        norm = self.ln1(x)\n\n        # print(f\"In transformer block\")\n        # print(f\"After first layer norm\")\n\n        # Calculating the attention values\n        attn = self.attn(norm)\n\n        # print(f\"After calculating attention\")\n\n        # Adding the residual connection\n        x = x + attn\n\n        # Adding layer normalization after the residual connection\n        norm = self.ln2(x)\n        # print(f\"After second layer norm\")\n        # Calculating the feedforward network\n        mlp = self.mlp(norm)\n        # print(f\"After feedforward network\")\n        # Adding the second residual connection\n        x = x + mlp\n        # print(f\"After second residual connection\")\n        # Returning the output of the transformer block\n        return x","metadata":{"id":"xTDAd66KIvvx","executionInfo":{"status":"ok","timestamp":1720393320866,"user_tz":240,"elapsed":3,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:11.887301Z","iopub.execute_input":"2024-07-08T01:04:11.887989Z","iopub.status.idle":"2024-07-08T01:04:11.895758Z","shell.execute_reply.started":"2024-07-08T01:04:11.887960Z","shell.execute_reply":"2024-07-08T01:04:11.894869Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## GPT\n\n`constructor` (5 points)\n\n1. create the token embedding table and the position embedding table\n2. create variable `self.blocks` that is a series of 4 `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n3. create a layer norm layer\n4. create a linear layer for predicting the next token\n\n`forward(self, idx, targets=None)`. (5 points)\n\n`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n1. get the token by using the token embedding table created in the constructor\n2. create the position embeddings\n3. sum the token and position embeddings to get the model input\n4. pass the model through the blocks, the layernorm layer, and the final linear layer\n5. compute the loss\n\n`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` (5 points)\n1. implement top p, top_k, and temperature for sampling\n\n","metadata":{"id":"SyFQXltDKNti"}},{"cell_type":"markdown","source":"![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)","metadata":{"id":"0Xa2bh2XDdKy"}},{"cell_type":"markdown","source":"### Explannation of concepts\n\n***Positional Encoding***\n\nPositional embeddings are a crucial component in Transformer-based models like GPT, where the architecture lacks any inherent mechanism to track the sequence order of the input tokens. Let's delve into how positional embeddings work and clarify their function.\n\n### Purpose of Positional Embeddings\nPositional embeddings provide the model with information about the relative or absolute position of tokens in the sequence. Since models like GPT process all input tokens simultaneously without recurrence or convolution, they rely on these embeddings to incorporate the order of the sequence into their calculations. This positional information is essential for tasks that depend on the order of words or characters, such as language understanding and generation.\n\n### How Positional Embeddings are Implemented\nHere's a step-by-step explanation:\n\n1. **Token Embeddings**:\n   - `token = self.token_embedding(idx)` retrieves embeddings for each token in the batch based on their indices (`idx`). These embeddings capture the semantic meaning of each token.\n   - Output shape: `[B, T, C]` where `B` is the batch size, `T` is the sequence length, and `C` is the embedding dimension.\n\n2. **Generating Positional Indices**:\n   - `position = torch.arange(0, idx.shape[1])` generates a tensor of positions from `0` to `T-1` (where `T` is the sequence length). This tensor is used to fetch positional embeddings.\n   - This line creates a tensor that effectively acts as an index for each position in the sequence, where `idx.shape[1]` corresponds to the sequence length `T`.\n\n3. **Positional Embeddings Lookup**:\n   - `position = self.position_embedding(position)` uses the position indices to retrieve the positional embeddings from a defined embedding table (`self.position_embedding`). This table is usually initialized randomly and then learned during training.\n   - It's important to note that there might be an error in this part of the code because the positional indices need to be expanded for all batches in the input. Typically, you would need to repeat or expand these indices to match the batch size `B`. Here's how you could do it:\n     ```python\n     position = self.position_embedding(position.unsqueeze(0).repeat(B, 1, 1))\n     ```\n   - After correction, `position` will have the same shape as `token`, i.e., `[B, T, C]`.\n\n4. **Combining Token and Positional Embeddings**:\n   - `x = token + position` sums the token embeddings and positional embeddings element-wise. The addition operation allows the model to consider both the semantic meaning of each token and its position in the sequence simultaneously.\n   - The resulting tensor `x` is then used as the input for the subsequent layers of the model. This combined embedding retains the shape `[B, T, C]`.\n\n### Summary\nPositional embeddings are critical for providing the necessary context of token order to Transformer models, enabling them to effectively process sequences of data. The combination of token and positional embeddings gives the model a comprehensive understanding of both the meaning of tokens and their positional relationships within sequences.","metadata":{"id":"dwySsTf4rCTo"}},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self, n_embd, n_head, vocab_size, sequence_length):\n        super().__init__()\n        # Embedding tables\n        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding = nn.Embedding(sequence_length, n_embd)\n\n        # Transformer Blocks\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head, sequence_length),\n            Block(n_embd, n_head, sequence_length),\n            Block(n_embd, n_head, sequence_length),\n            Block(n_embd, n_head, sequence_length)\n        )\n\n        # Layer Norm layer\n        self.ln = nn.LayerNorm(n_embd)\n\n        # Linear layer\n        # This will output values for the possible n number of tokens\n        self.linear = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        \"\"\"\n        forward takes `idx`,  a batch of context ids as input of size (B, T)\n        If targets is None, return the logits and None.\n        Else returns the logits and the loss\n\n\n        1. get the token by using the token embedding table created in the constructor\n        2. create the position embeddings\n        3. sum the token and position embeddings to get the model input\n        4. pass the model through the blocks, the layernorm layer, and the final linear layer\n        5. compute the loss\n        \"\"\"\n\n        # Get the token by using the token embedding table\n        token = self.token_embedding(idx) # B, T, C\n\n\n        # Create the position indices\n        # position = torch.arange(0, idx.size(1), device=idx.device)  # [T]\n\n        # Expand position indices to match the batch size\n        # and retrieve the corresponding positional embeddings\n        # position = self.position_embedding(position)[None, :, :].repeat(idx.size(0), 1, 1)  # [B, T, C]\n\n        # Improved way of calculating position embedding using torch's broadcasting function\n        position = self.position_embedding(torch.arange(idx.size(1), device=idx.device)).expand(idx.size(0), -1, -1)\n\n        # Sum the token and position embeddings to get the model input\n        x = token + position\n        # print(f\"After encoding\")\n        # Pass the model through the blocks, the layernorm layer, and the final linear layer\n        x = self.blocks(x)\n        # print(f\"After transformer blocks\")\n        x = self.ln(x)\n        # print(f\"After layer norm\")\n\n        logits = self.linear(x)\n        # print(f\"After linear layer\")\n\n        if targets is None:\n            return logits, None\n\n        # Compute the loss\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\n        return logits, loss","metadata":{"id":"8WT4oUN084ts","executionInfo":{"status":"ok","timestamp":1720393326574,"user_tz":240,"elapsed":1,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:17.173402Z","iopub.execute_input":"2024-07-08T01:04:17.173785Z","iopub.status.idle":"2024-07-08T01:04:17.186049Z","shell.execute_reply.started":"2024-07-08T01:04:17.173757Z","shell.execute_reply":"2024-07-08T01:04:17.185066Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\ndef generate(model, start_char, max_new_tokens, top_p, top_k, temperature):\n    \"\"\"\n    implement top p, top_k, and temperature for sampling\n    Generate text starting from start_char with controlled sampling\n    \"\"\"\n\n    start_token_index = encode([start_char])\n    print(\"Encoded indices:\", start_token_index)\n\n    # Check if indices are within the expected range\n    if any(idx >= len(chars) or idx < 0 for idx in start_token_index):\n        raise ValueError(\"Encoded index out of bounds. Check your vocabulary and encoding function.\")\n\n\n    input_ids = torch.tensor([start_token_index], dtype=torch.long).to(model.token_embedding.weight.device)\n\n    model.eval()  # Put the model in evaluation mode\n    generated_text = [start_char]  # List to collect generated characters\n\n    # print input ids\n    # print(input_ids)\n\n    for _ in range(max_new_tokens):\n      # Extracts the logits form the last batch, last token\n      # Here since we are working with characters, we only have one entry in a batch with one\n      # element in the sequence\n      # So we extract the logits generated for next character based on the input sequence\n      output = model.forward(input_ids)[0]\n      print(output.shape)\n      logits = output[-1, -1, :]\n\n      # Apply temperature\n      logits = logits / temperature\n\n      # Filter the logits with top k sampling\n      top_k_values, top_k_indices = torch.topk(logits, top_k)\n      logits = torch.zeros_like(logits).scatter_(0, top_k_indices, top_k_values)\n\n      # Apply softmax to convert to probabilities\n      probs = F.softmax(logits, dim=-1)\n\n      # Filter the logits with top p sampling\n      sorted_indices = torch.argsort(probs, descending=True)\n      sorted_probs = torch.sort(probs, descending=True)[0]\n      cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n      sorted_indices_to_remove = cumulative_probs > top_p\n      # Shift the mask to the right to keep at least one token\n      sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n      sorted_indices_to_remove[0] = 0\n\n      probs[sorted_indices[sorted_indices_to_remove]] = 0\n\n      # Renormalize the probabilities\n      probs /= probs.sum()\n\n      # Sample from the modified distribution\n      next_token_id = torch.multinomial(probs, 1).item()\n\n      # print(next_token_id)\n      # print(decode([next_token_id]))\n\n      # Add the sampled token to the input sequence\n      # input_ids = torch.cat([input_ids, torch.tensor([next_token_id], dtype=torch.long).to(input_ids.device)], dim=-1)\n\n      # Create a new tensor for next_token_id with an extra dimension\n      # Using 'unsqueeze' to add a sequence length dimension\n      next_token_id_tensor = torch.tensor([next_token_id], dtype=torch.long).to(input_ids.device).unsqueeze(0)\n\n      # Concatenate along the sequence length dimension (dim=1)\n      input_ids = torch.cat([input_ids, next_token_id_tensor], dim=-1)\n\n\n      next_char = decode([next_token_id])\n      generated_text.append(next_char)\n\n    return ''.join(generated_text)","metadata":{"id":"tXnu4Sg7QibG","executionInfo":{"status":"ok","timestamp":1720393486024,"user_tz":240,"elapsed":186,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:27.925553Z","iopub.execute_input":"2024-07-08T01:04:27.926275Z","iopub.status.idle":"2024-07-08T01:04:27.939740Z","shell.execute_reply.started":"2024-07-08T01:04:27.926241Z","shell.execute_reply":"2024-07-08T01:04:27.938640Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Training loop (15 points)\n\nimplement training loop","metadata":{"id":"Njzrwwiv-mfB"}},{"cell_type":"code","source":"model = GPT(n_embd, n_head, 65 ,block_size).to(device) # make you are running this on the GPU","metadata":{"id":"oJ0D51wNYyJD","executionInfo":{"status":"ok","timestamp":1720393738625,"user_tz":240,"elapsed":555,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:35.235547Z","iopub.execute_input":"2024-07-08T01:04:35.236256Z","iopub.status.idle":"2024-07-08T01:04:35.570259Z","shell.execute_reply.started":"2024-07-08T01:04:35.236224Z","shell.execute_reply":"2024-07-08T01:04:35.569284Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import Adam\n\n# Setup the optimizer\noptimizer = Adam(model.parameters(), lr=0.001)\n\n# Define the loss function\nloss_fn = nn.CrossEntropyLoss()","metadata":{"id":"IrdLrtMtVjNR","executionInfo":{"status":"ok","timestamp":1720393739833,"user_tz":240,"elapsed":4,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:37.835381Z","iopub.execute_input":"2024-07-08T01:04:37.835757Z","iopub.status.idle":"2024-07-08T01:04:39.140659Z","shell.execute_reply.started":"2024-07-08T01:04:37.835728Z","shell.execute_reply":"2024-07-08T01:04:39.139903Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data = encode(text)  # Convert entire text to indices\ndata = torch.tensor(data, dtype=torch.long)  #","metadata":{"id":"QNpg11owWm2o","executionInfo":{"status":"ok","timestamp":1720393741512,"user_tz":240,"elapsed":185,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:40.096284Z","iopub.execute_input":"2024-07-08T01:04:40.097338Z","iopub.status.idle":"2024-07-08T01:04:40.836338Z","shell.execute_reply.started":"2024-07-08T01:04:40.097297Z","shell.execute_reply":"2024-07-08T01:04:40.835525Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Assume 'data' is already a long tensor of encoded text\n# Randomly split data into training and validation sets\ntrain_size = int(0.9 * len(data))\ntrain_data, val_data = data[:train_size], data[train_size:]","metadata":{"id":"gOWPEILkay0i","executionInfo":{"status":"ok","timestamp":1720393742634,"user_tz":240,"elapsed":188,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"execution":{"iopub.status.busy":"2024-07-08T01:04:42.541065Z","iopub.execute_input":"2024-07-08T01:04:42.541398Z","iopub.status.idle":"2024-07-08T01:04:42.546337Z","shell.execute_reply.started":"2024-07-08T01:04:42.541373Z","shell.execute_reply":"2024-07-08T01:04:42.545303Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_data.shape, val_data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FYIML2D2c6c6","executionInfo":{"status":"ok","timestamp":1720393744196,"user_tz":240,"elapsed":186,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"aa198c2b-7cbf-4da8-8960-bb73087b5dd4","execution":{"iopub.status.busy":"2024-07-08T01:04:44.508965Z","iopub.execute_input":"2024-07-08T01:04:44.509364Z","iopub.status.idle":"2024-07-08T01:04:44.515597Z","shell.execute_reply.started":"2024-07-08T01:04:44.509337Z","shell.execute_reply":"2024-07-08T01:04:44.514657Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1003854]), torch.Size([111540]))"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\n# TensorBoard for monitoring\nwriter = SummaryWriter()\n\nval_loss_list = []\ntrain_loss_list = []\n\nfor iter in range(max_iters):\n    model.train()\n    x, y = get_batch(train_data, block_size, batch_size)\n    # print(x.shape, y.shape)\n    # Forward pass\n    logits, _ = model(x)\n\n    # print(logits.shape, y.shape)\n\n    # Reshape logits to [batch_size * sequence_length, vocab_size]\n    # and y to [batch_size * sequence_length]\n    logits = logits.view(-1, logits.size(-1))  # logits.size(-1) is 65 here\n    y = y.view(-1)\n\n    # Compute loss\n    loss = loss_fn(logits, y)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Logging\n    if iter % 100 == 0:\n        print(f\"Iteration {iter}: Loss {loss.item()}\")\n        train_loss_list.append(loss.item())\n        writer.add_scalar('Training Loss', loss.item(), iter)\n\n        # Validation step\n        with torch.no_grad():\n            model.eval()\n            x_val, y_val = get_batch(val_data, block_size, batch_size)\n            logits_val, _ = model(x_val)\n\n            # Reshape logits to [batch_size * sequence_length, vocab_size]\n            # and y to [batch_size * sequence_length]\n            logits_val = logits_val.view(-1, logits_val.size(-1))  # logits.size(-1) is 65 here\n            y_val = y_val.view(-1)\n\n            # Compute loss\n            val_loss = loss_fn(logits_val, y_val)\n            val_loss_list.append(val_loss.item())\n            # val_loss = loss_fn(logits_val.view(-1, vocab_size), y_val.view(-1))\n            print(f\"Validation Loss: {val_loss.item()}\")\n            writer.add_scalar('Validation Loss', val_loss.item(), iter)\n\nwriter.close()\n\ntorch.save(model.state_dict(), 'gpt_model.pth')\n","metadata":{"id":"qWtn2uTwYUrY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da32b30c-5ca7-4adb-8dc3-e36f7c3b0f8c","execution":{"iopub.status.busy":"2024-07-08T01:04:53.013380Z","iopub.execute_input":"2024-07-08T01:04:53.014199Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-07-08 01:04:55.556684: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-08 01:04:55.556809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-08 01:04:55.710912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Iteration 0: Loss 4.338671684265137\nValidation Loss: 5.4599690437316895\nIteration 100: Loss 2.5094008445739746\nValidation Loss: 2.537811279296875\nIteration 200: Loss 2.4577643871307373\nValidation Loss: 2.4633443355560303\nIteration 300: Loss 2.1233811378479004\nValidation Loss: 2.2226247787475586\nIteration 400: Loss 2.025768756866455\nValidation Loss: 2.1447715759277344\nIteration 500: Loss 1.9679816961288452\nValidation Loss: 2.1172282695770264\nIteration 600: Loss 1.992120623588562\nValidation Loss: 2.1238784790039062\nIteration 700: Loss 1.9585719108581543\nValidation Loss: 2.091581106185913\nIteration 800: Loss 2.0059618949890137\nValidation Loss: 2.0743470191955566\nIteration 900: Loss 1.9871422052383423\nValidation Loss: 2.1302638053894043\nIteration 1000: Loss 2.028229236602783\nValidation Loss: 2.104797124862671\nIteration 1100: Loss 2.0085501670837402\nValidation Loss: 2.115905284881592\nIteration 1200: Loss 2.0216920375823975\nValidation Loss: 2.131403684616089\nIteration 1300: Loss 2.042447090148926\nValidation Loss: 2.144277572631836\nIteration 1400: Loss 2.101025342941284\nValidation Loss: 2.207002639770508\nIteration 1500: Loss 2.0228543281555176\nValidation Loss: 2.1796066761016846\nIteration 1600: Loss 2.290348768234253\nValidation Loss: 2.389927625656128\nIteration 1700: Loss 2.1048405170440674\nValidation Loss: 2.200162172317505\nIteration 1800: Loss 2.103780508041382\nValidation Loss: 2.2607436180114746\nIteration 1900: Loss 2.1309595108032227\nValidation Loss: 2.2136075496673584\nIteration 2000: Loss 2.146259307861328\nValidation Loss: 2.2842915058135986\nIteration 2100: Loss 2.3755056858062744\nValidation Loss: 2.458054304122925\nIteration 2200: Loss 2.1400399208068848\nValidation Loss: 2.255096912384033\n","output_type":"stream"}]},{"cell_type":"code","source":"# show model summary from sumary writer\n# dir(writer)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vn2mF0DgKuAw","executionInfo":{"status":"ok","timestamp":1720389734813,"user_tz":240,"elapsed":29,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"41736702-e49a-4ff4-ea8b-4ec4d41b5fc4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":73,"outputs":[{"output_type":"execute_result","execution_count":73,"data":{"text/plain":["['__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__enter__',\n"," '__eq__',\n"," '__exit__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_check_caffe2_blob',\n"," '_encode',\n"," '_get_file_writer',\n"," 'add_audio',\n"," 'add_custom_scalars',\n"," 'add_custom_scalars_marginchart',\n"," 'add_custom_scalars_multilinechart',\n"," 'add_embedding',\n"," 'add_figure',\n"," 'add_graph',\n"," 'add_histogram',\n"," 'add_histogram_raw',\n"," 'add_hparams',\n"," 'add_image',\n"," 'add_image_with_boxes',\n"," 'add_images',\n"," 'add_mesh',\n"," 'add_onnx_graph',\n"," 'add_pr_curve',\n"," 'add_pr_curve_raw',\n"," 'add_scalar',\n"," 'add_scalars',\n"," 'add_tensor',\n"," 'add_text',\n"," 'add_video',\n"," 'all_writers',\n"," 'close',\n"," 'default_bins',\n"," 'file_writer',\n"," 'filename_suffix',\n"," 'flush',\n"," 'flush_secs',\n"," 'get_logdir',\n"," 'log_dir',\n"," 'max_queue',\n"," 'purge_step']"]},"metadata":{}}]},{"cell_type":"code","source":"# model = torch.load('gpt_model.pth')\n# Load the weights\n# model.load_state_dict(torch.load('gpt_model.pth'))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"x-xiQt0MZCUI","executionInfo":{"status":"error","timestamp":1720392982306,"user_tz":240,"elapsed":440,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"outputId":"b816e9a3-ae79-4c48-ef9c-982f40351488"},"execution_count":23,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for GPT:\n\tsize mismatch for token_embedding.weight: copying a param with shape torch.Size([65, 768]) from checkpoint, the shape in current model is torch.Size([53, 768]).\n\tsize mismatch for position_embedding.weight: copying a param with shape torch.Size([16, 768]) from checkpoint, the shape in current model is torch.Size([128, 768]).\n\tsize mismatch for linear.weight: copying a param with shape torch.Size([65, 768]) from checkpoint, the shape in current model is torch.Size([53, 768]).\n\tsize mismatch for linear.bias: copying a param with shape torch.Size([65]) from checkpoint, the shape in current model is torch.Size([53]).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-949597695ef8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model = torch.load('gpt_model.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT:\n\tsize mismatch for token_embedding.weight: copying a param with shape torch.Size([65, 768]) from checkpoint, the shape in current model is torch.Size([53, 768]).\n\tsize mismatch for position_embedding.weight: copying a param with shape torch.Size([16, 768]) from checkpoint, the shape in current model is torch.Size([128, 768]).\n\tsize mismatch for linear.weight: copying a param with shape torch.Size([65, 768]) from checkpoint, the shape in current model is torch.Size([53, 768]).\n\tsize mismatch for linear.bias: copying a param with shape torch.Size([65]) from checkpoint, the shape in current model is torch.Size([53])."]}]},{"cell_type":"markdown","source":"### Generate text\n\n\nprint some text that your model generates","metadata":{"id":"zy3v8Nv7YVUa"}},{"cell_type":"code","source":"# # Example settings\n# start_char = 'H'  # Starting character for generation\n# max_new_tokens = 10  # Generate 100 characters after the start_char\n# top_p = 0.9  # Use top-p sampling with p=0.9\n# top_k = 50  # Use top-k sampling with k=50\n# temperature = 1.0  # Set temperature to 1 for standard randomness\n\n# # Assuming 'model' is your trained GPT-like model instance\n# # Call the generate function\n# generated_text = generate(model,start_char, max_new_tokens, top_p, top_k, temperature)\n\n# # Print the generated text\n# print(\"Generated Text:\", generated_text)","metadata":{"id":"5l4soWviWG5M","executionInfo":{"status":"error","timestamp":1720392679551,"user_tz":240,"elapsed":308,"user":{"displayName":"Dhruv Parthasarathy","userId":"07667820098240588008"}},"colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"ebe9345d-fa96-4fa7-a387-32061d7799ac"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"Encoded indices: [6]\n"},{"output_type":"error","ename":"AttributeError","evalue":"'collections.OrderedDict' object has no attribute 'token_embedding'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-6d011eb660e5>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Assuming 'model' is your trained GPT-like model instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Call the generate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Print the generated text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-3d405b72cc99>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, start_char, max_new_tokens, top_p, top_k, temperature)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_token_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Put the model in evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'token_embedding'"]}]}]}