{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathydNU/gen-ai-coursework/blob/main/advanced-llms/CourseWork/INFO_7374_Lecture_10_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents\n",
        "\n"
      ],
      "metadata": {
        "id": "9DP6i7fVW_0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Shift to Agentic Workflows\n",
        "- Traditional non-agentic workflows: One-shot answers from LLMs.\n",
        "- Agentic workflows: Iterative process with planning, execution, revision, and tool use."
      ],
      "metadata": {
        "id": "O7Ig2-gDXBDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages of Agentic Workflows\n",
        "- Remarkably better results compared to non-agentic workflows.\n",
        "- Case study: Coding Benchmark (Human Eval) showing improved performance with agentic workflows.\n",
        "- GPT-3.5 with agentic workflow outperforming GPT-4 in zero-shot prompting.\n",
        "- Significant productivity boost in AI development with agentic workflows.\n",
        "- Expansion of AI capabilities through agentic workflows.\n",
        "\n",
        "\n",
        "You'll likely see better results with agentic workflow + GPT-4, than GPT-5 by itself. So learning how to use agents effectively is useful skill.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sub43nsiXrkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Design Patterns in Agentic Workflows\n",
        "\n",
        "**Reflection**: LLMs can review and improve their own output.\n",
        "\n",
        "Example: Code generation, review, and revision by the same LLM.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Tool Use**: LLMs using external tools to expand capabilities.\n",
        "\n",
        "Example: Integration with image manipulation or code execution tools.\n",
        "\n",
        "---\n",
        "\n",
        "**Planning**: LLMs outlining steps and executing tasks sequentially.\n",
        "\n",
        "Example: Research agents for literature review, breaking down topic into smaller sub topics.\n",
        "\n",
        "---\n",
        "\n",
        "**Multi Agent Collaboration**: Multiple LLMs collaborating to achieve complex tasks.\n",
        "\n",
        "Example: ChatDev, where LLMs take on different roles in software development. Multi-agent debate leading to better performance."
      ],
      "metadata": {
        "id": "sODESJBAXpYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "Large language models like ChatGPT, Claude, and Gemini are powerful tools for generating human-like text based on prompts. However, their initial outputs are not always ideal or fully satisfactory.\n",
        "\n",
        "The typical process involves:\n",
        "1. Prompting the model\n",
        "2. Receiving an imperfect output\n",
        "3. Providing critical feedback to help the model improve\n",
        "4. Getting an enhanced response based on that feedback\n",
        "\n",
        "But what if **step 3 could be automated?** What if the language model could analyze its own initial response, identify weaknesses and areas for improvement, and then refine the output - all without human intervention?\n",
        "\n",
        "This is the core idea behind **Reflection** - enabling AI models to engage in self-critique to iteratively enhance their responses. By building in the ability to evaluate their own outputs against certain criteria and heuristics, language models could spot gaps, errors, or places where more detail and nuance is needed. They could then revise the response, resulting in higher quality, more comprehensive outputs.\n",
        "\n",
        "Reflection turns the process of critique and revision into an automatic feedback loop within the model itself. The potential benefits include more efficient generation of strong responses, less need for human fine-tuning, and AI systems that continuously sharpen their own performance. It's an exciting avenue for making large language models more robust and self-optimizing."
      ],
      "metadata": {
        "id": "qRAVyRUpce-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Code Generation via Self-Reflection\n",
        "\n",
        "When tasked with writing code to accomplish a specific objective, large language models can be remarkably effective. By providing a clear prompt detailing the desired functionality, we can often obtain working code that gets the job done.\n",
        "\n",
        "However, the initial code output may not always be optimal in terms of correctness, style, efficiency, or other key factors. This is where the power of self-reflection comes into play.\n",
        "\n",
        "After generating code intended to carry out task X, we can prompt the model to carefully review its own work and provide constructive criticism. For example:\n",
        "\n",
        "```\n",
        "Here's code intended for task X: [previously generated code]\n",
        "\n",
        "Please check the code carefully for correctness, style, and efficiency. Provide constructive criticism on how the code could be improved.\n",
        "```\n",
        "\n",
        "By framing the request in this way, we're asking the model to put on its critic hat and analyze the strengths and weaknesses of the generated code. The LLM will often spot issues such as logical errors, edge cases not handled, inefficient algorithms, inconsistent styling, lack of comments/documentation, etc. It can then suggest targeted improvements.\n",
        "\n",
        "Armed with this internal feedback, we move to the next step - rewriting the code. The new prompt includes both the original code and the model's own constructive criticism:\n",
        "\n",
        "```\n",
        "Here is the code generated previously: [original code]\n",
        "\n",
        "Here is the constructive feedback on how to improve it: [self-critique]\n",
        "\n",
        "Please rewrite the code from scratch, thoroughly addressing the issues raised in the feedback while still accomplishing task X.\n",
        "```\n",
        "\n",
        "By explicitly asking the model to take its own suggestions into account during the rewrite, we can get a new version of the code that is enhanced along multiple dimensions. The LLM leverages both its original understanding of the task and its self-reflective analysis to produce a higher quality solution.\n",
        "\n",
        "We can even repeat this critique-rewrite loop multiple times, progressively polishing the code until the model no longer has any substantive feedback for itself.\n",
        "\n",
        "This iterative process of self-reflection, where the model serves as both generator and critic, can be a powerful tool for optimizing code quality. It allows LLMs to identify and overcome their own blindspots and limitations in a targeted way. Beyond just code, this technique can be applied to improve LLM performance on a wide variety of language tasks, using self-reflection to iteratively refine the outputs."
      ],
      "metadata": {
        "id": "QEE1OKG01nMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Enhancing Reflection with External Evaluation Tools\n",
        "\n",
        "The power of self-reflection for improving LLM outputs can be further augmented by providing the model with external tools to evaluate its own work. For example, when generating code, the model can be prompted to run its solution through a suite of unit tests to check correctness on various test cases. If the code fails any tests, the model can analyze the specific errors and propose targeted fixes.\n",
        "\n",
        "Similarly, for text generation tasks, the LLM can be asked to search the web for relevant information and cross-reference its output to check for factual accuracy, consistency, and completeness. Any discrepancies or gaps identified through this external validation process can then be fed back into the model's self-reflection to drive iterative improvements.\n"
      ],
      "metadata": {
        "id": "pNJUBzJR4Xfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Multi-Agent Approach to Reflection\n",
        "\n",
        "Another promising approach is to implement Reflection using a multi-agent framework. This involves creating two distinct AI agents with specialized roles: a generator agent focused on producing high-quality outputs, and a critic agent tasked with providing constructive feedback.\n",
        "\n",
        "The generator agent is prompted to create an initial response to the given task. The critic agent then analyzes this output and offers suggestions for improvement. This kicks off a back-and-forth dialogue between the two agents, with the generator proposing revisions based on the critic's feedback, and the critic evaluating each new iteration.\n",
        "\n",
        "Through this collaborative process of generation and critique, the output can be progressively refined. The multi-agent setup allows for a natural division of labor and a structured way to implement the Reflection pattern."
      ],
      "metadata": {
        "id": "BTZsFJjM4iU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Refine: Iterative Refinement with Self-Feedback (May 2023)\n",
        "\n",
        "https://arxiv.org/abs/2303.17651\n",
        "\n"
      ],
      "metadata": {
        "id": "o6aOw-034_ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/madaan/self-refine/main/docs/static/images/animation_oldstyle_oneloop.gif)"
      ],
      "metadata": {
        "id": "EEYz_ENM5Gk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reflection is a powerful tool for iterative improvement across various domains. The process involves the following high-level steps:\n",
        "\n",
        "1. Initial attempt\n",
        "2. Reflection\n",
        "3. Analysis\n",
        "4. Revision\n",
        "5. Evaluation\n",
        "6. Iteration (if needed)\n",
        "7. Final output\n",
        "\n",
        "Here's how these steps map to the two specific examples:\n",
        "\n",
        "Example 1: Drafting an email request\n",
        "1. Initial attempt: Write a direct request, e.g., \"Send me the data ASAP\".\n",
        "2. Reflection: Recognize that the phrasing may come across as impolite.\n",
        "3. Analysis: Consider how the recipient might perceive the message and think about ways to make the request more polite and professional.\n",
        "4. Revision: Revise the request, e.g., \"Hi Ashley, could you please send me the data at your earliest convenience?\".\n",
        "5. Evaluation: Review the revised message to ensure it strikes the right tone and clearly communicates the request.\n",
        "6. Iteration (if needed): If the revised message can still be improved, go through another round of reflection and revision.\n",
        "7. Final output: Send the polished, professional request to the colleague.\n",
        "\n",
        "Example 2: Writing code\n",
        "1. Initial attempt: Implement a \"quick and dirty\" solution.\n",
        "2. Reflection: Identify areas where the code can be improved in terms of efficiency and readability.\n",
        "3. Analysis: Pinpoint specific inefficiencies or unclear sections that need refactoring.\n",
        "4. Revision: Refactor the code, optimizing for efficiency and enhancing readability.\n",
        "5. Evaluation: Test the refactored code to verify that it still functions correctly and has improved in terms of performance and maintainability.\n",
        "6. Iteration (if needed): If further opportunities for optimization or clarity are identified, repeat the refactoring process.\n",
        "7. Final output: Commit the optimized, readable code to the codebase.\n",
        "\n",
        "By applying the reflection process to these specific examples, we can see how it leads to more effective communication in the email example and higher-quality code in the programming example. The same high-level steps can be adapted to suit various contexts and goals, demonstrating the versatility and value of reflection as a tool for continuous improvement."
      ],
      "metadata": {
        "id": "SoOJ-fgR5uet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://selfrefine.info/static/images/fig2.png)"
      ],
      "metadata": {
        "id": "Wt2GzK8q5H7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/3TMpxHE.png)"
      ],
      "metadata": {
        "id": "hdWyaqkW5-hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r8DtkUAXc-V",
        "outputId": "d60a9e26-d4ab-40e6-ec3c-f24b988a9426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(\n",
        "  api_key=userdata.get('TOGETHER_API_KEY'),\n",
        "  base_url='https://api.together.xyz/v1',\n",
        ")\n"
      ],
      "metadata": {
        "id": "xmHplwGRYJ4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "model = \"Qwen/Qwen1.5-4B-Chat\"\n",
        "\n",
        "def generate_tweet(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=50,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def evaluate_tweet(tweet):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a Twitter influencer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Please critique the following tweet:\\n\\n{tweet}\"}\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a social media manager for Northeastern University.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Generate an engaging tweet about Northeastern University:\"}\n",
        "]\n",
        "\n",
        "for i in range(2):\n",
        "    tweet = generate_tweet(messages)\n",
        "    print(f\"\\n#### Generated Tweet {i+1}:\")\n",
        "    print(tweet)\n",
        "\n",
        "    critique = evaluate_tweet(tweet)\n",
        "    print(f\"\\n#### Critique {i+1}:\")\n",
        "    print(critique)\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": tweet})\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Improve the tweet based on this critique. Only write the improve tweet:\\n\\n{critique}\"})\n",
        "\n",
        "print(\"\\n#### Final Tweet:\")\n",
        "messages.append({\"role\": \"assistant\", \"content\": generate_tweet(messages)})\n",
        "print(messages[-1][\"content\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6iHPUtDYvRh",
        "outputId": "04e105f3-9f0f-4fac-8e5f-cee9b835c600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#### Generated Tweet 1:\n",
            "\"Join us at Northeastern University and be part of a community that fosters creativity, collaboration, and innovation. #Northeastern #University #Community #Innovation\"\n",
            "\n",
            "#### Critique 1:\n",
            "The tweet you've provided is a promotion for Northeastern University, and it does a few things well. Firstly, it highlights the key values that the university is known for: creativity, collaboration, and innovation. By using hashtags like #Northeastern and #University, the tweet is easily discoverable by people interested in these topics.\n",
            "Secondly, the tweet is concise and straightforward, which is important for grabbing people's attention and making them want to know more. The call to action is clear:\n",
            "\n",
            "#### Generated Tweet 2:\n",
            "\"Be part of Northeastern University's community of creativity, collaboration, and innovation. Join now! #Northeastern #University #Community #Innovation\"\n",
            "\n",
            "#### Critique 2:\n",
            "The tweet aims to promote Northeastern University's community of creativity, collaboration, and innovation. The hashtags used (#Northeastern, #University, #Community, #Innovation) are also relevant to the tweet and will help the tweet reach a wider audience. The tweet's content is clear and concise, making it easy for readers to understand what Northeastern University is offering. Overall, the tweet is well-crafted and effective at promoting the university's community.\n",
            "\n",
            "#### Final Tweet:\n",
            "\"Explore Northeastern University's vibrant community of creativity, collaboration, and innovation. Join now! #Northeastern #University #Community #Innovation\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in messages:\n",
        "    print(message)\n",
        "    print('---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGNYcnnUZ1AO",
        "outputId": "315f7ada-97a8-43cf-a2a8-f41162b3151b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'system', 'content': 'You are a social media manager for Northeastern University.'}\n",
            "---\n",
            "{'role': 'user', 'content': 'Generate an engaging tweet about Northeastern University:'}\n",
            "---\n",
            "{'role': 'assistant', 'content': '\"Join Northeastern University as we continue to push the boundaries of knowledge and innovation! Check out our latest achievements and upcoming events at @NEU.edu.\" #NEU #Innovation #Education'}\n",
            "---\n",
            "{'role': 'user', 'content': \"Improve the tweet based on this critique. Only write the improve tweet:\\n\\nAs an AI language model, I don't have personal opinions or biases. However, I can provide a neutral critique of the tweet based on the key elements of a successful tweet:\\n\\n1. Clear and concise headline: The headline is clear and concise, and it clearly communicates the purpose of the tweet. It's a good way to capture people's attention and encourage them to click on the tweet to learn more.\\n\\n2. Use of hashtags: Hashtags are a great way to increase the visibility of a\"}\n",
            "---\n",
            "{'role': 'assistant', 'content': 'tweet and make it more discoverable by users who are interested in the topic. The use of relevant hashtags, such as #NEU and #Innovation, helps to connect the tweet with a broader audience and increase its reach.\\n\\n3. Emphasis'}\n",
            "---\n",
            "{'role': 'user', 'content': 'Improve the tweet based on this critique. Only write the improve tweet:\\n\\nCertainly! Please provide me with the tweet so I can review and make modifications.'}\n",
            "---\n",
            "{'role': 'assistant', 'content': '\"Join Northeastern University as we continue to push the boundaries of knowledge and innovation! Explore our latest achievements and upcoming events @NEU.edu. #NEU #Innovation #Education\"\\nThis revised tweet maintains the same key elements as the original,'}\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2e7Cw_FOYuuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reflexion: Language Agents with Verbal Reinforcement Learning (October 2023)\n",
        "\n",
        "https://arxiv.org/abs/2303.11366"
      ],
      "metadata": {
        "id": "DkceJoT1TCZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Reflexion agents reflect on task feedback signals to improve their decision making over time. The key aspects are:\n",
        "\n",
        "- They **verbally reflect** on the feedback signals they receive after completing a task\n",
        "- They **store** these reflections in an **episodic memory buffer**\n",
        "- In subsequent trials of the task, they use the stored reflections to **make better decisions**\n"
      ],
      "metadata": {
        "id": "yFQc4J1Ya9xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/VXYMKoy.png)"
      ],
      "metadata": {
        "id": "d7AP2NKubGUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reflexion's Three Models\n",
        "\n",
        " ![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Freflexion.1053729e.png&w=1080&q=75)\n",
        "\n",
        "Reflexion has three main parts:\n",
        "\n",
        "1. **Actor**: This part makes text and actions based on what it sees. It does something, looks at the result, and remembers what happened. It uses methods like Chain-of-Thought (CoT) and ReAct. It also has a memory to help it remember important things.\n",
        "\n",
        "2. **Evaluator**: This part gives a score to what the Actor did. It looks at what happened and decides how good or bad it was. The score depends on the task. It uses language models and rules to make decisions.\n",
        "\n",
        "3. **Self-Reflection**: This part helps the Actor get better. It's like a teacher giving tips. It looks at the score, what just happened, and things from the past. Then it makes suggestions to help the Actor do better next time. These suggestions go into the Actor's memory so it can keep learning and improving."
      ],
      "metadata": {
        "id": "6_EsBUPVbItD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing (Feb 2024)\n",
        "https://arxiv.org/abs/2305.11738\n",
        "\n",
        "\n",
        "CRITIC proposes that for some tasks, using the LLM to directly provide feedback for an answer is not sufficient. It is better to use external tools to help verify the correctness of an answer and use the tools' output as part of the critique."
      ],
      "metadata": {
        "id": "Zio0KPPZdeAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ![](https://i.imgur.com/K8ca6i2.png)"
      ],
      "metadata": {
        "id": "tI-L86lYdt8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ![](https://i.imgur.com/2s3MYxE.png)"
      ],
      "metadata": {
        "id": "yCAjv3gRdibU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool Use\n",
        "\n",
        "- Large Language Models (LLMs) can be made even more powerful by giving them access to additional tools.\n",
        "  - For example, LLMs can be connected to web search engines, allowing them to find up-to-date information to help answer questions better.\n",
        "  - They can also be given the ability to run computer code, which lets them give working code examples and solve programming problems.\n",
        "\n",
        "- By combining LLMs with these extra capabilities, they can provide responses that are more accurate and relevant to the specific situation.\n",
        "\n",
        "- This opens up exciting possibilities for what AI language models can do to assist humans with a wide variety of tasks."
      ],
      "metadata": {
        "id": "MIkRupyJdikY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools for Enhancing Large Language Models\n",
        "\n",
        "Web search tool:\n",
        "- When prompted with a question like \"What is the best coffee maker according to reviewers?\", an LLM can use a web search tool to gain context\n",
        "- The LLM generates a special string (e.g., `{tool: web-search, query: \"coffee maker reviews\"}`) to request a web search\n",
        "- The result is passed back to the LLM as additional input context for further processing\n",
        "\n",
        "Code execution tool:\n",
        "- When asked a question requiring computation (e.g., compound interest calculation), an LLM can use a code execution tool\n",
        "- The LLM generates a string (e.g., `{tool: python-interpreter, code: \"100 * (1+0.07)**12\"}`) to request code execution\n",
        "- This approach is more likely to result in the correct answer compared to generating the answer directly using a transformer network\n",
        "\n",
        "Database query tool:\n",
        "- LLMs can be integrated with databases to retrieve specific information\n",
        "- For example, if asked \"How many employees work at Company X?\", the LLM can generate a database query (e.g., `{tool: database, query: \"SELECT COUNT(*) FROM employees WHERE company = 'X'\"}`) to fetch the answer\n",
        "- This allows LLMs to provide accurate, up-to-date information from structured data sources\n",
        "\n",
        "Image analysis tool:\n",
        "- LLMs can be combined with computer vision models to analyze and describe images\n",
        "- When presented with an image, the LLM can generate a request (e.g., `{tool: image-analyzer, image: <image_data>}`) to extract information from the image\n",
        "- The image analysis results (e.g., object detection, scene description) can then be used by the LLM to answer questions or generate text related to the image\n",
        "\n",
        "Translation tool:\n",
        "- LLMs can leverage machine translation models to communicate in multiple languages\n",
        "- If prompted with a request in a foreign language, the LLM can generate a translation request (e.g., `{tool: translator, text: \"Bonjour, comment allez-vous?\", target_language: \"English\"}`)\n",
        "- The translated text is then passed back to the LLM, allowing it to understand and respond to the request in the original language"
      ],
      "metadata": {
        "id": "k8jifIS7fche"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expanding Tool Use in Agentic Workflows\n",
        "\n",
        "To use these tools, LLMs can be given detailed descriptions of the available functions, including:\n",
        "\n",
        "- A text description explaining the purpose of each function\n",
        "- Information about the arguments each function expects\n",
        "\n",
        "With this knowledge, the LLM is expected to automatically select the most appropriate function to call in order to complete a given task."
      ],
      "metadata": {
        "id": "imYutx8OgURb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://dl-staging-website.ghost.io/content/images/size/w1000/2024/04/TOOL-USE-3.png)"
      ],
      "metadata": {
        "id": "bEZhnL9-gM1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Large Numbers of Tools\n",
        "- Systems are being built where LLMs have access to hundreds of tools\n",
        "- Including all tool descriptions in the LLM context may not be feasible\n",
        "- Heuristics can be used to select the most relevant subset of tools to include in the context at each processing step\n",
        "  - Similar to retrieval augmented generation (RAG) systems, which use heuristics to select a subset of text when the full context is too large"
      ],
      "metadata": {
        "id": "-izBYqoKgdy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expansion of Tool Use Practices\n",
        "- Practices for tool use have exploded since the introduction of LMMs like LLaVa, GPT-4V, and Gemini\n",
        "- **GPT-4's function calling capability, released in the middle of last year, was a significant step toward general-purpose tool use**"
      ],
      "metadata": {
        "id": "1aBLBBfognSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import openai\n",
        "\n",
        "\n",
        "# Define function(s)\n",
        "tools = [\n",
        "  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"city\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The city e.g. San Francisco\"\n",
        "          },\n",
        "          \"country\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The country e.g. USA\"\n",
        "          },\n",
        "          \"unit\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\n",
        "              \"celsius\",\n",
        "              \"fahrenheit\"\n",
        "            ]\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "    {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "      \"name\": \"get_popular_sport_team\",\n",
        "      \"description\": \"Get the most popular professional sports team in a given location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"city\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The city e.g. San Francisco\"\n",
        "          },\n",
        "          \"sport\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The sport e.g. basketball\"\n",
        "          },\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "WtHIMHCpQKPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate\n",
        "response = client.chat.completions.create(\n",
        "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "\t\t    {\"role\": \"user\", \"content\": \"What is the current temperature of Paris?\"}\n",
        "\t\t],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "print(json.dumps(response.choices[0].message.dict()['tool_calls'], indent=2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90hCkHQflXar",
        "outputId": "f088fb46-ee47-40b7-a36a-f029330977fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": \"call_i36rt3cvvuip6wxwzmvx9k2u\",\n",
            "    \"function\": {\n",
            "      \"arguments\": \"{\\\"city\\\":\\\"Paris\\\",\\\"country\\\":\\\"France\\\",\\\"unit\\\":\\\"celsius\\\"}\",\n",
            "      \"name\": \"get_current_weather\"\n",
            "    },\n",
            "    \"type\": \"function\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate\n",
        "response = client.chat.completions.create(\n",
        "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "\t\t    {\"role\": \"user\", \"content\": \"What is the most popular basketball team in Oakland?\"}\n",
        "\t\t],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "print(json.dumps(response.choices[0].message.dict()['tool_calls'], indent=2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5-Irz_Fla_5",
        "outputId": "28ec48f4-65f7-4e2f-f1be-96516a29e382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": \"call_mvguz6xxho4whuzh7h0zk3ry\",\n",
            "    \"function\": {\n",
            "      \"arguments\": \"{\\\"city\\\":\\\"Oakland\\\",\\\"sport\\\":\\\"basketball\\\"}\",\n",
            "      \"name\": \"get_popular_sport_team\"\n",
            "    },\n",
            "    \"type\": \"function\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the LLM will only tell you what tool to use and what parameters to use, it will not directly tell you the answer. From this LLM response, we will extract the tool and the parameters and use it to call our tool."
      ],
      "metadata": {
        "id": "SPDgYmp2xn19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_name = response.choices[0].message.dict()['tool_calls'][0]['function']['name']\n",
        "params = json.loads(response.choices[0].message.dict()['tool_calls'][0]['function'][\"arguments\"])\n",
        "\n",
        "print(f\"{tool_name=}\")\n",
        "print(f\"{params=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNsHqCXjx9Vr",
        "outputId": "a75ced38-d379-41db-83ff-44fedda371ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool_name='get_popular_sport_team'\n",
            "params={'city': 'Oakland', 'sport': 'basketball'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrrk4_fbzeW_",
        "outputId": "86869791-7676-4559-90a2-f74429fe6616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_popular_sport_team(city, sport):\n",
        "    if city == 'Oakland' and sport == 'basketball':\n",
        "        return \"Warriors\"\n",
        "    else:\n",
        "        return \"not sure\"\n",
        "\n",
        "def get_current_weather(country, unit):\n",
        "    # add custom logic here\n",
        "    return \"call 3rd party api\"\n",
        "\n",
        "\n",
        "def call_tool(tool_name, params):\n",
        "    if tool_name == 'get_popular_sport_team':\n",
        "        return get_popular_sport_team(**params)\n",
        "    elif tool_name == 'get_current_weather':\n",
        "        return get_current_weather(**params)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "call_tool(tool_name, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zgCOfZcSyh5q",
        "outputId": "0b5fce91-4e5b-43a4-d3af-d5c8a1fd3ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Warriors'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gorilla: Large Language Model Connected with Massive APIs (May 2023)\n",
        "\n",
        "https://arxiv.org/abs/2305.15334"
      ],
      "metadata": {
        "id": "GqkoMH7GgpZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs had difficulty calling API/tools with the accurate input arguments and had a tendency to hallucinate the wrong usage of an API call.\n",
        "\n",
        "Gorilla is a finetuned LLama based model that, at the time, surpassed the performance of GPT 4 on writing API calls.\n",
        "\n"
      ],
      "metadata": {
        "id": "n4EPMdB3mDQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingGPT\n",
        "\n",
        "https://arxiv.org/abs/2303.17580\n",
        "\n",
        "![](https://lilianweng.github.io/posts/2023-06-23-agent/hugging-gpt.png)"
      ],
      "metadata": {
        "id": "tY324s62vdZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a rephrased version of the key points about using large language models (LLMs) for task planning and execution:\n",
        "\n",
        "#### Task Planning\n",
        "- The LLM acts as the \"brain\" to parse user requests into multiple subtasks\n",
        "- Each subtask has four attributes:\n",
        "  1. Task type\n",
        "  2. ID\n",
        "  3. Dependencies\n",
        "  4. Arguments\n",
        "- Few-shot examples are used to guide the LLM in task parsing and planning\n",
        "\n",
        "*prompt*\n",
        "\n",
        "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning."
      ],
      "metadata": {
        "id": "pClkektzvmz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Selection\n",
        "- The LLM distributes the subtasks to specialized expert models\n",
        "- The request to the expert model is framed as a multiple-choice question\n",
        "- The LLM is given a list of models to choose from\n",
        "- Task type based filtering is needed due to limited context length\n",
        "\n",
        "\n",
        "*prompt*\n",
        "\n",
        "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list."
      ],
      "metadata": {
        "id": "hIA5A0WSwNx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Task Execution\n",
        "- The selected expert models execute their assigned subtasks\n",
        "- The results of each subtask are logged\n",
        "\n",
        "*prompt*\n",
        "\n",
        "With the input and the inference results, the AI assistant needs to\n",
        "describe the process and results. The previous stages can be formed as:\n",
        "\n",
        "    User Input: {{ User Input }}\n",
        "    Task Planning: {{ Tasks }}\n",
        "    Model Selection: {{ Model Assignment }}\n",
        "    Task Execution: {{ Predictions }}\n",
        "\n",
        "\n",
        "You must first answer the user's request in a straightforward manner.\n",
        "Then describe the task process and show your analysis and model\n",
        "inference results to the user in the first person. If inference\n",
        "results contain a file path, you must tell the user the complete\n",
        "file path.\n"
      ],
      "metadata": {
        "id": "mbYsUeaXwOYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Response Generation\n",
        "- The LLM receives the execution results from the expert models\n",
        "- It then generates a summarized response for the end user based on the combined subtask results\n"
      ],
      "metadata": {
        "id": "HQpZqzKrwO18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoGPT\n",
        "\n",
        "```\n",
        "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
        "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
        "\n",
        "GOALS:\n",
        "\n",
        "1. {{user-provided goal 1}}\n",
        "2. {{user-provided goal 2}}\n",
        "3. ...\n",
        "4. ...\n",
        "5. ...\n",
        "\n",
        "Constraints:\n",
        "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
        "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
        "3. No user assistance\n",
        "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
        "5. Use subprocesses for commands that will not terminate within a few minutes\n",
        "\n",
        "Commands:\n",
        "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
        "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
        "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
        "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
        "5. List GPT Agents: \"list_agents\", args:\n",
        "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
        "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
        "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
        "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
        "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
        "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
        "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
        "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
        "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
        "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
        "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
        "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
        "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
        "19. Do Nothing: \"do_nothing\", args:\n",
        "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
        "\n",
        "Resources:\n",
        "1. Internet access for searches and information gathering.\n",
        "2. Long Term memory management.\n",
        "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
        "4. File output.\n",
        "\n",
        "Performance Evaluation:\n",
        "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
        "2. Constructively self-criticize your big-picture behavior constantly.\n",
        "3. Reflect on past decisions and strategies to refine your approach.\n",
        "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
        "\n",
        "You should only respond in JSON format as described below\n",
        "Response Format:\n",
        "{\n",
        "    \"thoughts\": {\n",
        "        \"text\": \"thought\",\n",
        "        \"reasoning\": \"reasoning\",\n",
        "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
        "        \"criticism\": \"constructive self-criticism\",\n",
        "        \"speak\": \"thoughts summary to say to user\"\n",
        "    },\n",
        "    \"command\": {\n",
        "        \"name\": \"command name\",\n",
        "        \"args\": {\n",
        "            \"arg name\": \"value\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "Ensure the response can be parsed by Python json.loads\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "OAqPNKRnxa6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient Tool Use with Chain-of-Abstraction Reasoning (Feb 2024)\n",
        "\n",
        "https://arxiv.org/abs/2401.17464"
      ],
      "metadata": {
        "id": "LgBmMnukmT2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key idea behind CoA is:\n",
        "\n",
        "1. Train LLMs to first generate reasoning chains with **abstract placeholders** (e.g., y1, y2, y3).\n",
        "\n",
        "2. Then, call domain-specific tools to fill in those placeholders with **specific knowledge**, grounding the final answer.\n",
        "\n",
        "This decoupling of general reasoning and domain knowledge allows for parallel processing, where LLMs can generate the next abstract chain while tools fill in the current one, accelerating the overall inference.\n",
        "\n",
        "#### Benefits of CoA\n",
        "\n",
        "CoA offers several advantages over prior methods where LLM decoding and API calls are interleaved:\n",
        "\n",
        "- Promotes effective planning by encouraging LLMs to interconnect multiple tool calls and adopt more feasible reasoning strategies.\n",
        "\n",
        "- Enables robust and efficient multistep reasoning by separating high-level reasoning from granular knowledge retrieval."
      ],
      "metadata": {
        "id": "20OQCivKm0eD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/TEeSRDH.png)\n",
        "\n",
        "![](https://i.imgur.com/IQor7uK.png)"
      ],
      "metadata": {
        "id": "xL5NyPVpmXpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Planning\n",
        "\n",
        "https://arxiv.org/abs/2402.02716 (Understanding the planning of LLM agents: A survey, Feb 2024)"
      ],
      "metadata": {
        "id": "R1D2QEGMbIcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Many complex tasks require multiple steps and tools to complete. An AI agent can break down the task into smaller, manageable steps and decide which tools to use at each step. For instance, if the goal is to create an image of a girl in the same pose as a boy in a given picture, the agent might plan the following steps:\n",
        "\n",
        "1. Use a pose detection tool to analyze the boy's pose in the input image.\n",
        "2. Use a pose-to-image generation tool to render an image of a girl in the detected pose.\n",
        "\n",
        "The agent can be fine-tuned or prompted to generate a plan specifying the tools, inputs, and outputs for each step, like this:\n",
        "\n",
        "```\n",
        "{tool: pose-detection, input: image.jpg, output: temp1}\n",
        "{tool: pose-to-image, input: temp1, output: final.jpg}\n",
        "```\n",
        "\n",
        "![](https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T140722.194.png)\n",
        "\n",
        "#### Example: Planning for a Research Agent\n",
        "\n",
        "Let's consider a text-based example involving a research agent. Suppose the agent is tasked with writing a summary of the impact of climate change on coral reefs. The agent might create a plan like this:\n",
        "\n",
        "```\n",
        "{tool: web-search, input: \"climate change impact on coral reefs\", output: search_results}\n",
        "{tool: text-summarization, input: search_results, output: key_points}\n",
        "{tool: article-writing, input: key_points, output: draft_summary}\n",
        "{tool: grammar-checking, input: draft_summary, output: final_summary}\n",
        "```\n",
        "\n",
        "In this plan, the agent:\n",
        "\n",
        "1. Searches the web for relevant information on the impact of climate change on coral reefs.\n",
        "2. Summarizes the key points from the search results.\n",
        "3. Writes a draft summary using the key points.\n",
        "4. Checks and corrects the grammar in the draft summary to produce the final output.\n",
        "\n",
        "By breaking down the task into smaller steps and specifying the appropriate tools for each step, the AI agent can effectively plan and execute complex tasks."
      ],
      "metadata": {
        "id": "hrkXXLuhqCLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utility and Challenges of Planning in AI Agents\n",
        "\n",
        "For complex tasks where a predefined set of steps cannot be specified in advance, AI agents with planning capabilities can dynamically determine the necessary steps to complete the task. This allows for more flexibility and adaptability in problem-solving.\n",
        "\n",
        "**Planning is a powerful capability that enables AI agents to tackle complex, multi-step problems.** However, the dynamic nature of planning can lead to less predictable results compared to predefined step-by-step approaches.\n",
        "\n",
        "It's important to note that planning is a less mature technology compared to other AI capabilities. As a result, it can be challenging to predict the exact actions an AI agent will take when using planning to solve a problem. However, the field of AI planning is rapidly evolving, and the abilities of AI agents in this area will continue to improve and become more reliable in the near future."
      ],
      "metadata": {
        "id": "ST1GPoC2qXaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Addressing Complexity and Uncertainty in AI Planning: Multi-Plan Selection\n",
        "\n",
        "AI agents face challenges when generating plans for complex tasks due to the inherent uncertainty in language models (LLMs). Although LLMs have strong reasoning abilities, a single plan generated by an LLM-based agent may be suboptimal or even infeasible. To overcome this issue, a more effective approach is multi-plan selection, which consists of two main steps:\n",
        "\n",
        "1. **Multi-Plan Generation**\n",
        "   - This step involves generating multiple candidate plans to form a diverse set of potential solutions.\n",
        "   - Common methods for multi-plan generation include:\n",
        "     - Incorporating uncertainty in the decoding process of generative models\n",
        "     - Chain of Thought (CoT) reasoning\n",
        "     - Tree of Thoughts (ToT) approach\n",
        "     - Self-consistency techniques\n",
        "\n",
        "2. **Optimal Plan Selection**\n",
        "   - After generating a set of candidate plans, the next step is to evaluate and select the most promising plan.\n",
        "   - This process involves assessing the feasibility, efficiency, and potential outcomes of each plan.\n",
        "   - Techniques such as simulations, heuristic evaluations, or expert feedback can be used to rank and select the optimal plan from the candidate set."
      ],
      "metadata": {
        "id": "amuHcqyPrClO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### External Memory\n",
        "\n",
        "* [Generative Agents (August 2023)](https://arxiv.org/abs/2304.03442): store the daily experiences of human like agents in text form, and retrieve memories based on composite score of recency and relevance to current situation\n",
        "* [MemoryBank (May 2023)](https://arxiv.org/abs/2305.10250), [TiM (Nov 2023)](https://arxiv.org/pdf/2311.08719.pdf), and [RecMind (March 2024)](https://arxiv.org/abs/2308.14296): encode each memory using an embedding model and put it into a vector index like FAISS. During retrieval, the description of the current status is used as a query to retrieve memories from the memory pool. Difference is how memory is updated.\n",
        "* [REMEMBER (October 2023)](https://arxiv.org/abs/2306.07929): stores historical memories in the form of a Q value table, where each record is (environment, task, action, Q-value), then during retrieval, positive and negative memories are both retrieved"
      ],
      "metadata": {
        "id": "2Ipvbfo_rInY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ### Embodied Memory: fine tuning LM\n",
        "\n",
        "* usually the experiential samples are collected from the agents' interactions with the environment\n",
        "* [CALM (October 2020)](https://arxiv.org/abs/2010.02903): uses ground truth trajectories to fine tune GPT2 on next token prediction task -->"
      ],
      "metadata": {
        "id": "nQjUcqKdr0yC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yj4F-uOJqhUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Agent Collaboration\n",
        "\n",
        "   - Multiple LLMs collaborating to achieve complex tasks.\n",
        "   - Example: ChatDev, where LLMs take on different roles in software development.\n",
        "   - Multi-agent debate leading to better performance."
      ],
      "metadata": {
        "id": "LsT97TsmuPyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatDev (December 2023)\n",
        "\n",
        "https://arxiv.org/pdf/2307.07924.pdf\n",
        "\n",
        "![](https://i.imgur.com/EeL8s8p.png)\n",
        "\n",
        "![](https://i.imgur.com/ZxQpBfL.png)\n",
        "\n",
        "![](https://i.imgur.com/C42iBOU.png)"
      ],
      "metadata": {
        "id": "IyVDcLIDurps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4mu_xs5DrHu_"
      }
    }
  ]
}