{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathydNU/gen-ai-coursework/blob/main/advanced-llms/CourseWork/INFO_7374_Lecture_9_LLM_Programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Programs"
      ],
      "metadata": {
        "id": "vJj7xZ0HFgO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/\n",
        "\n",
        "Focus is shifting from models to compound systems with multiple components\n",
        "Examples\n",
        "- AlphaCode 2: state-of-the-art results in programming through a carefully engineered system that uses LLMs to generate up to 1 million possible solutions for a task and then filter down the set\n",
        "- AlphaGeometry: combines an LLM with a traditional symbolic solver to tackle olympiad problems\n",
        "- enterprise LLM applications: Databricks found that 60% of LLM applications use some form of [retrieval-augmented generation (RAG)](https://arxiv.org/pdf/2005.11401.pdf), and 30% use multi-step chains\n",
        "\n",
        "This shift to compound systems opens many interesting design questions, but it is also exciting, because it means leading AI results can be achieved through clever engineering, not just scaling up training."
      ],
      "metadata": {
        "id": "gM6PnmpVFjQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Compound Systems?\n",
        "\n",
        "Definition: system that tackles AI tasks using multiple interacting components\n",
        "- Reasons for using compound systems:\n",
        "    1. Some tasks are easier to improve via system design than model scaling\n",
        "    2. Systems can be dynamic and incorporate timely data\n",
        "    3. Improving control and trust is easier with systems\n",
        "    4. Performance goals vary widely and require system flexibility\n",
        "- Compound systems match industry trends in other AI fields like self-driving cars\n",
        "- Compound systems will likely remain a leading paradigm as models improve"
      ],
      "metadata": {
        "id": "uO1reLAmFypC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/ZoZDECO.png)"
      ],
      "metadata": {
        "id": "3NoSUQlmF_7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlphaCodeium\n",
        "\n",
        "https://arxiv.org/pdf/2401.08500.pdf"
      ],
      "metadata": {
        "id": "cAGai0iJGAxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key observation is that generating additional useful tests is easier than generating a correct code solution. Adding specific tests requires mainly understanding the problem, some insight, and basic brute-force or logical reasoning.\n",
        "\n",
        "![](https://i.imgur.com/FLbbrzV.png)"
      ],
      "metadata": {
        "id": "4R-PFelrJ0gX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/Jg25n0W.png)\n",
        "\n",
        "The pre-processing phase represents a linear flow where AlphaCodium reasons about the problem, in natural language.\n",
        "\n",
        "The code iterations phase is where AlphaCodium generates, runs, and fixes a solution code against certain tests."
      ],
      "metadata": {
        "id": "i4TObsB5Vf8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Stage\n",
        "\n",
        "1. Problem reflection: Describe the problem, in bullet points, while addressing the problem goals, inputs, outputs, rules, constraints, and other relevant details that appear in the problem description.\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/CinCRPK.png)\n",
        "\n",
        "2. Public tests reasoning: Explain why each test input leads to the output.\n",
        "\n",
        "3. Generate possible solutions: Generate a list of 2-3 possible solutions to the problem, described in natural language.\n",
        "\n",
        "4. Rank solutions: Rank the possible solutions and choose the \"best solution\", in terms of correctness, simplicity, and robustness (not necessarily take the \"most efficient\" solution).\n",
        "\n",
        "5. Generate additional AI tests. Generate an additional 6-8 diverse input-output tests for the problem. Try to cover cases and aspects not covered by the original public tests.\n"
      ],
      "metadata": {
        "id": "auTrE8_wV6Wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Iterations Phase\n",
        "1. Generate an initial code solution: Choose a potential solution, then run on the public/AI tests. Select the one that has the highest pass rate.\n",
        "2. Iterate on public tests: start with initial code solution. Iteratively run public tests. If any tests fail, include the error message, then try to fix it.\n",
        "3. Iterate on AI generated tests: Continue the run-fix iterations on the AI generated tests"
      ],
      "metadata": {
        "id": "3CXDq2QkXxgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verifiers"
      ],
      "metadata": {
        "id": "RHvAGjZr9-xI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating a single completion is a bit risky. Can we have a model *check* the result before giving the final result?"
      ],
      "metadata": {
        "id": "oWCZ2RL3ybeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outcome Supervision\n",
        "\n",
        "provides feedback for the final result"
      ],
      "metadata": {
        "id": "8rYQe6Dop5db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Verify Step By Step\n",
        "\n",
        "https://arxiv.org/abs/2305.20050 (May 2023)"
      ],
      "metadata": {
        "id": "gF7kv3D0kvY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Process supervision: feedback for each intermediate reasoning step\n",
        "- Process supervision significantly outperforms outcome supervision for training models to solve problems from the MATH dataset\n",
        "- detecting and mitigating hallucinations is essential to improve reasoning capabilities\n",
        "    - One effective method involves training reward models to discriminate between desirable and undesirable outputs. The reward model can then be used in an RL pipeline, or to perform search.\n",
        "    - Resulting system is only as good as the reward model\n",
        "- Process supervision vs outcome supervision\n",
        "    - process supervision is easier for humans to interpret\n",
        "    - Models trained with outcome supervision often use incorrect reasoning to reach to correct final answer\n",
        "    - Process supervision can train a much more reliable reward model than outcome supervision\n",
        "- Evaluation\n",
        "    - Evaluate a reward model by its ability to perform best of N search over uniformly sampled solutions form the generator.\n",
        "    - For each test problem, we select the solution ranked highest by the reward model, and grade it based on its final answer, and report the fraction that are correct. A reward model that is more reliable will select the correct solution more often\n",
        "- Data Collection\n",
        "    - Show human a step-by-step solution produced by the generator. For each step, the human labels, *positive*, *negative*, or *neutral*. A positive label is correct and reasonable. Negative is incorrect or unreasonable. Neutral is ambiguous.\n",
        "    - 800k step level labels across 75k solutions to 12k problems.\n",
        "    - Select solutions where the answer is wrong, but the starting RM thinks is good\n",
        "    - ![](https://i.imgur.com/yrySkvD.png)\n",
        "- Training\n",
        "    - Train PRMs to predict the correctness of each step after the last token in each step. This prediction takes the form of a single token, and we maximize the log likelihood of these target tokens during training. The PRM can therefore be trained in a standard LM pipeline without special accommodations.\n",
        "    - When comparing process supervision and outcome supervision, the authors deliberately choose to supervise only up to the first incorrect step to make the comparison more straightforward\n",
        "- Solution Comparison\n",
        "    - To compare multiple solutions, it is necessary to compute a single score for each solution. This is an important but straightforward detail: we define the PRM score for a solution to be the probability that every step is correct under the PRM. We implement this as the product of the correctness probabilities for each step\n",
        "- Results\n",
        "    - ![](https://i.imgur.com/MLjemN5.png)"
      ],
      "metadata": {
        "id": "P6LiyKVkkmnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations (Feb 2024)\n",
        "\n",
        "https://arxiv.org/abs/2312.08935"
      ],
      "metadata": {
        "id": "t1_9u_j52AD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Process reward model\n",
        "- *automatically* constructed process wise supervision data\n",
        "- Applications\n",
        "    - Verification: can be used to rerank multiple outputs from LMs\n",
        "    - RL: Can be used to train LM policy\n",
        "- Inspired by MCTS\n",
        "    - define the quality of an intermediate step as its potential to deduce the correct final answer.\n",
        "    - by leveraging the correctness of the answer, we can automatically gather step wise supervision\n",
        "    - for a given math problem with a golden answer and a step-by-step solution, to achieve the label of a specific step, we utilize a fine tuned LLM to decode multiple subsequent reasoning paths from this step\n",
        "    - we further validate whether the decoded final answer matches with the golden answer.\n",
        "    - if a reasoning step can deduce more correct answers than another over multiple trials, it would be assigned a higher correctness score\n",
        "\n",
        "- Task formulation:\n",
        "    - Verification: given a problem $p$, sample $N$ candidate solutions. These candidates are then scored using a reward model, and the highest scoring solution is selected as the final answer. A better reward model elevates the likelihood of selecting the solution containing the correct answer.\n",
        "    - RL: use the PRM to supervise LLMs with step by step PPO\n",
        "- ORM: $$L_{ORM} = -(y_s log r_s + (1 - y_s) log (1 - r_s))$$\n",
        "- PRM: assigns a score to each reasoning step $$L_{PRM} = -\\sum_1^K y_{s_i}logr_{s_i} + (1 - y_{s_i})log(1 - r_{s_i})$$\n",
        "    - in their experiments, found little different for using a neutral class\n",
        "- Automatic PRM dataset collection\n",
        "    - Defining the quality of reasoning step: *potential to deduce the correct answer*\n",
        "    - To quantify and estimate the *potential* for a given reasoning step $s_i$, we use a LM (completer) to finalize $N$ subsequent reasoning processes from this step. Then we estimate the potential of this step based on the correctness of all decoded answers\n",
        "    - ![there are some typos](https://i.imgur.com/PSGEgrX.png)\n",
        "    - Estimation: From the completion results, how do we get the quality score for a step?\n",
        "        - Hard estimation: 1 if any completion leads to the correct solution, 0 if no completion leads to the correct solution\n",
        "        - Soft estimation: quality score is the proportion that leads to the correct solution\n",
        "- RL: after training the PRM, use PPO in a step by step manner. Instead of using PPO with ORM which only offers a reward at the end of the response, step by step PPO has rewards at the end of each reasoning step"
      ],
      "metadata": {
        "id": "o3gZE-T32FoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yklet23qFWwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DSPy\n",
        "\n",
        "\n",
        "The framework for programming—not prompting—foundation models\n",
        "\n",
        "\n",
        "Define a task, a metric, and a general architecture and DSPy will optimize it.\n",
        "\n",
        "For example:\n",
        "\n",
        "1. Task: sentiment classification\n",
        "2. Metric: Accuracy\n",
        "3. Architecture: Few shot with CoT\n",
        "\n",
        "DSPy attempts to be the PyTorch for optimizing LLM Programs.\n",
        "\n",
        "In PyTorch\n",
        "1. Task: sentiment classification\n",
        "2. Metric: Accuracy\n",
        "3. Architecture: BERT/GPT/Number of tranformer blocks/etc\n",
        "\n",
        "\n",
        "PyTorch tunes weights, DSPy can tune LM weights, few shot examples, prompt, and, demonstrations"
      ],
      "metadata": {
        "id": "xFX_L5IMGanj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0GfEsJEsxPg",
        "outputId": "e36fe695-6f7b-40d8-d3f9-6a59e1743fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'dspy': No such file or directory\n",
            "Cloning into 'dspy'...\n",
            "remote: Enumerating objects: 28454, done.\u001b[K\n",
            "remote: Counting objects: 100% (362/362), done.\u001b[K\n",
            "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
            "remote: Total 28454 (delta 209), reused 293 (delta 178), pack-reused 28092\u001b[K\n",
            "Receiving objects: 100% (28454/28454), 26.85 MiB | 6.60 MiB/s, done.\n",
            "Resolving deltas: 100% (12566/12566), done.\n",
            "Updating files: 100% (4381/4381), done.\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Collecting dspy-ai\n",
            "  Downloading dspy_ai-2.4.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting backoff~=2.2.1 (from dspy-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib~=1.3.2 in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (1.3.2)\n",
            "Collecting openai<2.0.0,>=0.28.1 (from dspy-ai)\n",
            "  Downloading openai-1.16.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2023.12.25)\n",
            "Collecting ujson (from dspy-ai)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (4.66.2)\n",
            "Collecting datasets<3.0.0,~=2.14.6 (from dspy-ai)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.31.0)\n",
            "Collecting optuna (from dspy-ai)\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pydantic==2.5.0 (from dspy-ai)\n",
            "  Downloading pydantic-2.5.0-py3-none-any.whl.metadata (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.6/174.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.0->dspy-ai) (0.6.0)\n",
            "Collecting pydantic-core==2.14.1 (from pydantic==2.5.0->dspy-ai)\n",
            "  Downloading pydantic_core-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.0->dspy-ai) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting xxhash (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets<3.0.0,~=2.14.6->dspy-ai) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (2024.2.2)\n",
            "Collecting alembic>=1.5.0 (from optuna->dspy-ai)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna->dspy-ai)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->dspy-ai) (2.0.29)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai) (2024.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->dspy-ai)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (4.0.3)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets<3.0.0,~=2.14.6->dspy-ai) (3.13.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dspy-ai) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->dspy-ai) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna->dspy-ai) (2.1.5)\n",
            "Downloading dspy_ai-2.4.0-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.5/407.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, ujson, pydantic-core, Mako, h11, dill, colorlog, backoff, pydantic, multiprocess, httpcore, alembic, optuna, httpx, openai, datasets, dspy-ai\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.16.3\n",
            "    Uninstalling pydantic_core-2.16.3:\n",
            "      Successfully uninstalled pydantic_core-2.16.3\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.4\n",
            "    Uninstalling pydantic-2.6.4:\n",
            "      Successfully uninstalled pydantic-2.6.4\n",
            "Successfully installed Mako-1.3.2 alembic-1.13.1 backoff-2.2.1 colorlog-6.8.2 datasets-2.14.7 dill-0.3.7 dspy-ai-2.4.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 multiprocess-0.70.15 openai-1.16.2 optuna-3.6.1 pydantic-2.5.0 pydantic-core-2.14.1 ujson-5.9.0 xxhash-3.4.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting openai~=0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai~=0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai~=0.28.1) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai~=0.28.1) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28.1) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28.1) (4.0.3)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.16.2\n",
            "    Uninstalling openai-1.16.2:\n",
            "      Successfully uninstalled openai-1.16.2\n",
            "Successfully installed openai-0.28.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "try: # When on google Colab, let's clone the notebook so we download the cache.\n",
        "    import google.colab\n",
        "    repo_path = 'dspy'\n",
        "    !git -C $repo_path pull origin || git clone https://github.com/stanfordnlp/dspy $repo_path\n",
        "except:\n",
        "    repo_path = '.'\n",
        "\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)\n",
        "\n",
        "# Set up the cache for this notebook\n",
        "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(repo_path, 'cache')\n",
        "\n",
        "import pkg_resources # Install the package if it's not installed\n",
        "if not \"dspy-ai\" in {pkg.key for pkg in pkg_resources.working_set}:\n",
        "    !pip install -U pip\n",
        "    !pip install dspy-ai\n",
        "    !pip install openai~=0.28.1\n",
        "    # !pip install -e $repo_path\n",
        "\n",
        "import dspy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"TOGETHER_API_KEY\"] = userdata.get('TOGETHER_API_KEY')"
      ],
      "metadata": {
        "id": "Zw_0Dp4Ss023"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = dspy.OpenAI(\n",
        "    model=\"Qwen/Qwen1.5-4B-Chat\",\n",
        "    max_tokens=400,\n",
        "    model_type=\"chat\",\n",
        "    api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
        "    api_base=\"https://api.together.xyz/v1\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "-UlkZ-QRs4gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm(\"what is your name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU0K7MDls7kO",
        "outputId": "dd1ff2fd-adff-4430-f069-21ba8db08d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am Qwen, a large language model created by Alibaba Cloud.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
        "dspy.settings.configure(lm=lm, rm=colbertv2_wiki17_abstracts)"
      ],
      "metadata": {
        "id": "fz9Y1vYps9BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Signature\n",
        "\n",
        "Textual specificiation of input/output of of a DSPy module\n",
        "\n",
        "* This is a way for you to tell the LM what it needs to do, not how.\n"
      ],
      "metadata": {
        "id": "KFFRHR9ntZUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shorthand signatures"
      ],
      "metadata": {
        "id": "xHxc-RzpuS0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n",
        "\n",
        "classify = dspy.Predict('sentence -> sentiment')\n",
        "classify(sentence=sentence).sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n7Tr58N2tC57",
        "outputId": "8045f51b-37dc-409f-f169-8e755fbfd822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from the XSum dataset.\n",
        "document = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\n",
        "\n",
        "summarize = dspy.ChainOfThought('document -> summary')\n",
        "response = summarize(document=document)\n",
        "\n",
        "print(response.summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ5FCnMKt4QM",
        "outputId": "851f65d5-8d9a-43da-dcf2-c49ab665565a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The article is about a 21-year-old football player who made seven appearances for a team called the Hammers and scored his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. He had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of his contract with the promoted Tykes has not been revealed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwoZh45duMbP",
        "outputId": "84cc3f59-4ad5-4af2-80ee-6fd3a1df0062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale='Produce the summary.',\n",
              "    summary=\"The article is about a 21-year-old football player who made seven appearances for a team called the Hammers and scored his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. He had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of his contract with the promoted Tykes has not been revealed.\"\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghhbys3Dt7iX",
        "outputId": "90fadc2c-631a-4390-f972-ae33c05d48ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the fields `document`, produce the fields `summary`.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Document: ${document}\n",
            "Reasoning: Let's think step by step in order to ${produce the summary}. We ...\n",
            "Summary: ${summary}\n",
            "\n",
            "---\n",
            "\n",
            "Document: The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\n",
            "Reasoning: Let's think step by step in order to\u001b[32m Produce the summary.\n",
            "Summary: The article is about a 21-year-old football player who made seven appearances for a team called the Hammers and scored his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. He had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of his contract with the promoted Tykes has not been revealed.\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiple outputs"
      ],
      "metadata": {
        "id": "MUITK1CyvD8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"it's a charming and often affecting journey.\"\n",
        "\n",
        "classify = dspy.Predict('sentence -> sentiment, emotion')\n",
        "classify(sentence=sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75_uq3fUu9P5",
        "outputId": "5465974c-6f8c-4cc0-d47c-eb30dc8a00da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    sentiment='positive',\n",
              "    emotion='charming, affecting'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUJb0TYSvkc1",
        "outputId": "d02da3c3-920d-480e-cb37-739e1e43fe67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the fields `sentence`, produce the fields `sentiment`, `emotion`.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Sentence: ${sentence}\n",
            "Sentiment: ${sentiment}\n",
            "Emotion: ${emotion}\n",
            "\n",
            "---\n",
            "\n",
            "Sentence: it's a charming and often affecting journey.\n",
            "Sentiment:\u001b[32m positive\n",
            "Emotion: charming, affecting\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiple Inputs"
      ],
      "metadata": {
        "id": "jO41sLf2vI6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n",
        "comment = \"the view was stunning.\"\n",
        "\n",
        "classify = dspy.Predict('sentence, comment -> sentiment')\n",
        "classify(sentence=sentence, comment=comment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYpB0CkxvNCw",
        "outputId": "c2f2f8fd-b4b0-46a5-904a-d2f96c75e54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    sentiment='Positive'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXO8Lg-PvhwP",
        "outputId": "58065ad7-dd92-469a-e439-e4e05f02ca2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the fields `sentence`, `comment`, produce the fields `sentiment`.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Sentence: ${sentence}\n",
            "Comment: ${comment}\n",
            "Sentiment: ${sentiment}\n",
            "\n",
            "---\n",
            "\n",
            "Sentence: it's a charming and often affecting journey.\n",
            "Comment: the view was stunning.\n",
            "Sentiment:\u001b[32m Positive\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Other potential signatures\n",
        "\n",
        "\n",
        "\n",
        "1. Question Answering: `\"question -> answer\"`\n",
        "\n",
        "2. Sentiment Classification: `\"sentence -> sentiment\"`\n",
        "\n",
        "3. Summarization: `\"document -> summary\"`\n",
        "\n",
        "4. Retrieval-Augmented Question Answering: `\"context, question -> answer\"`\n",
        "\n",
        "5. Multiple-Choice Question Answering with Reasoning: `\"question, choices -> reasoning, selection\"`"
      ],
      "metadata": {
        "id": "dMvLvbpkvsMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class based Signatures\n",
        "\n",
        "Adds more clarity to your prompt"
      ],
      "metadata": {
        "id": "81NABmmGuWTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Emotion(dspy.Signature):\n",
        "    \"\"\"Classify emotion among sadness, joy, love, anger, fear, surprise.\"\"\"\n",
        "\n",
        "    sentence = dspy.InputField()\n",
        "    sentiment = dspy.OutputField()\n",
        "\n",
        "sentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\n",
        "\n",
        "classify = dspy.Predict(Emotion)\n",
        "classify(sentence=sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgwhJGq8ubsR",
        "outputId": "5cc8f175-8dce-40ea-fd72-3122678f71bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    sentiment='sadness'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCR5AFErt8oD",
        "outputId": "9fdd4e69-8d73-4c5c-cc73-300b3db06e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Classify emotion among sadness, joy, love, anger, fear, surprise.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Sentence: ${sentence}\n",
            "Sentiment: ${sentiment}\n",
            "\n",
            "---\n",
            "\n",
            "Sentence: i started feeling a little vulnerable when the giant spotlight started blinding me\n",
            "Sentiment:\u001b[32m sadness\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiple inputs"
      ],
      "metadata": {
        "id": "h8mSnHAFwVUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CheckCitationFaithfulness(dspy.Signature):\n",
        "    \"\"\"Verify that the text is based on the provided context.\"\"\"\n",
        "\n",
        "    context = dspy.InputField(desc=\"facts here are assumed to be true\")\n",
        "    text = dspy.InputField()\n",
        "    faithfulness = dspy.OutputField(desc=\"True/False indicating if text is faithful to context\")\n",
        "\n",
        "context = \"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\n",
        "\n",
        "text = \"Lee scored 3 goals for Colchester United.\"\n",
        "\n",
        "faithfulness = dspy.ChainOfThought(CheckCitationFaithfulness)\n",
        "faithfulness(context=context, text=text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i28q4sE0uho5",
        "outputId": "267104f4-b3ab-4dbc-ef30-efbb72a536a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale=\"verify the faithfulness of the text to the context. We need to check if the text accurately reflects the information provided in the context.\\n\\nContext: The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\\n\\nText: Lee scored 3 goals for Colchester United.\\n\\nReasoning: The text states that Lee scored twice for Colchester United, which is not consistent with the information provided in the context. The context states that Lee scored his only goal for the Hammers and scored twice for Colchester United, but not three goals.\",\n",
              "    faithfulness='False'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDyZIn93waDK",
        "outputId": "fb550b19-0a8d-439c-f261-3deac925eb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Verify that the text is based on the provided context.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Context: facts here are assumed to be true\n",
            "\n",
            "Text: ${text}\n",
            "\n",
            "Reasoning: Let's think step by step in order to ${produce the faithfulness}. We ...\n",
            "\n",
            "Faithfulness: True/False indicating if text is faithful to context\n",
            "\n",
            "---\n",
            "\n",
            "Context: The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\n",
            "\n",
            "Text: Lee scored 3 goals for Colchester United.\n",
            "\n",
            "Reasoning: Let's think step by step in order to\u001b[32m verify the faithfulness of the text to the context. We need to check if the text accurately reflects the information provided in the context.\n",
            "\n",
            "Context: The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\n",
            "\n",
            "Text: Lee scored 3 goals for Colchester United.\n",
            "\n",
            "Reasoning: The text states that Lee scored twice for Colchester United, which is not consistent with the information provided in the context. The context states that Lee scored his only goal for the Hammers and scored twice for Colchester United, but not three goals.\n",
            "\n",
            "Faithfulness: False\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bK7hhhv8wdhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modules\n",
        "\n",
        "Each built-in module abstracts a prompting technique (like chain of thought or ReAct)"
      ],
      "metadata": {
        "id": "5dWw0EhmwwKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CoT"
      ],
      "metadata": {
        "id": "3gABdnMyzlkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are interesting topics in machine learning?\"\n",
        "\n",
        "# 1) Declare with a signature, and pass some config.\n",
        "answerer = dspy.ChainOfThought('question -> answer')\n",
        "\n",
        "# 2) Call with input argument.\n",
        "response = answerer(question=question)\n",
        "\n",
        "# 3) Access the outputs.\n",
        "response.answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "USbNab6Ywx2U",
        "outputId": "cc79935a-4ad2-4c1b-e4b5-98d220cc5f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Interesting topics in machine learning include deep learning, natural language processing, computer vision, and reinforcement learning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK0ilX6KxUDJ",
        "outputId": "9d211a83-502e-4d3a-ee36-b07388e25baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the fields `question`, produce the fields `answer`.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Question: ${question}\n",
            "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
            "Answer: ${answer}\n",
            "\n",
            "---\n",
            "\n",
            "Question: What are interesting topics in machine learning?\n",
            "Reasoning: Let's think step by step in order to produce the answer. We need to identify interesting topics in machine learning. Some interesting topics in machine learning include deep learning, natural language processing, computer vision, and reinforcement learning. These topics are currently being researched and have the potential to revolutionize various industries.\n",
            "Answer:\u001b[32m Interesting topics in machine learning include deep learning, natural language processing, computer vision, and reinforcement learning.\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sATgOigxdFH",
        "outputId": "3e8905f5-9b0f-4513-9dc4-4594a86e0068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale='produce the answer. We need to identify interesting topics in machine learning. Some interesting topics in machine learning include deep learning, natural language processing, computer vision, and reinforcement learning. These topics are currently being researched and have the potential to revolutionize various industries.',\n",
              "    answer='Interesting topics in machine learning include deep learning, natural language processing, computer vision, and reinforcement learning.'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CoT with Hint"
      ],
      "metadata": {
        "id": "XfraBfTFy5FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a simple signature for basic question answering\n",
        "class BasicQA(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
        "\n",
        "#Pass signature to ChainOfThought module\n",
        "generate_answer = dspy.ChainOfThoughtWithHint(BasicQA)\n",
        "\n",
        "# Call the predictor on a particular input alongside a hint.\n",
        "question='What is the color of the sky?'\n",
        "hint = \"It's what you often see during a sunny day.\"\n",
        "pred = generate_answer(question=question, hint=hint)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Predicted Answer: {pred.answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8SUvQnHx_yt",
        "outputId": "cf15841c-3d03-429a-f819-876481cd1378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the color of the sky?\n",
            "Predicted Answer: Blue.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FkzMpH1y8Rv",
        "outputId": "6f289fc5-677e-4f29-8ee1-f26519c485f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer questions with short factoid answers.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Question: ${question}\n",
            "\n",
            "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
            "\n",
            "Hint: ${hint}\n",
            "\n",
            "Answer: often between 1 and 5 words\n",
            "\n",
            "---\n",
            "\n",
            "Question: What is the color of the sky?\n",
            "\n",
            "Reasoning: Let's think step by step in order to Blue.\n",
            "\n",
            "Hint: It's what you often see during a sunny day.\n",
            "\n",
            "Answer:\u001b[32m Blue.\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ReACT\n",
        "\n",
        "https://arxiv.org/abs/2210.03629\n",
        "\n",
        "It is specifically designed to compose the interleaved steps of Thought, Action, and Observation."
      ],
      "metadata": {
        "id": "GpXv9lmlzY8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple signature for basic question answering\n",
        "class BasicQA(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
        "\n",
        "# Pass signature to ReAct module\n",
        "react_module = dspy.ReAct(BasicQA)\n",
        "\n",
        "# Call the ReAct module on a particular input\n",
        "question = 'What is the color of the sky?'\n",
        "result = react_module(question=question)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Final Predicted Answer (after ReAct process): {result.answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbALI7KdzAhs",
        "outputId": "c0024b0c-dc26-4a32-ff90-41db8fd10ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the color of the sky?\n",
            "Final Predicted Answer (after ReAct process): blue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHLuSWlczONB",
        "outputId": "6665fcb8-8ee9-481e-a1ae-6407f6cc127e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "You will be given `question` and you will respond with `answer`.\n",
            "\n",
            "To do this, you will interleave Thought, Action, and Observation steps.\n",
            "\n",
            "Thought can reason about the current situation, and Action can be the following types:\n",
            "\n",
            "(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n",
            "(2) Finish[answer], which returns the final `answer` and finishes the task\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Question: ${question}\n",
            "\n",
            "Thought 1: next steps to take based on last observation\n",
            "\n",
            "Action 1: always either Search[query] or, when done, Finish[answer]\n",
            "\n",
            "Observation 1: observations based on action\n",
            "\n",
            "Thought 2: next steps to take based on last observation\n",
            "\n",
            "Action 2: always either Search[query] or, when done, Finish[answer]\n",
            "\n",
            "---\n",
            "\n",
            "Question: What is the color of the sky?\n",
            "\n",
            "Thought 1: I need to find information about the color of the sky.\n",
            "\n",
            "Action 1: Search[sky color]\n",
            "\n",
            "Observation 1:\n",
            "[1] «Sky blue | Sky blue is the name of a colour that resembles the colour of the sky at noon. The entry for \"sky-blue\" in Murray's \"New English Dictionary\" (1919) reports a first sighting of the term in the article on \"silver\" in Ephraim Chambers's \"Cyclopaedia\" of 1728. However, many writers had used the term \"sky blue\" to name a colour before Chambers. For example, we find \"sky blue\" in \"A Collection of Voyages and Travels\" (London: Awnsham and John Churchill, 1704), vol. 2, p. 322, where John Nieuhoff describes certain flowers: \"they are of a lovely sky blue colour, and yellow in the middle\".»\n",
            "[2] «What Color Is Your Sky | What Color Is Your Sky is a studio album recorded by American country singer Jason Michael Carroll. It is Carroll's fourth studio album, the first since his 2011 Numbers album.»\n",
            "[3] «Atmospheric optics | Atmospheric optics deals with how the unique optical properties of the Earth's atmosphere cause a wide range of spectacular optical phenomena. The blue color of the sky is a direct result of Rayleigh scattering which redirects higher frequency (blue) sunlight back into the field of view of the observer. Because blue light is scattered more easily than red light, the sun takes on a reddish hue when it is observed through a thick atmosphere, as during a sunrise or sunset. Additional particulate matter in the sky can scatter different colors at different angles creating colorful glowing skies at dusk and dawn. Scattering off of ice crystals and other particles in the atmosphere are responsible for halos, afterglows, coronas, rays of sunlight, and sun dogs. The variation in these kinds of phenomena is due to different particle sizes and geometries.»\n",
            "\n",
            "Thought 2: The color of the sky is blue. Action 1: Finish[blue]\n",
            "\n",
            "Action 2:\u001b[32m Finish[blue]\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieve"
      ],
      "metadata": {
        "id": "7CUQSUdjzsDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query='When was the first FIFA World Cup held?'\n",
        "\n",
        "# Call the retriever on a particular query.\n",
        "retrieve = dspy.Retrieve(k=3)\n",
        "topK_passages = retrieve(query).passages\n",
        "\n",
        "print(f\"Top {retrieve.k} passages for question: {query} \\n\", '-' * 30, '\\n')\n",
        "\n",
        "for idx, passage in enumerate(topK_passages):\n",
        "    print(f'{idx+1}]', passage, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O509WzCzzO6i",
        "outputId": "ed3e7a36-b3e9-4ae9-f482-1cbfb48c3480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 passages for question: When was the first FIFA World Cup held? \n",
            " ------------------------------ \n",
            "\n",
            "1] History of the FIFA World Cup | The FIFA World Cup was first held in 1930, when FIFA president Jules Rimet decided to stage an international football tournament. The inaugural edition, held in 1930, was contested as a final tournament of only thirteen teams invited by the organization. Since then, the World Cup has experienced successive expansions and format remodeling to its current 32-team final tournament preceded by a two-year qualifying process, involving over 200 teams from around the world. \n",
            "\n",
            "2] 1950 FIFA World Cup | The 1950 FIFA World Cup, held in Brazil from 24 June to 16 July 1950, was the fourth FIFA World Cup. It was the first World Cup since 1938, the planned 1942 and 1946 competitions having been cancelled owing to World War II. It was won by Uruguay, who had won the inaugural competition in 1930, clinching the cup by beating the hosts Brazil 2–1 in the deciding match of the four-team final group (this was the only tournament not decided by a one-match final). It was also the first tournament where the trophy was referred to as the Jules Rimet Cup, to mark the 25th anniversary of Jules Rimet's presidency of FIFA. \n",
            "\n",
            "3] 1970 FIFA World Cup | The 1970 FIFA World Cup was the ninth FIFA World Cup, the quadrennial international football championship for men's national teams. Held from 31 May to 21 June in Mexico, it was the first World Cup tournament staged in North America, and the first held outside Europe and South America. Teams representing 75 nations from all six populated continents entered the competition, and its qualification rounds began in May 1968. Fourteen teams qualified from this process to join host nation Mexico and defending champions England in the sixteen-team final tournament. El Salvador, Israel, and Morocco made their first appearances at the final stage, and Peru their first since 1930. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "\n",
        "Data is represented using the `Example` type. It is similar to python `dict`s but have additional utility functions.\n",
        "\n",
        "All DSPy modules return `Prediction`s which are subclassed from `Example`"
      ],
      "metadata": {
        "id": "Xke83I_x02EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pair = dspy.Example(question=\"This is a question?\", answer=\"This is an answer.\")\n",
        "\n",
        "print(qa_pair)\n",
        "print(qa_pair.question)\n",
        "print(qa_pair.answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSG65PKa03fr",
        "outputId": "767f30dc-7112-440f-da60-bf867a8bd1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)\n",
            "This is a question?\n",
            "This is an answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our example above, we specify the field values, but we don't know which field are inputs and which are outputs. Think back to when we were doing supervised fine tuning. We had our prompt (the input) and the completion (the output).\n",
        "\n",
        "\n",
        "To represent this in DSPy, we can use the `with_inputs` function."
      ],
      "metadata": {
        "id": "ydTL94VhALuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_summary = dspy.Example(article= \"This is an article.\", summary= \"This is a summary.\").with_inputs(\"article\")\n",
        "\n",
        "input_key_only = article_summary.inputs()\n",
        "non_input_key_only = article_summary.labels()\n",
        "\n",
        "print(\"Example object with Input fields only:\", input_key_only)\n",
        "print(\"Example object with Non-Input fields only:\", non_input_key_only)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVncV799CXWr",
        "outputId": "286f150e-7fcf-427b-adc7-ad5e21139819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example object with Input fields only: Example({'article': 'This is an article.'}) (input_keys=None)\n",
            "Example object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "\n",
        "In PyTorch, we have metrics that we are optimizing. For example, it may be the negative log likelihood loss, or an L2 loss.\n",
        "\n",
        "\n",
        "There are calculations that are can be automatically computed.\n",
        "\n",
        "\n",
        "In DSPy, metrics are functions that take the inputs and outputs of your DSPy program and return a score.\n",
        "\n",
        "For classification tasks such as sentiment analysis, it may be possible to use simple metrics like F1 or accuracy but for more sophisticated answers, this may not be sufficient.\n",
        "\n",
        "\n",
        "For more sophisticated tasks, you may want to use a form of LM feedback. For example, we can leverage techniques we've used previously, such as LLM as a Judge, or a reward model.\n",
        "\n",
        "Metrics are flexible you can implement any python logic that you want. For example, you could count the number of words to and return a score of 1 when it is fewer than 5 words and 0 otherwise.\n",
        "\n",
        "You can also use criteria and return the average or minimum of each score. For example, you can count the number of words and get the score from the LLM as a Judge, normalize, then return the minimum of both scores.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WzrdeCN305gU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exact match metric"
      ],
      "metadata": {
        "id": "krdHSu16FwK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_answer(example, pred, trace=None):\n",
        "    return example.answer.lower() == pred.answer.lower()"
      ],
      "metadata": {
        "id": "_VR1tzivDy9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trace\n",
        "\n",
        "\n",
        "When compiling, `trace` is not None, we want to be strict about judging things, so we will only return `True`, since we use these examples for bootstrapping demonstrations.\n",
        "\n",
        "`trace` is `None` if it is used for evaluation or optimization\n",
        "Otherwise, it will return `bool` for bootstrapping demonstrations"
      ],
      "metadata": {
        "id": "wyOAu6F9F0pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_context_and_answer(example, pred, trace=None):\n",
        "    # check the gold label and the predicted answer are the same\n",
        "    answer_match = example.answer.lower() == pred.answer.lower()\n",
        "\n",
        "    # check the predicted answer comes from one of the retrieved contexts\n",
        "    context_match = any((pred.answer.lower() in c) for c in pred.context)\n",
        "\n",
        "    if trace is None: # if we're doing evaluation or optimization\n",
        "        return (answer_match + context_match) / 2.0\n",
        "    else: # if we're doing bootstrapping, i.e. self-generating good demonstrations of each step\n",
        "        return answer_match and context_match"
      ],
      "metadata": {
        "id": "HSuQwisjFzZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metric(example, pred, trace=None):\n",
        "    answer_match = example.sentiment.lower() == pred.sentiment.lower()\n",
        "    word_count = len(pred.sentiment.strip().split()) == 1\n",
        "    if trace is None:\n",
        "        return min(int(answer_match), int(word_count))\n",
        "    else:\n",
        "        return answer_match and word_count"
      ],
      "metadata": {
        "id": "vngfWewGGXnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "dev_set = [dspy.Example(statement=\"Today is a lovely day.\", sentiment=\"Positive\").with_inputs(\"statement\")]\n",
        "\n",
        "program = dspy.Predict(\"statement -> sentiment\")\n",
        "\n",
        "for x in dev_set:\n",
        "    pred = program(**x.inputs())\n",
        "    print(\"Prediction: \")\n",
        "    print(pred)\n",
        "    score = metric(x, pred)\n",
        "    print(\"Score: \")\n",
        "    print(score)\n",
        "    scores.append(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBZTMVD8XebC",
        "outputId": "f9c83029-5e2f-47c9-e97d-6bd3ba7e244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: \n",
            "Prediction(\n",
            "    sentiment='Positive'\n",
            ")\n",
            "Score: \n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using AI Feedback"
      ],
      "metadata": {
        "id": "AxXqX_ACYfyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Assess(dspy.Signature):\n",
        "    \"\"\"Assess the quality of a tweet along the specific dimension.\"\"\"\n",
        "\n",
        "    assessed_text = dspy.InputField()\n",
        "    assessment_question = dspy.InputField()\n",
        "    assessment_answer = dspy.OutputField(desc=\"yes/no\")\n"
      ],
      "metadata": {
        "id": "3WPDf4xiYkdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metric(example, pred, trace=None):\n",
        "    question, answer, tweet = example.question, example.answer, pred.output\n",
        "\n",
        "    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\n",
        "    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\n",
        "\n",
        "    correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\n",
        "    print(correct)\n",
        "    lm.inspect_history(n=1)\n",
        "\n",
        "\n",
        "    engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\n",
        "    print(engaging)\n",
        "    lm.inspect_history(n=1)\n",
        "\n",
        "    correct, engaging = [m.assessment_answer.lower() == 'yes.' for m in [correct, engaging]]\n",
        "    score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\n",
        "\n",
        "    if trace is not None: return score >= 2\n",
        "    return score / 2.0"
      ],
      "metadata": {
        "id": "LgudDM1GYGvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "dev_set = [dspy.Example(question=\"Is machine learning cool?\", answer=\"Machine learning is very cool.\").with_inputs(\"question\")]\n",
        "\n",
        "program = dspy.Predict(\"question -> output\")\n",
        "\n",
        "for x in dev_set:\n",
        "    pred = program(**x.inputs())\n",
        "    print(pred)\n",
        "    score = metric(x, pred)\n",
        "    print(score)\n",
        "    scores.append(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5lftLhGZk1p",
        "outputId": "654bb0ef-5709-4fc7-f905-a24823c1724f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    output='Yes, machine learning is cool!'\n",
            ")\n",
            "Prediction(\n",
            "    assessment_answer='Yes.'\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Assess the quality of a tweet along the specific dimension.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Assessed Text: ${assessed_text}\n",
            "Assessment Question: ${assessment_question}\n",
            "Assessment Answer: yes/no\n",
            "\n",
            "---\n",
            "\n",
            "Assessed Text: Yes, machine learning is cool!\n",
            "Assessment Question: The text should answer `Is machine learning cool?` with `Machine learning is very cool.`. Does the assessed text contain this answer?\n",
            "Assessment Answer:\u001b[32m Yes.\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "Prediction(\n",
            "    assessment_answer='Yes.'\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Assess the quality of a tweet along the specific dimension.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Assessed Text: ${assessed_text}\n",
            "Assessment Question: ${assessment_question}\n",
            "Assessment Answer: yes/no\n",
            "\n",
            "---\n",
            "\n",
            "Assessed Text: Yes, machine learning is cool!\n",
            "Assessment Question: Does the assessed text make for a self-contained, engaging tweet?\n",
            "Assessment Answer:\u001b[32m Yes.\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKOanNNSaYYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizers"
      ],
      "metadata": {
        "id": "6NeSGkWKbbhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional deep neural networks (DNNs) can be optimized with gradient descent, given a loss function and some training data.\n",
        "\n",
        "A DSPy module has 3 kinds of parameters:\n",
        "1. LM weights\n",
        "2. instructions\n",
        "3. demonstrations of the input/output behavior\n",
        "\n",
        "\n",
        "Parameter updates\n",
        "1. LM Weights -> Fine Tuning LM model weights\n",
        "2. Instructions -> Updating instructions\n",
        "3. Demonstrations -> selecting/creating few shot examples\n",
        "\n",
        "Compiling often leads to better prompts than what humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things, much more systematically, and tune the metrics directly.\n"
      ],
      "metadata": {
        "id": "s_QsiowseJBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dspy.teleprompt import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRW_q_9qsRA1",
        "outputId": "a35f4a9e-8271-4916-df8c-f53309f8bd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dspy/cache/compiler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labeled Few Shot\n",
        "\n",
        "Construct few shot exmaples from provided labeled Q/A pairs"
      ],
      "metadata": {
        "id": "EJ8YkydIe-4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dspy.teleprompt import LabeledFewShot\n",
        "from dspy import Example\n",
        "\n",
        "dataset = [\n",
        "    Example(text=\"I'm so excited for my vacation next week!\", emotion=\"happy\"),\n",
        "    Example(text=\"I feel like I'm not good enough.\", emotion=\"sad\"),\n",
        "    Example(text=\"I can't believe I got the job! I'm thrilled!\", emotion=\"happy\"),\n",
        "    Example(text=\"I'm really worried about my presentation tomorrow.\", emotion=\"anxious\"),\n",
        "    Example(text=\"I miss my family so much. I wish I could see them.\", emotion=\"sad\"),\n",
        "    Example(text=\"I'm having the best day ever!\", emotion=\"happy\"),\n",
        "    Example(text=\"I'm so stressed out about this deadline.\", emotion=\"anxious\"),\n",
        "    Example(text=\"I feel like crying. Nothing seems to be going right.\", emotion=\"sad\"),\n",
        "    Example(text=\"I'm over the moon! We're finally getting married!\", emotion=\"happy\"),\n",
        "    Example(text=\"I'm terrified of heights. I don't think I can do this.\", emotion=\"anxious\"),\n",
        "    Example(text=\"I feel so alone. No one understands me.\", emotion=\"sad\"),\n",
        "    Example(text=\"I can't stop smiling! Today has been amazing!\", emotion=\"happy\"),\n",
        "    Example(text=\"I'm really anxious about this exam. I hope I pass.\", emotion=\"anxious\"),\n",
        "    Example(text=\"I'm feeling down. I think I need a hug.\", emotion=\"sad\"),\n",
        "    Example(text=\"I'm so grateful for my friends and family!\", emotion=\"happy\"),\n",
        "    Example(text=\"I'm having a panic attack. I can't breathe.\", emotion=\"anxious\"),\n",
        "    Example(text=\"I feel like I'm in a rut. Nothing excites me anymore.\", emotion=\"sad\"),\n",
        "    Example(text=\"I'm on top of the world! Everything is going perfectly!\", emotion=\"happy\"),\n",
        "    Example(text=\"I'm so nervous about this first date. What if it goes badly?\", emotion=\"anxious\"),\n",
        "    Example(text=\"I feel so empty inside. I don't know what to do.\", emotion=\"sad\")\n",
        "]\n",
        "\n",
        "dataset = [e.with_inputs(\"text\") for e in dataset]\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "train_set =  dataset[:train_size]\n",
        "dev_set = dataset[train_size:]\n",
        "\n",
        "\n",
        "\n",
        "class EmotionClassifier(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.program = dspy.Predict(\"text -> emotion\")\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.program(text=text)\n",
        "\n",
        "def metric(example, pred, trace=None):\n",
        "    text, gt_emotion, predicted_emotion = example.text, example.emotion, pred.emotion\n",
        "    score = predicted_emotion.strip().lower() == gt_emotion\n",
        "\n",
        "    if trace is not None: return score\n",
        "    return float(score)\n",
        "\n",
        "\n",
        "program = EmotionClassifier()\n",
        "labeled_fewshot_optimizer = LabeledFewShot(k=3)\n",
        "your_dspy_program_compiled = labeled_fewshot_optimizer.compile(student = program, trainset=train_set)"
      ],
      "metadata": {
        "id": "yBA5S9kJeHzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_dspy_program_compiled(dev_set[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsW_1zAh0UI7",
        "outputId": "17c27aa4-ce86-48d2-bfc3-51cdb0d657d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    emotion='depressed'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBjPcFpz0dFF",
        "outputId": "62e9d7a6-aacf-4d35-8abd-e4fa8060a72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the fields `text`, produce the fields `emotion`.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Text: ${text}\n",
            "Emotion: ${emotion}\n",
            "\n",
            "---\n",
            "\n",
            "Text: I'm really anxious about this exam. I hope I pass.\n",
            "Emotion: anxious\n",
            "\n",
            "---\n",
            "\n",
            "Text: I'm having a panic attack. I can't breathe.\n",
            "Emotion: anxious\n",
            "\n",
            "---\n",
            "\n",
            "Text: I'm so stressed out about this deadline.\n",
            "Emotion: anxious\n",
            "\n",
            "---\n",
            "\n",
            "Text: I feel like I'm in a rut. Nothing excites me anymore.\n",
            "Emotion:\u001b[32m depressed\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_dspy_program_compiled.save(\"./emotion_classifier_few_shot_labeled.json\")"
      ],
      "metadata": {
        "id": "2WmSTN3P0_H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F22T6A441Gmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"./emotion_classifier_few_shot_labeled.json\", \"r\") as f:\n",
        "    data = json.loads(f.read())\n",
        "    print(json.dumps(data, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMAaxqCR1HSX",
        "outputId": "05963633-b5f9-4645-e80f-a029dd31b612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"program\": {\n",
            "        \"lm\": null,\n",
            "        \"traces\": [],\n",
            "        \"train\": [],\n",
            "        \"demos\": [\n",
            "            {\n",
            "                \"text\": \"I'm really anxious about this exam. I hope I pass.\",\n",
            "                \"emotion\": \"anxious\"\n",
            "            },\n",
            "            {\n",
            "                \"text\": \"I'm having a panic attack. I can't breathe.\",\n",
            "                \"emotion\": \"anxious\"\n",
            "            },\n",
            "            {\n",
            "                \"text\": \"I'm so stressed out about this deadline.\",\n",
            "                \"emotion\": \"anxious\"\n",
            "            }\n",
            "        ],\n",
            "        \"signature_instructions\": \"Given the fields `text`, produce the fields `emotion`.\",\n",
            "        \"signature_prefix\": \"Emotion:\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bootstrap Few Shot\n",
        "\n",
        "Uses your program to self-generate complete demonstrations for every stage of your program. Will simply use the generated demonstrations (if they pass the metric) without any further optimization."
      ],
      "metadata": {
        "id": "yf74wsro6X-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "class EmotionCotClassifier(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.program = dspy.ChainOfThought(\"text -> emotion\")\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.program(text=text)\n",
        "\n",
        "def metric(example, pred, trace=None):\n",
        "    text, gt_emotion, predicted_emotion = example.text, example.emotion, pred.emotion\n",
        "    score = predicted_emotion.strip().lower() == gt_emotion\n",
        "\n",
        "    if trace is not None: return score\n",
        "    return float(score)\n",
        "\n",
        "program = EmotionCotClassifier()\n",
        "\n",
        "fewshot_optimizer = BootstrapFewShot(metric=metric, max_bootstrapped_demos=2, max_labeled_demos=4, max_rounds=1, max_errors=5)\n",
        "\n",
        "compiled_few_shot = fewshot_optimizer.compile(student=program, trainset=train_set[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0FOiY5nxbwX",
        "outputId": "a0e1852e-4d9d-4664-a321-84f60e40a968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2/4 [00:01<00:01,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 3 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_few_shot(dev_set[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtwydLN1y_AP",
        "outputId": "948576de-daf8-43f4-b881-68d5c960e7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale='${produce the emotion}. We need to analyze the text to determine the underlying emotion. The text mentions a feeling of being stuck in a rut, which suggests a lack of interest or motivation. Therefore, the emotion that best fits this text is \"boredom\".',\n",
              "    emotion='boredom'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6__qgHK3JJJ",
        "outputId": "10c491ed-aefa-4398-c0d3-e0b01bd66a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the fields `text`, produce the fields `emotion`.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Text: ${text}\n",
            "Reasoning: Let's think step by step in order to ${produce the emotion}. We ...\n",
            "Emotion: ${emotion}\n",
            "\n",
            "---\n",
            "\n",
            "Text: I'm so excited for my vacation next week!\n",
            "Reasoning: Let's think step by step in order to ${produce the emotion}. We need to consider the context of the text. The text mentions a vacation, which is typically associated with positive emotions such as excitement and anticipation. Therefore, the emotion that best fits the text is \"happy\".\n",
            "Emotion: happy\n",
            "\n",
            "---\n",
            "\n",
            "Text: I feel like I'm not good enough.\n",
            "Reasoning: Let's think step by step in order to ${produce the emotion}. We ...\n",
            "Emotion: sad\n",
            "\n",
            "---\n",
            "\n",
            "Text: I'm really worried about my presentation tomorrow.\n",
            "Emotion: anxious\n",
            "\n",
            "---\n",
            "\n",
            "Text: I can't believe I got the job! I'm thrilled!\n",
            "Emotion: happy\n",
            "\n",
            "---\n",
            "\n",
            "Text: I feel like I'm in a rut. Nothing excites me anymore.\n",
            "Reasoning: Let's think step by step in order to\u001b[32m ${produce the emotion}. We need to consider the context of the text. The text mentions a feeling of being stuck or unfulfilled, which is typically associated with negative emotions such as boredom or dissatisfaction. Therefore, the emotion that best fits the text is \"bored\".\n",
            "Emotion: bored\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtdeGtbD3MBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bootstrap Few Shot With Random Search\n",
        "\n",
        " Applies BootstrapFewShot several times with random search over generated demonstrations, and selects the best program."
      ],
      "metadata": {
        "id": "ab-4Uf6I6deu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
        "\n",
        "class EmotionCotClassifier(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.program = dspy.ChainOfThought(\"text -> emotion\")\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.program(text=text)\n",
        "\n",
        "def metric(example, pred, trace=None):\n",
        "    text, gt_emotion, predicted_emotion = example.text, example.emotion, pred.emotion\n",
        "    score = predicted_emotion.strip().lower() == gt_emotion\n",
        "\n",
        "    if trace is not None: return score\n",
        "    return float(score)\n",
        "\n",
        "program = EmotionCotClassifier()\n",
        "\n",
        "fewshot_with_random_search_optimizer = BootstrapFewShotWithRandomSearch(metric=metric, max_bootstrapped_demos=2, max_labeled_demos=4, max_rounds=1, max_errors=5)\n",
        "\n",
        "compiled_few_shot = fewshot_with_random_search_optimizer.compile(student=program, trainset=train_set[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxVkej-T6s6B",
        "outputId": "6601e502-4838-4c4a-bc17-d381fd70e5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Going to sample between 1 and 2 traces per predictor.\n",
            "Will attempt to train 16 candidate sets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 1.0 / 4  (25.0): 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 1.0 / 4  (25.0%)\n",
            "Score: 25.0 for set: [0]\n",
            "New best score: 25.0 for seed -3\n",
            "Scores so far: [25.0]\n",
            "Best score: 25.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:01<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "New best score: 100.0 for seed -2\n",
            "Scores so far: [25.0, 100.0]\n",
            "Best score: 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2/4 [00:00<00:00, 517.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 3 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:01<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3/4 [00:01<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 4 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:00<00:00,  5.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:01,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:01<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:01,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:00<00:00,  5.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:00, 957.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:00<00:00, 481.77it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:01,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:00<00:00,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3/4 [00:01<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 4 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:01<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:02,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:01<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2/4 [00:00<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 3 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:01<00:00,  3.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2/4 [00:00<00:00,  6.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 3 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:00<00:00, 385.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2/4 [00:00<00:00, 1063.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 3 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:00<00:00, 706.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:00, 1163.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:00<00:00, 827.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2/4 [00:00<00:00, 1198.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 3 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:00<00:00, 761.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3/4 [00:00<00:00, 10.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 4 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:00<00:00, 719.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3/4 [00:00<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 4 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3.0 / 4  (75.0): 100%|██████████| 4/4 [00:01<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 3.0 / 4  (75.0%)\n",
            "Score: 75.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 75.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:01,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:00<00:00,  5.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 75.0, 100.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:01,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 1 full traces after 2 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 4.0 / 4  (100.0): 100%|██████████| 4/4 [00:01<00:00,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 4.0 / 4  (100.0%)\n",
            "Score: 100.0 for set: [4]\n",
            "Scores so far: [25.0, 100.0, 100.0, 75.0, 75.0, 75.0, 75.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 100.0, 75.0, 75.0, 100.0, 100.0]\n",
            "Best score: 100.0\n",
            "Average of max per entry across top 1 scores: 1.0\n",
            "Average of max per entry across top 2 scores: 1.0\n",
            "Average of max per entry across top 3 scores: 1.0\n",
            "Average of max per entry across top 5 scores: 1.0\n",
            "Average of max per entry across top 8 scores: 1.0\n",
            "Average of max per entry across top 9999 scores: 1.0\n",
            "19 candidate programs found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_few_shot(dev_set[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muxObsec62dZ",
        "outputId": "7d597906-e481-45e7-d0d8-04a42b2ad93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale='${produce the emotion}. We need to analyze the text to determine the underlying emotion. The text mentions a feeling of being stuck in a rut, which suggests a lack of interest or motivation. Therefore, the emotion that best fits this text is \"boredom\".',\n",
              "    emotion='boredom'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3UwtmdK7C20",
        "outputId": "9b8cc038-b8c3-4656-eea5-93d965a47946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the fields `text`, produce the fields `emotion`.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Text: ${text}\n",
            "Reasoning: Let's think step by step in order to ${produce the emotion}. We ...\n",
            "Emotion: ${emotion}\n",
            "\n",
            "---\n",
            "\n",
            "Text: I'm really worried about my presentation tomorrow.\n",
            "Emotion: anxious\n",
            "\n",
            "---\n",
            "\n",
            "Text: I feel like I'm not good enough.\n",
            "Emotion: sad\n",
            "\n",
            "---\n",
            "\n",
            "Text: I'm so excited for my vacation next week!\n",
            "Emotion: happy\n",
            "\n",
            "---\n",
            "\n",
            "Text: I can't believe I got the job! I'm thrilled!\n",
            "Emotion: happy\n",
            "\n",
            "---\n",
            "\n",
            "Text: I feel like I'm in a rut. Nothing excites me anymore.\n",
            "Reasoning: Let's think step by step in order to\u001b[32m ${produce the emotion}. We need to analyze the text to determine the underlying emotion. The text mentions a feeling of being stuck in a rut, which suggests a lack of interest or motivation. Therefore, the emotion that best fits this text is \"boredom\". \n",
            "Emotion: boredom\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading a program\n",
        "\n",
        "```\n",
        "loaded_program = YOUR_PROGRAM_CLASS()\n",
        "loaded_program.load(path=YOUR_SAVE_PATH)\n",
        "```"
      ],
      "metadata": {
        "id": "OHsivJmg7Mf1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIjVWtHS7Fwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typed Predictors\n"
      ],
      "metadata": {
        "id": "joQsMgMU7xQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Input(BaseModel):\n",
        "    context: str = Field(description=\"The context for the question\")\n",
        "    query: str = Field(description=\"The question to be answered\")\n",
        "\n",
        "class Output(BaseModel):\n",
        "    answer: str = Field(description=\"The answer for the question\")\n",
        "    confidence: float = Field(ge=0, le=1, description=\"The confidence score for the answer\")"
      ],
      "metadata": {
        "id": "022OvRq99bTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QASignature(dspy.Signature):\n",
        "    \"\"\"Answer the question based on the context and query provided, and on the scale of 10 tell how confident you are about the answer.\"\"\"\n",
        "\n",
        "    input: Input = dspy.InputField()\n",
        "    output: Output = dspy.OutputField()"
      ],
      "metadata": {
        "id": "bpMu6PyB9cqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = dspy.TypedPredictor(QASignature)"
      ],
      "metadata": {
        "id": "OE05o8iD9pOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_query_pair = Input(\n",
        "    context=\"The quick brown fox jumps over the lazy dog\",\n",
        "    query=\"What does the fox jumps over?\",\n",
        ")\n",
        "\n",
        "prediction = predictor(input=doc_query_pair)"
      ],
      "metadata": {
        "id": "_yHrJbIO9rhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jZ-de5V9tbf",
        "outputId": "7e427dbc-5006-407b-f115-cfadce36ff54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    output=Output(answer='The fox jumps over the lazy dog.', confidence=0.95)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = prediction.output.answer\n",
        "confidence_score = prediction.output.confidence\n",
        "\n",
        "print(f\"Prediction: {prediction}\\n\\n\")\n",
        "print(f\"Answer: {answer}, Answer Type: {type(answer)}\")\n",
        "print(f\"Confidence Score: {confidence_score}, Confidence Score Type: {type(confidence_score)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkdB_LXi9uMt",
        "outputId": "d1241e10-9370-48d2-b994-fa0f3263aded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Prediction(\n",
            "    output=Output(answer='The fox jumps over the lazy dog.', confidence=0.95)\n",
            ")\n",
            "\n",
            "\n",
            "Answer: The fox jumps over the lazy dog., Answer Type: <class 'str'>\n",
            "Confidence Score: 0.95, Confidence Score Type: <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OKpTbzG9zwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assertions/Suggestions\n",
        "\n",
        "https://arxiv.org/abs/2312.13382\n",
        "\n",
        "Suggestions will not fail if the condition is not met, it will continue to update the prompt to add refinements up to a specified retry count. After this retry amount, it will move onto the next module.\n",
        "\n",
        "Assertions will stop when the condition is violated."
      ],
      "metadata": {
        "id": "oj-xLb5wDpZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/dCKOJ82.png)"
      ],
      "metadata": {
        "id": "aeuz7FneGsv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/SMh3v9L.png)"
      ],
      "metadata": {
        "id": "Ls6C1n4mGu6B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILIkvhl9DtIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}