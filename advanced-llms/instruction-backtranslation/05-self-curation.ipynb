{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Self-Curation\n","\n","This notebook presents the advanced self-curation process for selecting high-quality instruction-output pairs. It includes criteria for quality assessment, filtering techniques, and the selection mechanism to ensure only the best pairs are used for fine-tuning the base model.\n","\n","## Steps\n","1. Define the criteria for assessing the quality of instruction-output pairs.\n","2. Implement filtering techniques to evaluate and select high-quality pairs.\n","3. Save the curated instruction-output pairs for fine-tuning."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install Pytorch & other libraries\n","!pip install \"torch==2.1.2\" tensorboard\n","\n","# Install Hugging Face libraries\n","!pip install  --upgrade \\\n","  \"transformers==4.36.2\" \\\n","  \"datasets==2.16.1\" \\\n","  \"accelerate==0.26.1\" \\\n","  \"evaluate==0.4.1\" \\\n","  \"bitsandbytes==0.42.0\" \\\n","  \"trl==0.7.10\"  \\\n","  \"peft==0.7.1\" \\\n","\n","# install peft & trl from github\n","!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n","!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from huggingface_hub import login\n","import os\n"," \n","login(\n","  token=\"\", # ADD YOUR TOKEN HERE\n","  add_to_git_credential=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","\n","# Curated examples form the LIMA dataset which have responses for selected prompts\n","data_path = \"\" \n"," \n","# Load our test dataset\n","eval_dataset = load_dataset(\"json\", data_files=data_path,  split=\"train\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load generated responses \n","import json\n","\n","generated_responses_path = \"/kaggle/input/generated-responses-from-backward-model/generated_responses.json\"\n","\n","with open(generated_responses_path, 'r') as gen_res:\n","    generated_responses = json.load(gen_res)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading model for curation of responses with respect to instructions\n","\n","import torch\n","from peft import AutoPeftModelForCausalLM\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","\n","# The model that you want to use from the Hugging Face hub\n","model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","# Quantization configuration\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","# Loads model from hugging face and device mapping\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map,\n","#     use_auth_token=os.getenv('HUGGINGFACE_TOKEN')\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","# This tokenizer is passed into the model for splitting the input data into chunks\n","# This is also obtained from hugging face\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_auth_token=os.getenv('HUGGINGFACE_TOKEN'))\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["system_prompt= \"\"\"Below is an instruction from an user and a candidate answer. Evaluate whether or\n","not the answer is a good example of how AI Assistant should respond to the user’s\n","instruction.  Please assign a score using the following 5-point scale:1:  It means the answer is incomplete, vague, off-topic, controversial, or not\n","exactly what the user asked for.  For example, some content seems missing, numbered\n","list does not start from the beginning, the opening sentence repeats user’s question.\n","Or the response is from another person’s perspective with their personal experience\n","(e.g.  taken from blog posts), or looks like an answer from a forum.  Or it contains\n","promotional text, navigation text, or other irrelevant information.2:  It means the answer addresses most of the asks from the user.  It does not\n","directly address the user’s question.  For example, it only provides a high-level\n","methodology instead of the exact solution to user’s question.3:  It means the answer is helpful but not written by an AI Assistant.  It addresses\n","all the basic asks from the user.  It is complete and self contained with the\n","drawback that the response is not written from an AI assistant’s perspective, but\n","from other people’s perspective.  The content looks like an excerpt from a blog post,\n","web page, or web search results.  For example, it contains personal experience or\n","opinion, mentions comments section, or share on social media, etc.4:  It means the answer is written from an AI assistant’s perspective with a\n","clear focus of addressing the instruction.  It provide a complete, clear, and\n","comprehensive response to user’s question or instruction without missing or\n","irrelevant information.  It is well organized, self-contained, and written in a\n","helpful tone.  It has minor room for improvement, e.g.  more concise and focused.5:  It means it is a perfect answer from an AI Assistant.  It has a clear focus on\n","being a helpful AI Assistant, where the response looks like intentionally written\n","to address the user’s question or instruction without any irrelevant sentences.  The\n","answer provides high quality content, demonstrating expert knowledge in the area, is\n","very well written, logical, easy-to-follow, engaging and insightful.\n","Please first provide the score in the format \"Score:  <rating>\" and then provide a brief reasoning you used to derive the rating score in the last line.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["few_shot_chat_format = [\n","{\n","    \"role\": \"user\",\n","    \"content\": \"<generated instruction>Explain the impact of blockchain technology on the financial industry.</generated instruction>\\n<output>Blockchain is a major trend in tech that enhances data security. It's used in various sectors beyond finance, like healthcare and logistics.</output>\"\n","},\n","{\n","    \"role\": \"assistant\",\n","    \"content\": \"Score: 1\\nReasoning: The response is off-topic and vague regarding the specific impact on the financial industry. It generalizes the use of blockchain across various sectors without directly addressing the financial industry.\"\n","},\n","{\n","    \"role\": \"user\",\n","    \"content\": \"<generated instruction>List the steps involved in the scientific method.</generated instruction>\\n<output>The scientific method involves forming a hypothesis and then testing it through experiments to see if it's true.</output>\"\n","},\n","{\n","    \"role\": \"assistant\",\n","    \"content\": \"Score: 2\\nReasoning: The answer addresses the basic asks of the instruction but is incomplete and does not provide a detailed list of all the steps involved in the scientific method.\"\n","},\n","{\n","    \"role\": \"user\",\n","    \"content\": \"<generated instruction>Describe how to prepare a simple pasta dish.</generated instruction>\\n<output>To make a pasta dish, you need pasta, a sauce, and optional ingredients like meat or vegetables. Cook the pasta, prepare the sauce, combine them, and serve.</output>\"\n","},\n","{\n","    \"role\": \"assistant\",\n","    \"content\": \"Score: 3\\nReasoning: The response is helpful and addresses all the basic asks from the user. It is complete and self-contained but lacks details that would make it more engaging and thorough.\"\n","},\n","{\n","    \"role\": \"user\",\n","    \"content\": \"<generated instruction>Discuss the role of antioxidants in maintaining health.</generated instruction>\\n<output>Antioxidants play a critical role in protecting the body against free radicals, which can cause oxidative stress leading to cell damage and various diseases. Foods rich in antioxidants can help maintain overall health and prevent diseases related to aging and lifestyle.</output>\"\n","},\n","{\n","    \"role\": \"assistant\",\n","    \"content\": \"Score: 4\\nReasoning: The answer is well-written from an AI assistant’s perspective with a clear focus on addressing the instruction. It provides a complete and comprehensive response without irrelevant information but could include more specific examples of antioxidant-rich foods for an even clearer guide.\"\n","},\n","{\n","    \"role\": \"user\",\n","    \"content\": \"<generated instruction>Provide a detailed explanation of the process of evaporation in the water cycle.</generated instruction>\\n<output>Evaporation is a key part of the water cycle where water from oceans, lakes, and rivers turns into vapor. Heat from the sun causes the water to heat up and rise into the atmosphere. This process helps form clouds and is essential for rainfall and the continuation of the water cycle.</output>\"\n","},\n","{\n","    \"role\": \"assistant\",\n","    \"content\": \"Score: 5\\nReasoning: The response is a perfect answer from an AI Assistant. It has a clear focus on being a helpful AI Assistant, where the response looks intentionally written to address the user’s question or instruction without any irrelevant sentences. The answer provides high-quality content, demonstrating expert knowledge in the area, is very well written, logical, easy-to-follow, engaging and insightful.\"\n","}\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to generate prompts\n","\n","def generate_prompts(start, end):\n","    prompts = []\n","\n","    for i in range(start,end):\n","        question=f\"\"\"<generated instruction>{generated_responses[i]}</generated instruction>\\n<output>{eval_dataset[i][\"data\"][1]['content']}</output>\"\"\"\n","\n","        \n","        # conversation=[{\"role\": \"system\", \"content\": system_prompt}] + few_shot_chat_format + [{\"role\": \"user\", \"content\" : question}]\n","        conversation=[{\"role\": \"user\", \"content\" : system_prompt + \"\\n\\n\" + question}]\n","        prompts.append(conversation)\n","        \n","    return prompts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Call function to generate prompts\n","\n","prompts = generate_prompts(0, len(generated_responses))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_scores = []\n","\n","for p in prompts:\n","    prompt = pipe.tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)\n","    outputs = pipe(prompt, max_new_tokens=512, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","    output_scores.append(outputs[0]['generated_text'][len(prompt):])\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","\n","def extract_first_digit(input_string: str):\n","    # Use regular expression to find the first occurrence of one or more digits\n","    match = re.search(r\"\\d+\", input_string)\n","    if match:\n","        return match.group(0)  # Return the first found group of digits\n","    else:\n","        return None  # Return None if no digits are found\n","\n","    \n","def extract(output: str):\n","    \n","    parts = output.split(\"\\n\\n\")\n","    \n","    score_part = parts[0]\n","    reason = parts[1].join(\"\\n\\n\")\n","    \n","    score = extract_first_digit(score_part)\n","    \n","    return score, reason"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["score_reason = []\n","\n","for i in range(len(output_scores)):\n","    output_score = output_scores[i]\n","    extracted_values = extract(output_score)\n","    \n","    score_reason.append((generated_responses[i], eval_dataset[i][\"data\"][1]['content']) + extracted_values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","\n","# Convert list of lists to list of dictionaries to enhance JSON structure\n","json_data = [\n","    {\"instruction\": entry[0], \"response\": entry[1], \"score\": entry[2], \"reason\": entry[3]}\n","    for entry in score_reason\n","]\n","\n","# Convert to JSON string\n","json_string = json.dumps(json_data, indent=4)\n","print(json_string)  # To see the output on the console\n","\n","# Save the JSON to a file\n","with open('self_curated.json', 'w') as f:\n","    json.dump(json_data, f, indent=4)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.12.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"vscode":{"interpreter":{"hash":"7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"}}},"nbformat":4,"nbformat_minor":4}
