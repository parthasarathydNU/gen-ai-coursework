{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install Pytorch & other libraries\n!pip install \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  --upgrade \\\n  \"transformers==4.36.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  \"trl==0.7.10\"  \\\n  \"peft==0.7.1\" \\\n\n# install peft & trl from github\n!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom huggingface_hub import login\nimport os\n \nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=False\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Curated examples form the LIMA dataset which have responses for selected prompts\ndata_path = \"\" \n \n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=data_path,  split=\"train\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load generated responses \nimport json\n\ngenerated_responses_path = \"/kaggle/input/generated-responses-from-backward-model/generated_responses.json\"\n\nwith open(generated_responses_path, 'r') as gen_res:\n    generated_responses = json.load(gen_res)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading model for curation of responses with respect to instructions\n\nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\n\n# The model that you want to use from the Hugging Face hub\nmodel_name = \"meta-llama/Llama-2-7b-chat-hf\"\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\n# Quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\n# Loads model from hugging face and device mapping\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map,\n#     use_auth_token=os.getenv('HUGGINGFACE_TOKEN')\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\n# This tokenizer is passed into the model for splitting the input data into chunks\n# This is also obtained from hugging face\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_auth_token=os.getenv('HUGGINGFACE_TOKEN'))\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"system_prompt= \"\"\"Below is an instruction from an user and a candidate answer. Evaluate whether or\nnot the answer is a good example of how AI Assistant should respond to the user’s\ninstruction.  Please assign a score using the following 5-point scale:1:  It means the answer is incomplete, vague, off-topic, controversial, or not\nexactly what the user asked for.  For example, some content seems missing, numbered\nlist does not start from the beginning, the opening sentence repeats user’s question.\nOr the response is from another person’s perspective with their personal experience\n(e.g.  taken from blog posts), or looks like an answer from a forum.  Or it contains\npromotional text, navigation text, or other irrelevant information.2:  It means the answer addresses most of the asks from the user.  It does not\ndirectly address the user’s question.  For example, it only provides a high-level\nmethodology instead of the exact solution to user’s question.3:  It means the answer is helpful but not written by an AI Assistant.  It addresses\nall the basic asks from the user.  It is complete and self contained with the\ndrawback that the response is not written from an AI assistant’s perspective, but\nfrom other people’s perspective.  The content looks like an excerpt from a blog post,\nweb page, or web search results.  For example, it contains personal experience or\nopinion, mentions comments section, or share on social media, etc.4:  It means the answer is written from an AI assistant’s perspective with a\nclear focus of addressing the instruction.  It provide a complete, clear, and\ncomprehensive response to user’s question or instruction without missing or\nirrelevant information.  It is well organized, self-contained, and written in a\nhelpful tone.  It has minor room for improvement, e.g.  more concise and focused.5:  It means it is a perfect answer from an AI Assistant.  It has a clear focus on\nbeing a helpful AI Assistant, where the response looks like intentionally written\nto address the user’s question or instruction without any irrelevant sentences.  The\nanswer provides high quality content, demonstrating expert knowledge in the area, is\nvery well written, logical, easy-to-follow, engaging and insightful.\nPlease first provide the score in the format \"Score:  <rating>\" and then provide a brief reasoning you used to derive the rating score in the last line.\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"few_shot_chat_format = [\n{\n    \"role\": \"user\",\n    \"content\": \"<generated instruction>Explain the impact of blockchain technology on the financial industry.</generated instruction>\\n<output>Blockchain is a major trend in tech that enhances data security. It's used in various sectors beyond finance, like healthcare and logistics.</output>\"\n},\n{\n    \"role\": \"assistant\",\n    \"content\": \"Score: 1\\nReasoning: The response is off-topic and vague regarding the specific impact on the financial industry. It generalizes the use of blockchain across various sectors without directly addressing the financial industry.\"\n},\n{\n    \"role\": \"user\",\n    \"content\": \"<generated instruction>List the steps involved in the scientific method.</generated instruction>\\n<output>The scientific method involves forming a hypothesis and then testing it through experiments to see if it's true.</output>\"\n},\n{\n    \"role\": \"assistant\",\n    \"content\": \"Score: 2\\nReasoning: The answer addresses the basic asks of the instruction but is incomplete and does not provide a detailed list of all the steps involved in the scientific method.\"\n},\n{\n    \"role\": \"user\",\n    \"content\": \"<generated instruction>Describe how to prepare a simple pasta dish.</generated instruction>\\n<output>To make a pasta dish, you need pasta, a sauce, and optional ingredients like meat or vegetables. Cook the pasta, prepare the sauce, combine them, and serve.</output>\"\n},\n{\n    \"role\": \"assistant\",\n    \"content\": \"Score: 3\\nReasoning: The response is helpful and addresses all the basic asks from the user. It is complete and self-contained but lacks details that would make it more engaging and thorough.\"\n},\n{\n    \"role\": \"user\",\n    \"content\": \"<generated instruction>Discuss the role of antioxidants in maintaining health.</generated instruction>\\n<output>Antioxidants play a critical role in protecting the body against free radicals, which can cause oxidative stress leading to cell damage and various diseases. Foods rich in antioxidants can help maintain overall health and prevent diseases related to aging and lifestyle.</output>\"\n},\n{\n    \"role\": \"assistant\",\n    \"content\": \"Score: 4\\nReasoning: The answer is well-written from an AI assistant’s perspective with a clear focus on addressing the instruction. It provides a complete and comprehensive response without irrelevant information but could include more specific examples of antioxidant-rich foods for an even clearer guide.\"\n},\n{\n    \"role\": \"user\",\n    \"content\": \"<generated instruction>Provide a detailed explanation of the process of evaporation in the water cycle.</generated instruction>\\n<output>Evaporation is a key part of the water cycle where water from oceans, lakes, and rivers turns into vapor. Heat from the sun causes the water to heat up and rise into the atmosphere. This process helps form clouds and is essential for rainfall and the continuation of the water cycle.</output>\"\n},\n{\n    \"role\": \"assistant\",\n    \"content\": \"Score: 5\\nReasoning: The response is a perfect answer from an AI Assistant. It has a clear focus on being a helpful AI Assistant, where the response looks intentionally written to address the user’s question or instruction without any irrelevant sentences. The answer provides high-quality content, demonstrating expert knowledge in the area, is very well written, logical, easy-to-follow, engaging and insightful.\"\n}\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate prompts\n\ndef generate_prompts(start, end):\n    prompts = []\n\n    for i in range(start,end):\n        question=f\"\"\"<generated instruction>{generated_responses[i]}</generated instruction>\\n<output>{eval_dataset[i][\"data\"][1]['content']}</output>\"\"\"\n\n        \n        # conversation=[{\"role\": \"system\", \"content\": system_prompt}] + few_shot_chat_format + [{\"role\": \"user\", \"content\" : question}]\n        conversation=[{\"role\": \"user\", \"content\" : system_prompt + \"\\n\\n\" + question}]\n        prompts.append(conversation)\n        \n    return prompts","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call function to generate prompts\n\nprompts = generate_prompts(0, len(generated_responses))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_scores = []\n\nfor p in prompts:\n    prompt = pipe.tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=512, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n    output_scores.append(outputs[0]['generated_text'][len(prompt):])\n    torch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef extract_first_digit(input_string: str):\n    # Use regular expression to find the first occurrence of one or more digits\n    match = re.search(r\"\\d+\", input_string)\n    if match:\n        return match.group(0)  # Return the first found group of digits\n    else:\n        return None  # Return None if no digits are found\n\n    \ndef extract(output: str):\n    \n    parts = output.split(\"\\n\\n\")\n    \n    score_part = parts[0]\n    reason = parts[1].join(\"\\n\\n\")\n    \n    score = extract_first_digit(score_part)\n    \n    return score, reason","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_reason = []\n\nfor i in range(len(output_scores)):\n    output_score = output_scores[i]\n    extracted_values = extract(output_score)\n    \n    score_reason.append((generated_responses[i], eval_dataset[i][\"data\"][1]['content']) + extracted_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Convert list of lists to list of dictionaries to enhance JSON structure\njson_data = [\n    {\"instruction\": entry[0], \"response\": entry[1], \"score\": entry[2], \"reason\": entry[3]}\n    for entry in score_reason\n]\n\n# Convert to JSON string\njson_string = json.dumps(json_data, indent=4)\nprint(json_string)  # To see the output on the console\n\n# Save the JSON to a file\nwith open('self_curated.json', 'w') as f:\n    json.dump(json_data, f, indent=4)\n","metadata":{},"execution_count":null,"outputs":[]}]}