{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install Pytorch & other libraries\n!pip install \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  --upgrade \\\n  \"transformers==4.36.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  \"trl==0.7.10\"  \\\n  \"peft==0.7.1\" \\\n\n# install peft & trl from github\n!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = \"path_of_backwards_model\"\ndata_path = \"path_of_conversations_cureated_in_previous_notebook\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom huggingface_hub import login\nimport os\n \nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=False\n)\n ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the Fine-Tuned Model for Inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n \npeft_model_id = model_path\n# peft_model_id = args.output_dir\n \n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\ntokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n# load into pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on test samples","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randint\n\n \n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=data_path,  split=\"train\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_entry(idx:int, max_tokens = 100):\n    prompt = pipe.tokenizer.apply_chat_template(\n        eval_dataset[idx][\"data\"][:2], \n        tokenize=False, add_generation_prompt=True)\n    \n    outputs = pipe(prompt, \n                   max_new_tokens=max_tokens, \n                   do_sample=False, temperature=0.1, \n                   top_k=50, top_p=0.1, \n                   eos_token_id=pipe.tokenizer.eos_token_id, \n                   pad_token_id=pipe.tokenizer.pad_token_id)\n    \n    length = len(prompt)\n    content = outputs[0]['generated_text'][length:].strip()\n    del prompt\n    del outputs\n    \n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Collect garbage\n    gc.collect()\n    \n    return content","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_responses = []\n\nfor i in range(0, len(eval_dataset)):\n    generated_responses.append(eval_entry(i))\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Collect garbage\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]}]}