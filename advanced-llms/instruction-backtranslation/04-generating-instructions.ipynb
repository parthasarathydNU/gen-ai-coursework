{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Generating Instructions\n","\n","This notebook describes the methodology for generating instruction prompts using the trained backward model. It outlines the process of generating instructions, handling the outputs, and performing a preliminary evaluation of the instruction quality.\n","\n","## Steps\n","1. Load the trained backward model and the prepared web corpus.\n","2. Generate instruction prompts for each entry in the web corpus.\n","3. Collect and save the generated instructions for self-curation."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Install Pytorch & other libraries\n","!pip install \"torch==2.1.2\" tensorboard\n","\n","# Install Hugging Face libraries\n","!pip install  --upgrade \\\n","  \"transformers==4.36.2\" \\\n","  \"datasets==2.16.1\" \\\n","  \"accelerate==0.26.1\" \\\n","  \"evaluate==0.4.1\" \\\n","  \"bitsandbytes==0.42.0\" \\\n","  \"trl==0.7.10\"  \\\n","  \"peft==0.7.1\" \\\n","\n","# install peft & trl from github\n","!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n","!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_path = \"path_of_backwards_model\"\n","data_path = \"path_of_conversations_cureated_in_previous_notebook\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from huggingface_hub import login\n","import os\n"," \n","login(\n","  token=\"\", # ADD YOUR TOKEN HERE\n","  add_to_git_credential=False\n",")\n"," "]},{"cell_type":"markdown","metadata":{},"source":["## Load the Fine-Tuned Model for Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, pipeline\n"," \n","peft_model_id = model_path\n","# peft_model_id = args.output_dir\n"," \n","# Load Model with PEFT adapter\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","  peft_model_id,\n","  device_map=\"auto\",\n","  torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n","# load into pipeline\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Run Inference on test samples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from random import randint\n","\n"," \n","# Load our test dataset\n","eval_dataset = load_dataset(\"json\", data_files=data_path,  split=\"train\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def eval_entry(idx:int, max_tokens = 100):\n","    prompt = pipe.tokenizer.apply_chat_template(\n","        eval_dataset[idx][\"data\"][:2], \n","        tokenize=False, add_generation_prompt=True)\n","    \n","    outputs = pipe(prompt, \n","                   max_new_tokens=max_tokens, \n","                   do_sample=False, temperature=0.1, \n","                   top_k=50, top_p=0.1, \n","                   eos_token_id=pipe.tokenizer.eos_token_id, \n","                   pad_token_id=pipe.tokenizer.pad_token_id)\n","    \n","    length = len(prompt)\n","    content = outputs[0]['generated_text'][length:].strip()\n","    del prompt\n","    del outputs\n","    \n","    # Clear GPU cache\n","    torch.cuda.empty_cache()\n","\n","    # Collect garbage\n","    gc.collect()\n","    \n","    return content"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generated_responses = []\n","\n","for i in range(0, len(eval_dataset)):\n","    generated_responses.append(eval_entry(i))\n","    # Clear GPU cache\n","    torch.cuda.empty_cache()\n","\n","    # Collect garbage\n","    gc.collect()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.12.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"vscode":{"interpreter":{"hash":"7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"}}},"nbformat":4,"nbformat_minor":4}
