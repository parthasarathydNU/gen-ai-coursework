{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathydNU/gen-ai-coursework/blob/main/sentence-similarity/notebooks/sentence-similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "479429b1-3c25-4cc5-bfdc-598e03f846fe",
      "metadata": {
        "id": "479429b1-3c25-4cc5-bfdc-598e03f846fe"
      },
      "source": [
        "# Task:\n",
        "\n",
        "Derive similarity score between two sentences applying three different techniques.\n",
        "- Reference Article: [A beginner’s guide to measuring sentence similarity](https://medium.com/@igniobydigitate/a-beginners-guide-to-measuring-sentence-similarity-f3c78b9da0bc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bb0231-ccf2-41a4-b897-cff2223b9180",
      "metadata": {
        "id": "59bb0231-ccf2-41a4-b897-cff2223b9180"
      },
      "source": [
        "## Sentence Embedding\n",
        "\n",
        "Sentence embedding represents a sentence as a vector of numbers. This numerical representation of a sentence is called sentence embedding. In a word embedding corresponds to a particular feature or aspect of the word. A sentence embedding is based on a similar concept where the dimensions collectively capture different aspects of the words used in the sentence, the grammatical structure of the sentence, and maybe some more underlying information.\n",
        "\n",
        "There are various ways in which a sentence embedding can be created. Once we have each sentence represented as a vector of numbers, then the problem of finding sentence similarity translates to the problem of finding similarity between these numeric vectors.\n",
        "\n",
        "In this notebook I will discuss a couple of statistical techniques to create numeric representations of sentences and briefly explore an idea of how one can utilize word embeddings for the same task. I will also discuss how similarity between sentence embeddings can be computed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757c4245-ab9c-4381-a71c-829edc119884",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "757c4245-ab9c-4381-a71c-829edc119884"
      },
      "source": [
        "## Sample sentences\n",
        "\n",
        "We take sentences form two unrelated movies to work with. This is done with the goal to demonstrate how sentences turn out to be either similar or dissimilar across these movies. I expect sentences from spiderman to show higher similarity with other senteces from the same movie and a lower similarity score from the sentences from the movie Godfather."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b272645-5ead-4af5-a12e-0463f88279ad",
      "metadata": {
        "id": "1b272645-5ead-4af5-a12e-0463f88279ad"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ec961c-ccc6-4320-9c0b-d49845d31955",
      "metadata": {
        "id": "85ec961c-ccc6-4320-9c0b-d49845d31955"
      },
      "outputs": [],
      "source": [
        "# Sample sentences from Spider-Man\n",
        "spiderman_sentences = [\n",
        "    \"With great power comes great responsibility.\",\n",
        "    \"I missed the part where that's my problem.\",\n",
        "    \"You're not Superman, you know.\",\n",
        "    \"Remember, with great power comes great responsibility.\",\n",
        "    \"I'm just Peter Parker. I'm Spider-Man no more.\",\n",
        "    \"Whatever life holds in store for me, I will never forget these words.\",\n",
        "    \"The truth is, I am Spider-Man.\",\n",
        "    \"This is my gift, my curse. Who am I? I'm Spider-Man.\",\n",
        "    \"Sometimes, to do what's right, we have to be steady and give up the things we want the most.\",\n",
        "    \"I want to tell you the truth... here it is: I'm Spider-Man.\"\n",
        "]\n",
        "\n",
        "# Sample sentences from The Godfather\n",
        "godfather_sentences = [\n",
        "    \"I'm gonna make him an offer he can't refuse.\",\n",
        "    \"Revenge is a dish best served cold.\",\n",
        "    \"A man who doesn't spend time with his family can never be a real man.\",\n",
        "    \"Leave the gun. Take the cannoli.\",\n",
        "    \"The lawyer with the briefcase can steal more money than the man with the gun.\",\n",
        "    \"It's not personal, Sonny. It's strictly business.\",\n",
        "    \"Women and children can be careless, but not men.\",\n",
        "    \"Power wears out those who do not have it.\",\n",
        "    \"Friendship is everything. Friendship is more than talent. It is more than the government. It is almost the equal of family.\",\n",
        "    \"Great men are not born great, they grow great.\"\n",
        "]\n",
        "\n",
        "all_sentences = spiderman_sentences + godfather_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e72365-8f84-45d6-a1ce-23be3f47a47a",
      "metadata": {
        "id": "35e72365-8f84-45d6-a1ce-23be3f47a47a"
      },
      "source": [
        "## Bag of words\n",
        "\n",
        "The basic idea is to find out which words are present in a sentence and assess the importance of a word based on how many times it occurs in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eeec257-9c01-4195-9b18-cc0a7a339128",
      "metadata": {
        "id": "1eeec257-9c01-4195-9b18-cc0a7a339128"
      },
      "source": [
        "#### Creating a dictionary and removing stop words\n",
        "\n",
        "Words such as is, are, a, an, the etc do not add much value in terms of providing context to a sentence. These are called stop words. So before we go ahead and count the frequency of words, we want to remove these stop words from the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f110552-8b1b-4857-9ca9-9bf44a2ffbc5",
      "metadata": {
        "id": "5f110552-8b1b-4857-9ca9-9bf44a2ffbc5"
      },
      "outputs": [],
      "source": [
        "# Define a list of stop words\n",
        "stop_words = {\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
        "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n",
        "    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
        "    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
        "    'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
        "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
        "    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
        "    'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad141e9f-da0c-48ed-af90-a0ffc34d0c0f",
      "metadata": {
        "id": "ad141e9f-da0c-48ed-af90-a0ffc34d0c0f"
      },
      "source": [
        "#### Removing punctuations\n",
        "Before we go ahead and remove stop words, we first want to remove punctuations from the words so that they also get flagged as stop words. And even if not we don't want to consider men and men. as different words. So we remove punctuations.\n",
        "\n",
        "Explanation of code below:\n",
        "\n",
        "- str.maketrans('', '', string.punctuation) creates a translation table that maps each character in string.punctuation to None.\n",
        "- str.maketrans is a static method that returns a translation table usable for str.translate.\n",
        "- The first two arguments are empty strings ('') because we are not replacing any characters, only removing.\n",
        "- The third argument is string.punctuation, which contains all punctuation characters.\n",
        "- word.translate(...) uses the translation table to remove all punctuation characters from the word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3206f6-23ee-401c-abff-e4a7673920ea",
      "metadata": {
        "id": "eb3206f6-23ee-401c-abff-e4a7673920ea"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def remove_punctuation(word):\n",
        "    return word.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c386e66-a1fe-44fa-98df-ce2bbd1b8fa8",
      "metadata": {
        "id": "0c386e66-a1fe-44fa-98df-ce2bbd1b8fa8",
        "outputId": "a9e5d3e9-3d66-461a-c706-b79615ee8caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence: This is an example sentence showing the removal of stop words.\n",
            "Filtered sentence: example sentence showing removal stop words\n"
          ]
        }
      ],
      "source": [
        "# Sample sentence\n",
        "sentence = \"This is an example sentence showing the removal of stop words.\"\n",
        "\n",
        "def remove_stop_words(sentence):\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_words = [remove_punctuation(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a sentence\n",
        "    filtered_sentence = ' '.join(filtered_words)\n",
        "\n",
        "    return filtered_sentence\n",
        "\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"Filtered sentence:\", remove_stop_words(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefa7986-2909-4f5c-a8f7-9cc069139005",
      "metadata": {
        "id": "eefa7986-2909-4f5c-a8f7-9cc069139005",
        "outputId": "0643c3e5-ce6c-4d3e-c708-3912a377c598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spiderman_original:\n",
            "With great power comes great responsibility.\n",
            "I missed the part where that's my problem.\n",
            "You're not Superman, you know.\n",
            "\n",
            "spiderman_stop_removed:\n",
            "['great', 'power', 'comes', 'great', 'responsibility']\n",
            "['missed', 'part', 'thats', 'problem']\n",
            "['youre', 'superman', 'know']\n"
          ]
        }
      ],
      "source": [
        "# Removing stopwords from all sentences in our database\n",
        "\n",
        "spiderman_stop_removed = []\n",
        "godfather_stop_removed = []\n",
        "\n",
        "for sentence in spiderman_sentences:\n",
        "    spiderman_stop_removed.append(remove_stop_words(sentence).split())\n",
        "\n",
        "for sentence in godfather_sentences:\n",
        "    godfather_stop_removed.append(remove_stop_words(sentence).split())\n",
        "\n",
        "print(\"spiderman_original:\")\n",
        "print(spiderman_sentences[0])\n",
        "print(spiderman_sentences[1])\n",
        "print(spiderman_sentences[2])\n",
        "print()\n",
        "print(\"spiderman_stop_removed:\")\n",
        "print(spiderman_stop_removed[0])\n",
        "print(spiderman_stop_removed[1])\n",
        "print(spiderman_stop_removed[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa72ace-4b7f-46fd-8c6a-fc7f0d30c4e4",
      "metadata": {
        "id": "1aa72ace-4b7f-46fd-8c6a-fc7f0d30c4e4"
      },
      "source": [
        "#### Creating a dictionary of words in all sentences\n",
        "\n",
        "Now that we have removed the stop words, let's create a dictionary of all non stop words and create a dataset where for each sentence we have a row of frequency of each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a54e91ca-6805-4562-b7f8-d61de830a280",
      "metadata": {
        "id": "a54e91ca-6805-4562-b7f8-d61de830a280",
        "outputId": "59965a04-af63-4c9a-a6b5-3f5d9354cba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique words : 82\n"
          ]
        }
      ],
      "source": [
        "# Creating a set of all unique words\n",
        "\n",
        "# Let's combine all arrays into one\n",
        "all_sentences_stop_removed = spiderman_stop_removed+ godfather_stop_removed\n",
        "\n",
        "# get unique words from all sentences and put it into a set\n",
        "unique_words = set()\n",
        "for sentence in all_sentences_stop_removed:\n",
        "    for word in sentence:\n",
        "        unique_words.add(word)\n",
        "\n",
        "print(f\"Total unique words : {len(unique_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b521c8-8a46-4e8b-bc14-474d6261a71a",
      "metadata": {
        "id": "19b521c8-8a46-4e8b-bc14-474d6261a71a"
      },
      "source": [
        "#### Creating the dataframe that shows the word count of each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7387cf1f-0d41-484f-9110-e2c4c9aa0957",
      "metadata": {
        "id": "7387cf1f-0d41-484f-9110-e2c4c9aa0957"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c0d444-e06c-45d5-9edd-9b66f94ac1fa",
      "metadata": {
        "id": "06c0d444-e06c-45d5-9edd-9b66f94ac1fa",
        "outputId": "a9984247-2293-4892-8ff6-81625fed3ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    remember  equal  know  superman  man  lawyer  money  real  cannoli  im  \\\n",
            "0          0      0     0         0    0       0      0     0        0   0   \n",
            "1          0      0     0         0    0       0      0     0        0   0   \n",
            "2          0      0     1         1    0       0      0     0        0   0   \n",
            "3          1      0     0         0    0       0      0     0        0   0   \n",
            "4          0      0     0         0    0       0      0     0        0   2   \n",
            "5          0      0     0         0    0       0      0     0        0   0   \n",
            "6          0      0     0         0    0       0      0     0        0   0   \n",
            "7          0      0     0         0    0       0      0     0        0   1   \n",
            "8          0      0     0         0    0       0      0     0        0   0   \n",
            "9          0      0     0         0    0       0      0     0        0   1   \n",
            "10         0      0     0         0    0       0      0     0        0   1   \n",
            "11         0      0     0         0    0       0      0     0        0   0   \n",
            "12         0      0     0         0    2       0      0     1        0   0   \n",
            "13         0      0     0         0    0       0      0     0        1   0   \n",
            "14         0      0     0         0    1       1      1     0        0   0   \n",
            "15         0      0     0         0    0       0      0     0        0   0   \n",
            "16         0      0     0         0    0       0      0     0        0   0   \n",
            "17         0      0     0         0    0       0      0     0        0   0   \n",
            "18         0      1     0         0    0       0      0     0        0   0   \n",
            "19         0      0     0         0    0       0      0     0        0   0   \n",
            "\n",
            "    ...  whats  make  spend  strictly  peter  wears  gonna  right  women  dish  \n",
            "0   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "1   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "2   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "3   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "4   ...      0     0      0         0      1      0      0      0      0     0  \n",
            "5   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "6   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "7   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "8   ...      1     0      0         0      0      0      0      1      0     0  \n",
            "9   ...      0     0      0         0      0      0      0      0      0     0  \n",
            "10  ...      0     1      0         0      0      0      1      0      0     0  \n",
            "11  ...      0     0      0         0      0      0      0      0      0     1  \n",
            "12  ...      0     0      1         0      0      0      0      0      0     0  \n",
            "13  ...      0     0      0         0      0      0      0      0      0     0  \n",
            "14  ...      0     0      0         0      0      0      0      0      0     0  \n",
            "15  ...      0     0      0         1      0      0      0      0      0     0  \n",
            "16  ...      0     0      0         0      0      0      0      0      1     0  \n",
            "17  ...      0     0      0         0      0      1      0      0      0     0  \n",
            "18  ...      0     0      0         0      0      0      0      0      0     0  \n",
            "19  ...      0     0      0         0      0      0      0      0      0     0  \n",
            "\n",
            "[20 rows x 82 columns]\n"
          ]
        }
      ],
      "source": [
        "# Create a frequency matrix\n",
        "frequency_matrix = []\n",
        "\n",
        "for sentence in all_sentences_stop_removed:\n",
        "    word_count = Counter(sentence)\n",
        "    frequency_matrix.append([word_count.get(word, 0) for word in unique_words])\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(frequency_matrix, columns=list(unique_words))\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c41b13-ea8e-4304-95fd-028c73a26560",
      "metadata": {
        "id": "98c41b13-ea8e-4304-95fd-028c73a26560"
      },
      "source": [
        "#### Using cosine similarity to calculate how similar the sentences are\n",
        "\n",
        "Consider that two n-dimensional arrays are plotted as two vectors in an n-dimensional space. Cosine similarity measures the angle between these two vectors and returns a value between -1 and 1. Mathematically, given two vectors A and B, cosine similarity is calculated as follows:\n",
        "\n",
        "A. B / ( |A| |B| )\n",
        "\n",
        "where,\n",
        "\n",
        "\n",
        "- A.B = Dot product between two vectors. It is calculated by adding the product of corresponding vector values.\n",
        "\n",
        "\n",
        "- |A|, |B| = Magnitude of a vector. It is the square root of the sum of squares of all the vector values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6536aafd-8047-4dbb-8ea9-78d94b341649",
      "metadata": {
        "id": "6536aafd-8047-4dbb-8ea9-78d94b341649"
      },
      "source": [
        "#### The cosine similarity function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1593b1ce-69ba-4606-9246-823abf2942f2",
      "metadata": {
        "id": "1593b1ce-69ba-4606-9246-823abf2942f2"
      },
      "outputs": [],
      "source": [
        "def consine(vec1, vec2):\n",
        "    # Compute the dot product\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "\n",
        "    # Compute the Euclidean norm (magnitude) of each vector\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return dot_product / (norm_vec1 * norm_vec2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06bfec8f-8449-4888-9a65-53deb2e848da",
      "metadata": {
        "id": "06bfec8f-8449-4888-9a65-53deb2e848da",
        "outputId": "51ec1b83-28a0-4edd-dbd4-43e58572ec09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['With great power comes great responsibility.', \"I missed the part where that's my problem.\", \"You're not Superman, you know.\", 'Remember, with great power comes great responsibility.', \"I'm just Peter Parker. I'm Spider-Man no more.\", 'Whatever life holds in store for me, I will never forget these words.', 'The truth is, I am Spider-Man.', \"This is my gift, my curse. Who am I? I'm Spider-Man.\", \"Sometimes, to do what's right, we have to be steady and give up the things we want the most.\", \"I want to tell you the truth... here it is: I'm Spider-Man.\", \"I'm gonna make him an offer he can't refuse.\", 'Revenge is a dish best served cold.', \"A man who doesn't spend time with his family can never be a real man.\", 'Leave the gun. Take the cannoli.', 'The lawyer with the briefcase can steal more money than the man with the gun.', \"It's not personal, Sonny. It's strictly business.\", 'Women and children can be careless, but not men.', 'Power wears out those who do not have it.', 'Friendship is everything. Friendship is more than talent. It is more than the government. It is almost the equal of family.', 'Great men are not born great, they grow great.']\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "# Testing this between two sentences within the spiderman movie\n",
        "print(all_sentences)\n",
        "print(len(all_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6154c758-2a95-4ac6-a249-621fe0c8604f",
      "metadata": {
        "id": "6154c758-2a95-4ac6-a249-621fe0c8604f",
        "outputId": "83338ef6-9d66-4ab8-e35a-5f6078b5a6d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'With great power comes great responsibility.'"
            ]
          },
          "execution_count": 260,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's try the first and the 4th statement\n",
        "all_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3262f92-806b-474c-ba90-50d9c027c3a7",
      "metadata": {
        "id": "f3262f92-806b-474c-ba90-50d9c027c3a7",
        "outputId": "c41140c8-9619-47b6-87d1-a609d969f9c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Remember, with great power comes great responsibility.'"
            ]
          },
          "execution_count": 261,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_sentences[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26fd9347-801f-4d16-9362-d2222f87c1ab",
      "metadata": {
        "id": "26fd9347-801f-4d16-9362-d2222f87c1ab",
        "outputId": "e6eb6942-334c-4ff1-dd3a-66e3fe7e580c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9354143466934852"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "consine(df.loc[0], df.loc[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff241a9-0ff7-47c4-8112-93abc8e86d24",
      "metadata": {
        "id": "fff241a9-0ff7-47c4-8112-93abc8e86d24",
        "outputId": "94a5485f-4fcf-4cf6-c0be-f1fe555a0ed0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Friendship is everything. Friendship is more than talent. It is more than the government. It is almost the equal of family.'"
            ]
          },
          "execution_count": 263,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's pick a sentence that does not have power or responsibility in it\n",
        "all_sentences[18]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "393f848c-df8c-41fa-a70b-a95408d6bff4",
      "metadata": {
        "id": "393f848c-df8c-41fa-a70b-a95408d6bff4",
        "outputId": "c90ea828-ae35-4835-e65b-0551a04725b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "consine(df.loc[0], df.loc[18])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67270059-ef4c-4f47-a477-228c4bbb3ad0",
      "metadata": {
        "id": "67270059-ef4c-4f47-a477-228c4bbb3ad0"
      },
      "source": [
        "This is not similar at at all!\n",
        "\n",
        "Let's write a function that given two sentences does this process end to end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028bd49f-31f0-470e-8b64-eaeedd4206a0",
      "metadata": {
        "id": "028bd49f-31f0-470e-8b64-eaeedd4206a0"
      },
      "outputs": [],
      "source": [
        "def findSimilarityBagOfWords(sentence1, sentence2):\n",
        "    # combining sentences to an array\n",
        "    sentences = [sentence1, sentence2]\n",
        "    stop_removed = []\n",
        "\n",
        "    # removing stop words and punctuation from the words\n",
        "    for sentence in sentences:\n",
        "        stop_removed.append(remove_stop_words(sentence).split())\n",
        "\n",
        "    # getting list of unique words\n",
        "    unique_words = set()\n",
        "    for sentence in stop_removed:\n",
        "        for word in sentence:\n",
        "            unique_words.add(word)\n",
        "\n",
        "    # Create a frequency matrix\n",
        "    frequency_matrix = []\n",
        "\n",
        "    for sentence in stop_removed:\n",
        "        word_count = Counter(sentence)\n",
        "        frequency_matrix.append([word_count.get(word, 0) for word in unique_words])\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(frequency_matrix, columns=list(unique_words))\n",
        "    print(f\"Similarity score: {consine(df.loc[0], df.loc[1])}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6025dcc-9685-44b2-a124-e60a4c94b139",
      "metadata": {
        "id": "c6025dcc-9685-44b2-a124-e60a4c94b139",
        "outputId": "352e48e0-2a07-44cf-827d-4193bf5903a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score: 0.6666666666666667\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>out</th>\n",
              "      <th>dogs</th>\n",
              "      <th>let</th>\n",
              "      <th>cats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   out  dogs  let  cats\n",
              "0    1     1    1     0\n",
              "1    1     0    1     1"
            ]
          },
          "execution_count": 266,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findSimilarityBagOfWords(\"Who let the dogs out!\", \"Who let the cats out!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed66a5d1-d062-4589-9d1c-de5a6b0c7899",
      "metadata": {
        "id": "ed66a5d1-d062-4589-9d1c-de5a6b0c7899",
        "outputId": "7ef0afa1-af99-4652-ad0d-ef91586f3ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score: 0.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mia</th>\n",
              "      <th>resist</th>\n",
              "      <th>mamma</th>\n",
              "      <th>go</th>\n",
              "      <th>my</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mia  resist  mamma  go  my\n",
              "0    1       0      1   1   0\n",
              "1    0       1      0   0   1"
            ]
          },
          "execution_count": 267,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findSimilarityBagOfWords(\"Mamma mia , here we go again \", \"My my! how can I resist you ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557c752b-14e3-4f31-9357-2aa30bc832c0",
      "metadata": {
        "id": "557c752b-14e3-4f31-9357-2aa30bc832c0",
        "outputId": "e39b1672-9258-4cfc-d07e-de99237bd8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the bustling city of Metropolis, the skyline is dominated by towering skyscrapers that reach for the heavens. The streets below are a hive of activity, with people from all walks of life hurrying to and fro. The air is filled with the sounds of honking cars, distant sirens, and the constant chatter of passersby. Amidst the urban chaos, pockets of tranquility can be found in the form of small parks and green spaces, offering a brief respite from the hustle and bustle. At night, the city transforms into a sea of lights, with neon signs and street lamps illuminating the dark, creating a vibrant and lively atmosphere that never seems to sleep.\n",
            "\n",
            "In the tranquil town of Riverview, life moves at a leisurely pace. The town is known for its picturesque scenery, with rolling hills and a serene river that winds its way through the heart of the community. The streets are lined with charming houses, each with well-tended gardens bursting with colorful flowers. The sound of birds singing fills the air, and the occasional laughter of children playing can be heard in the distance. Riverview's town square is a hub of local activity, where residents gather for farmers' markets, craft fairs, and community events. As the sun sets, the town is bathed in a golden glow, and the sky is painted with hues of pink and orange, bringing a peaceful end to another day in this idyllic setting.\n"
          ]
        }
      ],
      "source": [
        "paragraph1 = \"In the bustling city of Metropolis, the skyline is dominated by towering skyscrapers that reach for the heavens. The streets below are a hive of activity, with people from all walks of life hurrying to and fro. The air is filled with the sounds of honking cars, distant sirens, and the constant chatter of passersby. Amidst the urban chaos, pockets of tranquility can be found in the form of small parks and green spaces, offering a brief respite from the hustle and bustle. At night, the city transforms into a sea of lights, with neon signs and street lamps illuminating the dark, creating a vibrant and lively atmosphere that never seems to sleep.\"\n",
        "paragraph2 = \"In the tranquil town of Riverview, life moves at a leisurely pace. The town is known for its picturesque scenery, with rolling hills and a serene river that winds its way through the heart of the community. The streets are lined with charming houses, each with well-tended gardens bursting with colorful flowers. The sound of birds singing fills the air, and the occasional laughter of children playing can be heard in the distance. Riverview's town square is a hub of local activity, where residents gather for farmers' markets, craft fairs, and community events. As the sun sets, the town is bathed in a golden glow, and the sky is painted with hues of pink and orange, bringing a peaceful end to another day in this idyllic setting.\"\n",
        "\n",
        "print(paragraph1)\n",
        "print()\n",
        "print(paragraph2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32588b65-702b-437d-8cbc-d416c95556ed",
      "metadata": {
        "id": "32588b65-702b-437d-8cbc-d416c95556ed",
        "outputId": "5a4aa5ca-e340-42d3-ae85-2b5f072613e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score: 0.05466133744605251\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>streets</th>\n",
              "      <th>markets</th>\n",
              "      <th>hustle</th>\n",
              "      <th>occasional</th>\n",
              "      <th>bringing</th>\n",
              "      <th>neon</th>\n",
              "      <th>metropolis</th>\n",
              "      <th>hub</th>\n",
              "      <th>pink</th>\n",
              "      <th>orange</th>\n",
              "      <th>...</th>\n",
              "      <th>distance</th>\n",
              "      <th>serene</th>\n",
              "      <th>bathed</th>\n",
              "      <th>community</th>\n",
              "      <th>form</th>\n",
              "      <th>scenery</th>\n",
              "      <th>activity</th>\n",
              "      <th>hues</th>\n",
              "      <th>residents</th>\n",
              "      <th>day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 123 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   streets  markets  hustle  occasional  bringing  neon  metropolis  hub  \\\n",
              "0        1        0       1           0         0     1           1    0   \n",
              "1        1        1       0           1         1     0           0    1   \n",
              "\n",
              "   pink  orange  ...  distance  serene  bathed  community  form  scenery  \\\n",
              "0     0       0  ...         0       0       0          0     1        0   \n",
              "1     1       1  ...         1       1       1          2     0        1   \n",
              "\n",
              "   activity  hues  residents  day  \n",
              "0         1     0          0    0  \n",
              "1         1     1          1    1  \n",
              "\n",
              "[2 rows x 123 columns]"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findSimilarityBagOfWords(paragraph1, paragraph2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d768cd-31e8-4ac9-9be5-e7bc5cf9240e",
      "metadata": {
        "id": "e5d768cd-31e8-4ac9-9be5-e7bc5cf9240e"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "#### Explanation of TF-IDF\n",
        "\n",
        "Term frequency - Inverse Document Frequency\n",
        "\n",
        "The bag of words approach gives equal weight to all words. However, a more sophisticated approach is the TF-IDF approach. TF-IDF stands for Term Frequency — Inverse Document Frequency. This approach is based on the rationale that the most common words are usually the least significant ones. While stop words are removed in the bag of words approach, **TF-IDF provides a more sophisticated approach to automatically give less weight to frequent words that appear in the whole corpus.**\n",
        "\n",
        "Let's break it down further:\n",
        "\n",
        "**Term Frequency** : How frequent a term appears within the given document. Can be within the same sentence, paragraph or whole text.\n",
        "\n",
        "**Inverse Document Frequency** : This is representative of how rare this word is across all the documents in the corpus.\n",
        "\n",
        "IDF is calculated by taking the logarithm of the ratio of the total number of documents and the number of documents containing the word (document frequency). The more frequently the word appears across the corpus, the lower its inverse document frequency making it less important. Similarly, the rarer the word in the corpus, the higher its inverse document frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e20525d0-6f8e-47b1-b7b0-c87dc0fd45c3",
      "metadata": {
        "id": "e20525d0-6f8e-47b1-b7b0-c87dc0fd45c3"
      },
      "source": [
        "#### What makes TF-IDF better than Bag of words ?\n",
        "\n",
        "Let's find out by trying it out.\n",
        "\n",
        "**Step 1**: Create a corpus of words, that we have already done for the bag of words.\n",
        "\n",
        "**Step 2**: Create a dicrionary of words from the corpus. Already done for bag of words `unique_words`\n",
        "\n",
        "**Step 3**: Creating the word embeddings. This is where the difference comes in between bag of words and TF-IDF. Instead of just sticking to the term frequency, we also calculate the Inverse Document Frequency and multiple it with the TF to get the TF-IDF value for the word in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01cfd304-e753-4361-9e2c-98d9a70aaca7",
      "metadata": {
        "id": "01cfd304-e753-4361-9e2c-98d9a70aaca7",
        "outputId": "51bd9554-54ca-4c43-9de8-f740b16c5d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    remember     equal      know  superman       man    lawyer     money  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2   0.000000  0.000000  2.995732  2.995732  0.000000  0.000000  0.000000   \n",
            "3   2.995732  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "12  0.000000  0.000000  0.000000  0.000000  4.605170  0.000000  0.000000   \n",
            "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "14  0.000000  0.000000  0.000000  0.000000  2.302585  2.995732  2.995732   \n",
            "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "18  0.000000  2.995732  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "        real   cannoli        im  ...     whats      make     spend  strictly  \\\n",
            "0   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "1   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "4   0.000000  0.000000  3.218876  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "7   0.000000  0.000000  1.609438  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "8   0.000000  0.000000  0.000000  ...  2.995732  0.000000  0.000000  0.000000   \n",
            "9   0.000000  0.000000  1.609438  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "10  0.000000  0.000000  1.609438  ...  0.000000  2.995732  0.000000  0.000000   \n",
            "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "12  2.995732  0.000000  0.000000  ...  0.000000  0.000000  2.995732  0.000000   \n",
            "13  0.000000  2.995732  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "14  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  2.995732   \n",
            "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "17  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "19  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "       peter     wears     gonna     right     women      dish  \n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4   2.995732  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.000000  2.995732  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  2.995732  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000  2.995732  \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.000000  0.000000  2.995732  0.000000  \n",
            "17  0.000000  2.995732  0.000000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[20 rows x 82 columns]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Let's go through all sentences and words and create a counter of how many times a word has appeared across documents\n",
        "idf = {}\n",
        "\n",
        "# We pick the array of all sentences that have stop words removed [][]\n",
        "for words_arr in all_sentences_stop_removed:\n",
        "    unique_words_in_sentence = set(words_arr)\n",
        "\n",
        "    # for each of these unique words, increment the value in the idf array\n",
        "    for word in unique_words_in_sentence:\n",
        "        if word in idf :\n",
        "            idf[word] =  idf[word] + 1\n",
        "        else:\n",
        "            idf[word] = 1\n",
        "\n",
        "number_of_documents = len(all_sentences_stop_removed)\n",
        "\n",
        "\n",
        "\n",
        "# Create a frequency matrix\n",
        "# Each row corresponds to each sentence\n",
        "# Each colunm corresponds to one unique word in the corpus of unique words\n",
        "frequency_matrix = []\n",
        "\n",
        "for sentence in all_sentences_stop_removed:\n",
        "    word_count = Counter(sentence)\n",
        "    frequency_matrix.append([word_count.get(word, 0) * (math.log(number_of_documents / idf[word]) ) for word in unique_words])\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(frequency_matrix, columns=list(unique_words))\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f60b4a-7c80-4eab-93b4-b238ef71b2cd",
      "metadata": {
        "id": "69f60b4a-7c80-4eab-93b4-b238ef71b2cd"
      },
      "source": [
        "We notice that the text embeddings are completely different in this case. However the calculation for the similarity still remains the same. We can use cosine similarity from the values present in this dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554ca5ab-3e06-47ba-bb40-8bd050146f0d",
      "metadata": {
        "id": "554ca5ab-3e06-47ba-bb40-8bd050146f0d",
        "outputId": "c14ae437-ecfa-4cfc-9639-7ea7f7af223b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With great power comes great responsibility.\n",
            "Remember, with great power comes great responsibility.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8724395002378096"
            ]
          },
          "execution_count": 271,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(all_sentences[0])\n",
        "print(all_sentences[3])\n",
        "consine(df.loc[0], df.loc[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297c1d95-2fc4-47a6-9b8e-4d31000fae3e",
      "metadata": {
        "id": "297c1d95-2fc4-47a6-9b8e-4d31000fae3e"
      },
      "source": [
        "Initially using bag of words, we got 0.9354143466934852\n",
        "\n",
        "Now with TF-IDF we get 0.8724395002378096\n",
        "\n",
        "Let's write it into it's own function so that we can compare different methods in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f03fe0-1fda-4cd8-a6af-03ff9d08d2cc",
      "metadata": {
        "id": "a8f03fe0-1fda-4cd8-a6af-03ff9d08d2cc"
      },
      "outputs": [],
      "source": [
        "def findSimilarityTfIdf(sentence1, sentence2):\n",
        "    # combining sentences to an array\n",
        "    sentences = [sentence1, sentence2]\n",
        "    stop_removed = []\n",
        "\n",
        "    # removing stop words and punctuation from the words\n",
        "    for sentence in sentences:\n",
        "        stop_removed.append(remove_stop_words(sentence).split())\n",
        "\n",
        "    # getting list of unique words\n",
        "    unique_words = set()\n",
        "    for sentence in stop_removed:\n",
        "        for word in sentence:\n",
        "            unique_words.add(word)\n",
        "\n",
        "    # Let's go through all sentences and words and create a counter of how many times a word has appeared across documents\n",
        "    idf = {}\n",
        "    # We pick the array of all sentences that have stop words removed [][]\n",
        "    for words_arr in stop_removed:\n",
        "        unique_words_in_sentence = set(words_arr)\n",
        "\n",
        "        # for each of these unique words, increment the value in the idf dictionary\n",
        "        for word in unique_words_in_sentence:\n",
        "            if word in idf :\n",
        "                idf[word] =  idf[word] + 1\n",
        "            else:\n",
        "                idf[word] = 1\n",
        "\n",
        "    number_of_documents = len(stop_removed)\n",
        "\n",
        "    # Create a frequency matrix\n",
        "    # Each row corresponds to each sentence\n",
        "    # Each colunm corresponds to one unique word in the corpus of unique words\n",
        "    frequency_matrix = []\n",
        "\n",
        "    for sentence in stop_removed:\n",
        "        sentence_word_counter = Counter(sentence)\n",
        "        frequency_matrix.append([sentence_word_counter.get(word, 0) * (math.log(number_of_documents / idf[word]) ) for word in unique_words])\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(frequency_matrix, columns=list(unique_words))\n",
        "    print(f\"Similarity score: {consine(df.loc[0], df.loc[1])}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d3b116-640f-47d1-b081-b394a55a99f6",
      "metadata": {
        "id": "16d3b116-640f-47d1-b081-b394a55a99f6"
      },
      "outputs": [],
      "source": [
        "def compareMethods(sentence1, sentence2, showDf):\n",
        "    print(f\"Sentence 1: {sentence1}\")\n",
        "    print()\n",
        "    print(f\"Sentence 2: {sentence2}\")\n",
        "    print()\n",
        "    print(\"Bag of words\")\n",
        "    bowDf = findSimilarityBagOfWords(sentence1, sentence2)\n",
        "    if(showDf):\n",
        "        print(bowDf)\n",
        "    print()\n",
        "    print(\"TF-IDF\")\n",
        "    tfIdfDf = findSimilarityTfIdf(sentence1, sentence2)\n",
        "    if(showDf):\n",
        "        print(tfIdfDf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6364279-bc20-4586-8ee8-a40720c16039",
      "metadata": {
        "id": "b6364279-bc20-4586-8ee8-a40720c16039",
        "outputId": "14b5286d-c5d4-40c9-9676-dfec3356bace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: Who let the dogs out!\n",
            "Sentence 2: Who let the cats out!\n",
            "\n",
            "Bag of words\n",
            "Similarity score: 0.6666666666666667\n",
            "   out  dogs  let  cats\n",
            "0    1     1    1     0\n",
            "1    1     0    1     1\n",
            "\n",
            "TF-IDF\n",
            "Similarity score: 0.0\n",
            "   out      dogs  let      cats\n",
            "0  0.0  0.693147  0.0  0.000000\n",
            "1  0.0  0.000000  0.0  0.693147\n"
          ]
        }
      ],
      "source": [
        "compareMethods(\"Who let the dogs out!\", \"Who let the cats out!\", True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e133c276-4b42-47ab-9fa3-9e61913f4d11",
      "metadata": {
        "id": "e133c276-4b42-47ab-9fa3-9e61913f4d11"
      },
      "source": [
        "## Insights\n",
        "\n",
        "We took two sentences\n",
        "- Who let the dogs out!\n",
        "- Who let the cats out!\n",
        "\n",
        "And we have a look at the word embeddings that were created using the two methods:\n",
        "\n",
        "TF-IDF:\n",
        "```\n",
        "   out      dogs  let      cats\n",
        "0  0.0  0.693147  0.0  0.000000\n",
        "1  0.0  0.000000  0.0  0.693147\n",
        "```\n",
        "\n",
        "Bag of words:\n",
        "```\n",
        "   out  dogs  let  cats\n",
        "0    1     1    1     0\n",
        "1    1     0    1     1\n",
        "```\n",
        "\n",
        "We notice that while TF-IDF heavily penalizes the common words such as out and let in the corpus, those words are given the same priority as the rare words when we use the bag of words method. This way we can get to understand how TF IDF works better than Bag of words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eefe23d-a2f7-4e33-9ef3-0125694f6687",
      "metadata": {
        "id": "0eefe23d-a2f7-4e33-9ef3-0125694f6687"
      },
      "source": [
        "## Semantic understanding\n",
        "\n",
        "While the above two methods are able to statistically capture the similarities amongst the words, they fail at instances where we need to capture the semantic understanding of the sentences and then find similar ones.\n",
        "\n",
        "For example let's look at the following sentences where we praise two leaders of the world.\n",
        "\n",
        "Barack Obama: \"Barack Obama's eloquence and unwavering commitment to social justice and equality have inspired millions around the globe, making him a beacon of hope and progressive change.\"\n",
        "\n",
        "Angela Merkel: \"Angela Merkel's steadfast leadership and pragmatic approach to governance have earned her immense respect, as she navigated Germany through numerous crises with remarkable poise and integrity.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed212843-d4fc-4359-a9ca-45868dcacb4a",
      "metadata": {
        "id": "ed212843-d4fc-4359-a9ca-45868dcacb4a",
        "outputId": "6df9cdaf-5220-43de-d387-279dbb3c46d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: Barack Obama's eloquence and unwavering commitment to social justice and equality have inspired millions around the globe, making him a beacon of hope and progressive change.\n",
            "\n",
            "Sentence 2: Angela Merkel's steadfast leadership and pragmatic approach to governance have earned her immense respect, as she navigated Germany through numerous crises with remarkable poise and integrity.\n",
            "\n",
            "Bag of words\n",
            "Similarity score: 0.0\n",
            "\n",
            "TF-IDF\n",
            "Similarity score: 0.0\n"
          ]
        }
      ],
      "source": [
        "sentence1 = \"Barack Obama's eloquence and unwavering commitment to social justice and equality have inspired millions around the globe, making him a beacon of hope and progressive change.\"\n",
        "sentence2 = \"Angela Merkel's steadfast leadership and pragmatic approach to governance have earned her immense respect, as she navigated Germany through numerous crises with remarkable poise and integrity.\"\n",
        "compareMethods(sentence1, sentence2, showDf=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b1acca-028a-4515-87a6-29592515e2ba",
      "metadata": {
        "id": "87b1acca-028a-4515-87a6-29592515e2ba"
      },
      "source": [
        "> This is crazy !! Both these sentences are about world leaders and in praise of them. This shows how Bag of words and Tf IDf are pretty rudimentary techniques objectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b601cc9c-808a-457f-b63f-65b734e3c6e3",
      "metadata": {
        "id": "b601cc9c-808a-457f-b63f-65b734e3c6e3"
      },
      "source": [
        "## Other Traditional Methods for Sentence Similarity\n",
        "\n",
        "Before diving into neural network-based methods for calculating sentence similarity, it is beneficial to explore several traditional techniques. These methods are straightforward to implement and provide a solid foundation for understanding text similarity. Here are some notable traditional methods:\n",
        "\n",
        "### 1. Jaccard Similarity\n",
        "Jaccard similarity measures the similarity between two sets by comparing the size of their intersection to the size of their union. It is useful for comparing text based on the presence or absence of terms.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    return len(intersection) / len(union)\n",
        "```\n",
        "\n",
        "### 2. Levenshtein Distance (Edit Distance)\n",
        "Levenshtein distance calculates the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into another. It measures the similarity between two strings.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "import Levenshtein\n",
        "\n",
        "levenshtein_distance = Levenshtein.distance(sentence1, sentence2)\n",
        "```\n",
        "\n",
        "### 3. Overlap Coefficient\n",
        "The overlap coefficient measures the overlap between two sets relative to the smaller set. It is particularly useful when comparing sets of different sizes.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def overlap_coefficient(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    return len(intersection) / min(len(set1), len(set2))\n",
        "```\n",
        "\n",
        "### 4. Dice Coefficient\n",
        "The Dice coefficient measures the similarity between two sets based on the ratio of twice the size of the intersection to the sum of the sizes of the sets.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def dice_coefficient(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    return 2 * len(intersection) / (len(set1) + len(set2))\n",
        "```\n",
        "\n",
        "### Summary\n",
        "These traditional methods offer various ways to measure the similarity between sentences without requiring complex model training. Each method has its strengths and weaknesses, depending on the specific use case and the nature of the text data. Exploring these techniques provides a solid foundation before moving on to more advanced, neural network-based methods for calculating sentence similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a695eb3-d443-4f60-8de7-fbced8b17f34",
      "metadata": {
        "id": "2a695eb3-d443-4f60-8de7-fbced8b17f34"
      },
      "source": [
        "## Applying a third technique for assignment completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefaa5b8-23a4-4a81-be32-39e93c70d14e",
      "metadata": {
        "id": "eefaa5b8-23a4-4a81-be32-39e93c70d14e"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    return len(intersection) / len(union)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c9bb2c-52ad-4e3a-835c-3a413440792c",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "03c9bb2c-52ad-4e3a-835c-3a413440792c"
      },
      "outputs": [],
      "source": [
        "def overlap_coefficient(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    return len(intersection) / min(len(set1), len(set2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1824a0d7-b70f-4fa5-ad2d-014aca64a57d",
      "metadata": {
        "id": "1824a0d7-b70f-4fa5-ad2d-014aca64a57d"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    return 2 * len(intersection) / (len(set1) + len(set2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f03b1d33-a599-4ce4-aca3-42d674fd5a6f",
      "metadata": {
        "id": "f03b1d33-a599-4ce4-aca3-42d674fd5a6f"
      },
      "outputs": [],
      "source": [
        "# Comparing all traditional methods\n",
        "def compare_other_traditional_methods(sentence1, sentence2):\n",
        "    setSentence1 = set(sentence1.split())\n",
        "    setSentence2 = set(sentence2.split())\n",
        "\n",
        "    print(f\"Jaccard similarity for sentence 1 and sentence 2 : {jaccard_similarity(setSentence1, setSentence2)}\")\n",
        "    print()\n",
        "    print(f\"Overlap Coefficient for sentence 1 and sentence 2 : {overlap_coefficient(setSentence1, setSentence2)}\")\n",
        "    print()\n",
        "    print(f\"Dice Coefficient for sentence 1 and sentence 2 : {dice_coefficient(setSentence1, setSentence2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54fd4299-2e40-46df-abd7-957a0b574a3f",
      "metadata": {
        "id": "54fd4299-2e40-46df-abd7-957a0b574a3f"
      },
      "outputs": [],
      "source": [
        "# Example 1:\n",
        "sentence1 = \"the cat is on the mat\"\n",
        "sentence2 = \"the mat is under the cat\"\n",
        "# Example 2\n",
        "sentence3 = \"The quick brown fox jumped over the lazy dog\"\n",
        "sentence4 = \"The fast brown fox jumped over the sleepy dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16514750-8b58-4bc7-be58-472017a28e32",
      "metadata": {
        "id": "16514750-8b58-4bc7-be58-472017a28e32",
        "outputId": "2d8ca9a9-b3c4-4567-a6c7-4c5747315656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jaccard similarity for sentence 1 and sentence 2 : 0.6666666666666666\n",
            "\n",
            "Overlap Coefficient for sentence 1 and sentence 2 : 0.8\n",
            "\n",
            "Dice Coefficient for sentence 1 and sentence 2 : 0.8\n"
          ]
        }
      ],
      "source": [
        "compare_other_traditional_methods(sentence1, sentence2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40abc245-fbc7-48a0-ae24-cb463ff0b569",
      "metadata": {
        "id": "40abc245-fbc7-48a0-ae24-cb463ff0b569"
      },
      "source": [
        "# Further reading\n",
        "\n",
        "We got a good gist of how similarity search works and how it can be implemented using traditional techniques. Next we will refer to the following resources and implement advanced techniques that help us to capture the theme, context and semantic meaning of the sentences while embedding them. These form the basis of applications like search engines, language translation and chatbots.\n",
        "\n",
        "## Reference Links:\n",
        "- [Word embeddings: Helping computers understand language semantics](https://medium.com/@igniobydigitate/word-embeddings-helping-computers-understand-language-semantics-dd3456b1f700)\n",
        "- [word_vectors_game_of_thrones-LIVE- GitHub Notebook](https://github.com/llSourcell/word_vectors_game_of_thrones-LIVE/blob/master/Thrones2Vec.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does ChatGPT understand words ?\n",
        "\n",
        "When dealing with advanced AI models like ChatGPT, it's important to understand that these models don't inherently grasp the concepts of meaning, grammar, or emotions. Instead, their functionality is rooted in recognizing and replicating patterns found within the vast amounts of text they were trained on during the model training phase. Essentially, these models operate as sophisticated pattern recognizers. For example, if ChatGPT were trained solely on texts with grammatical errors, it would likely produce outputs that mirror those same errors, because that's the pattern it recognizes as correct.\n",
        "\n",
        "At the core of such models lies a mathematical framework—think of it as a complex equation with numerous variables, which we refer to as weights. During training, these weights are adjusted to capture the nuances and patterns of the language in the training corpus. However, to process language, which is inherently non-numerical, AI models like ChatGPT need to convert text into a form they can understand: numbers. This is done by transforming words into vectors, numerical representations that can be processed by the model. This transformation is where word embeddings come into play. Word embeddings are a powerful tool for translating the textual information into a numerical form that preserves the underlying meanings and relationships, setting the stage for how AI models process and generate language.\n",
        "\n",
        "## What are Word Embeddings ?\n",
        "\n",
        "Word embeddings are a method for representing words in a numerical format, essentially providing a mathematical depiction of their meanings. These embeddings convert words into vectors of real-valued numbers, where each dimension within the vector corresponds to a specific semantic feature or aspect of the word. For instance, one dimension of the vector might signify the word’s gender association, while another could relate to its tense. The numerical values within these vectors reflect the strength of the association between the word and each respective feature.\n",
        "\n",
        "Think of word embeddings as a computer's dictionary. Just as we refer to a dictionary to understand the meanings of words, a computer utilizes word embeddings to retrieve the numerical vector representation of words. This allows the AI model to handle and process language data efficiently and meaningfully. Next, let’s explore how these word embeddings are calculated and how they contribute to the model's ability to interpret and generate human-like text.\n",
        "\n",
        "### Example 1: Similarity\n",
        "Word embeddings can capture semantic similarity by positioning similar words close together in the vector space. For instance:\n",
        "\n",
        "- **Word:** \"king\"\n",
        "- **Similar Words:** \"queen\", \"monarch\", \"royalty\"\n",
        "\n",
        "The embeddings for these words will be closer in the vector space, indicating their relatedness in terms of meaning and usage.\n",
        "\n",
        "### Example 2: Relationships\n",
        "Embeddings can also capture relationships between words, often exemplified by the famous example of vector arithmetic:\n",
        "\n",
        "- **Calculation:** vector(\"king\") - vector(\"man\") + vector(\"woman\")\n",
        "- **Result:** vector close to \"queen\"\n",
        "\n",
        "This demonstrates how embeddings can encode certain relationships and analogies, providing insights into linguistic structures.\n",
        "\n",
        "### Example 3: Contextual Features\n",
        "Different dimensions of a word's vector can represent various aspects of its meaning, such as tense or part of speech:\n",
        "\n",
        "- **Word:** \"run\"\n",
        "  - **Tense:** Past, Present, Future\n",
        "  - **Form:** Verb, Noun (as in \"a long run\")\n",
        "\n",
        "The embeddings will slightly vary depending on how \"run\" is used contextually, highlighting its different grammatical and semantic properties.\n",
        "\n",
        "### Example 4: Synonyms and Antonyms\n",
        "Word embeddings can differentiate between synonyms and antonyms by their placement relative to one another:\n",
        "\n",
        "- **Synonyms:** \"happy\", \"joyful\", \"cheerful\"\n",
        "- **Antonyms:** \"happy\", \"sad\"\n",
        "\n",
        "The vectors for synonyms will be nearer to each other, whereas the vectors for antonyms will be positioned further apart.\n",
        "\n",
        "These examples help illustrate how word embeddings provide a nuanced and multidimensional representation of language, enabling AI models to perform a variety of tasks that require a deep understanding of word meanings and relationships.\n"
      ],
      "metadata": {
        "id": "pa5qM052CnDW"
      },
      "id": "pa5qM052CnDW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring this topic using text from the game of thrones book \"A Song of Ice and Fire\"\n",
        "\n",
        "Reference Notebook: https://github.com/llSourcell/word_vectors_game_of_thrones-LIVE/blob/master/Thrones2Vec.ipynb\n",
        "\n",
        "You can refer to the above notebook for a quick rundown, however I have tried to take a slower approach here by adding in more explanations and examples to illustrate each aspect of the process."
      ],
      "metadata": {
        "id": "hdAB0I7GWbG3"
      },
      "id": "hdAB0I7GWbG3"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/llSourcell/word_vectors_game_of_thrones-LIVE/master/data/got1.txt\"\n",
        "\n",
        "# Download the data\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the data as a text file. This is saved in the local workspace as got1.txt\n",
        "with open(\"got1.txt\", \"w\") as text_file:\n",
        "    text_file.write(response.text)"
      ],
      "metadata": {
        "id": "uvi__KMYMUHX"
      },
      "id": "uvi__KMYMUHX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "with open(\"got1.txt\", \"r\") as text_file:\n",
        "    text = text_file.read()"
      ],
      "metadata": {
        "id": "ZpbLhKS5bNO9"
      },
      "id": "ZpbLhKS5bNO9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning the raw data\n",
        "\n",
        "Explanation of the below python script:\n",
        "\n",
        "- **Library Import and Model Download:** The script begins by importing necessary modules from the `nltk` library, which is essential for natural language processing. It downloads models for sentence tokenization and accessing a list of stop words.\n",
        "\n",
        "- **Sentence Tokenization:** The text is split into individual sentences using `sent_tokenize`. This step is crucial for ensuring that contextual boundaries are respected during further processing.\n",
        "\n",
        "- **Text Cleaning Function:** The `clean_text` function is defined to clean each sentence by:\n",
        "  - **Removing Punctuation:** This reduces noise and prevents the model from treating punctuation as part of the word.\n",
        "  - **Lowercasing Text:** Standardizes the text to avoid distinguishing the same words based on case.\n",
        "  - **Removing Stop Words:** Filters out common but semantically weak words like \"and\", \"the\", etc., using NLTK's predefined list of stop words for English. This focuses the analysis on more meaningful content.\n",
        "\n",
        "- **Applying Cleaning:** Each sentence from the `corpus` is cleaned using the `clean_text` function. This involves stripping unnecessary characters and words as per the defined rules.\n",
        "\n",
        "- **Displaying Processed Text:** Finally, the script displays the last sentence from the cleaned text. This provides a quick sample of the processed output and ensures the text is ready for further analysis or processing tasks.\n",
        "\n",
        "- **Use Case:** This preprocessing is foundational for more complex NLP tasks like sentiment analysis, topic modeling, or training machine learning models, where the quality of input data significantly impacts the output."
      ],
      "metadata": {
        "id": "TJWo0BZ9X4D4"
      },
      "id": "TJWo0BZ9X4D4"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Downloading the required models\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Setting up stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean text: remove punctuation, lowercase text, remove stop words\n",
        "def clean_text(text):\n",
        "    # Remove punctuation and convert text to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "    # Split the text into words and remove stop words\n",
        "    words = [word for word in text.split() if word not in stop_words]\n",
        "    # Join words back into a single string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Splitting the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Cleaning each sentence\n",
        "cleaned_sentences = [clean_text(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "B9J3EFEaXXMf",
        "outputId": "2b38a0a4-ab09-49a9-aff9-574053be93ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "B9J3EFEaXXMf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The original text"
      ],
      "metadata": {
        "id": "k1655VWhb4DU"
      },
      "id": "k1655VWhb4DU"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[105:115]"
      ],
      "metadata": {
        "id": "aQ0RlJA6bnBe",
        "outputId": "ce7d8dd8-b640-445e-eab7-f1c7facbc615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aQ0RlJA6bnBe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“The dragons cannot come to life.',\n",
              " 'They are carved of stone, child.',\n",
              " 'In olden days, our island was the westernmost outpost of the great Freehold of Valyria.',\n",
              " 'It was the Valyrians who raised this citadel, and they had ways of shaping stone since lost to us.',\n",
              " 'A castle must have towers wherever two walls meet at an angle, for defense.',\n",
              " 'The Valyrians fashioned these towers in the shape of dragons to make their fortress seem more fearsome, just as they crowned their walls with a thousand gargoyles instead of simple crenellations.” He took her small pink hand in his own frail spotted one and gave it a gentle squeeze.',\n",
              " '“So you see, there is nothing to fear.”\\n\\nShireen was unconvinced.',\n",
              " '“What about the thing in the sky?',\n",
              " 'Dalla and Matrice were talking by the well, and Dalla said she heard the red woman tell Mother that it was dragonsbreath.',\n",
              " 'If the dragons are breathing, doesn’t that mean they are coming to life?”\\n\\nThe red woman, Maester Cressen thought sourly.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaned Text"
      ],
      "metadata": {
        "id": "NRE20kxVb7yO"
      },
      "id": "NRE20kxVb7yO"
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences[105:115]"
      ],
      "metadata": {
        "id": "pnIxo4A-bbJb",
        "outputId": "92d39091-2d2c-4558-9101-2cb598c306a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pnIxo4A-bbJb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dragons cannot come life',\n",
              " 'carved stone child',\n",
              " 'olden days island westernmost outpost great freehold valyria',\n",
              " 'valyrians raised citadel ways shaping stone since lost us',\n",
              " 'castle must towers wherever two walls meet angle defense',\n",
              " 'valyrians fashioned towers shape dragons make fortress seem fearsome crowned walls thousand gargoyles instead simple crenellations took small pink hand frail spotted one gave gentle squeeze',\n",
              " 'see nothing fear shireen unconvinced',\n",
              " 'thing sky',\n",
              " 'dalla matrice talking well dalla said heard red woman tell mother dragonsbreath',\n",
              " 'dragons breathing doesnt mean coming life red woman maester cressen thought sourly']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Word2Vec\n",
        "\n",
        "Word2Vec is a popular machine learning technique in natural language processing (NLP) that transforms words into vectors. By representing words in a multi-dimensional vector space, Word2Vec captures the semantic relationships between words based on their contextual usage.\n",
        "\n",
        "This allows the model to predict similar words, identify analogies, and even perform arithmetic operations with words\n",
        "\n",
        " (like \"king\" - \"man\" + \"woman\" = \"queen\").\n",
        "\n",
        " We use Word2Vec because it effectively captures word associations from a large corpus of text, which can significantly enhance various applications such as recommendation systems, search engines, and text analysis.\n",
        "\n",
        " The gensim library's Word2Vec model is used for training."
      ],
      "metadata": {
        "id": "geAP69B3fSav"
      },
      "id": "geAP69B3fSav"
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "t9S2SVM7fhDo"
      },
      "id": "t9S2SVM7fhDo",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenizing each sentence into words: This step converts each cleaned sentence\n",
        "# into a list of words.\n",
        "# Word2Vec requires input as a list of word lists.\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in cleaned_sentences]\n",
        "tokenized_sentences[115]"
      ],
      "metadata": {
        "id": "wpyYuZMgfvIN",
        "outputId": "81e853b3-2ef7-4014-b226-43f22e62eb8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wpyYuZMgfvIN",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ill',\n",
              " 'enough',\n",
              " 'shes',\n",
              " 'filled',\n",
              " 'head',\n",
              " 'mother',\n",
              " 'madness',\n",
              " 'must',\n",
              " 'poison',\n",
              " 'daughters',\n",
              " 'dreams',\n",
              " 'well']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Word2Vec model"
      ],
      "metadata": {
        "id": "SUV_yiM2hO_b"
      },
      "id": "SUV_yiM2hO_b"
    },
    {
      "cell_type": "code",
      "source": [
        "# This step involves setting up the model\n",
        "# with the desired parameters.\n",
        "\n",
        "# vector_size: The number of \"dimensions\" of the embeddings.\n",
        "#              Check the paragraph below to understand what dimensions mean\n",
        "# window:      The maximum distance between a target word and words around the\n",
        "#              target word.\n",
        "# min_count:   The minimum occurrence of words to consider when training the model;\n",
        "#              words with an occurrence less than this count will be ignored.\n",
        "#              Here we say that the word has to appear at least once\n",
        "# workers:     The number of worker threads to use in training the model for\n",
        "#              parallelization.\n",
        "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "ur5ekh6lf8fu"
      },
      "id": "ur5ekh6lf8fu",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensions\n",
        "\n",
        "In Word2Vec and similar word embedding models, \"dimensions\" refer to the number of features in each word vector. Here's a concise explanation:\n",
        "\n",
        "1. **Semantic Features**: Each dimension represents different semantic or grammatical aspects of a word, although these specific features are not explicitly labeled.\n",
        "\n",
        "2. **Vector Space**: Words with similar meanings are positioned closer together in this multi-dimensional space, facilitating tasks like synonym identification or analogy solving.\n",
        "\n",
        "3. **Dimensionality Choice**: Common sizes for vectors are 100, 200, or 300 dimensions. More dimensions can capture finer semantic details but require more data and computational resources. Fewer dimensions may not adequately represent complex word relationships.\n",
        "\n",
        "4. **Importance**: The number of dimensions determines the richness of the semantic information encoded. Optimal dimensionality balances detailed representation with computational efficiency and data availability."
      ],
      "metadata": {
        "id": "5NEV_CjEgpBL"
      },
      "id": "5NEV_CjEgpBL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "K-jr1ZZri13V"
      },
      "id": "K-jr1ZZri13V"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Word2Vec model: This step involves building the vocabulary and\n",
        "# training the model on the tokenized sentences.\n",
        "model.train(tokenized_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "id": "NzkXdjxkgUNI",
        "outputId": "ad9ca068-9f36-4b00-e4b7-b14913a5a88f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NzkXdjxkgUNI",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(819115, 845200)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of training the Word2Vec model, `corpus_count` and `epochs` are key parameters used during the model training process. Let’s delve into what each of these terms means and where they are defined:\n",
        "\n",
        "### `corpus_count`\n",
        "- **Definition**: `corpus_count` is a property of the Word2Vec model that indicates the total number of sentences or documents in the dataset used for training the model.\n",
        "- **Where It’s Defined**: It is automatically set by the Word2Vec model when you first pass your training data (in this case, `tokenized_sentences`) to it. This occurs during the initialization or any preprocessing stage where the model is made aware of the data structure and size. For instance, when you create a new Word2Vec instance and provide your tokenized sentences, it calculates how many sentences (or documents) are in your data.\n",
        "\n",
        "### `epochs`\n",
        "- **Definition**: `epochs` refers to the number of iterations over the entire dataset that the model will perform during training. Each epoch involves passing through the entire dataset once.\n",
        "- **Where It’s Defined**: The number of epochs can be defined manually when setting up the model, or you can use the default setting provided by the Word2Vec implementation in the gensim library. In gensim's Word2Vec, if not specified, the default number of epochs is often set based on the size of your data and other training parameters. You can adjust this to increase or decrease based on how well your model is learning or to avoid overfitting.\n",
        "\n",
        "### Example of Usage in Training\n",
        "- **`total_examples=model.corpus_count`**: This tells the model how many training examples (in this case, sentences) are there in total. It uses this number to regulate the training process, particularly how it adjusts learning rates and handles batching of data.\n",
        "- **`epochs=model.epochs`**: This sets how many times the model will go through the entire dataset. Each pass allows the model to refine its understanding and adjustment of the word vectors.\n",
        "\n",
        "### Why They Matter\n",
        "- **Optimizing Training**: Properly setting `corpus_count` and `epochs` ensures that the model trains effectively, using all available data and iterating through it the appropriate number of times to capture the complexities of the language represented in the corpus.\n",
        "- **Balancing Efficiency and Performance**: Adjusting epochs and accurately defining the corpus count helps balance training efficiency (in terms of computational resources) and performance (in terms of model accuracy and quality of the embeddings).\n",
        "\n",
        "These parameters are crucial for controlling the training process and directly impact the quality of the word vectors that the model produces. Adjusting them based on your specific dataset and needs can lead to better model performance."
      ],
      "metadata": {
        "id": "RMHLP5XFiz2y"
      },
      "id": "RMHLP5XFiz2y"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Saving the trained model: This allows us to use the model later without retraining.\n",
        "# model.save(\"word2vec.model\")\n",
        "\n",
        "# # Loading the model (optional, for demonstration here)\n",
        "# model = Word2Vec.load(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "6rruHrhsixZc"
      },
      "id": "6rruHrhsixZc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the model to get the vector of a word (for example, 'dragons')\n",
        "vector = model.wv['dragons']  # Get the vector for the word 'dragons'\n",
        "\n",
        "# Printing the vector to see its dimensions and values\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "ACBedtIxjYQv",
        "outputId": "8bc7a3c2-1910-4d13-d699-54572613e8af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ACBedtIxjYQv",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.36631426  0.60955584  0.13336399  0.14445439  0.06204806 -0.7928069\n",
            "  0.2086861   0.9930074  -0.09372328 -0.3835683  -0.18408199 -0.72677\n",
            " -0.07172154  0.14044607  0.16102912 -0.45973134  0.17065719 -0.5876251\n",
            "  0.00873428 -0.89726406  0.33968502  0.11368651  0.45285192 -0.35902897\n",
            " -0.21950306  0.10265506 -0.40286672 -0.22197504 -0.3265237   0.19055262\n",
            "  0.47440296  0.09198808  0.24462108 -0.42796713 -0.35232052  0.6048444\n",
            "  0.03814029 -0.22557105 -0.13154188 -0.8310859   0.20507827 -0.48912492\n",
            " -0.07025751 -0.13140215  0.42509505 -0.23210727 -0.37323052  0.02947376\n",
            "  0.28366733  0.35020363  0.33744666 -0.5979548  -0.41649908 -0.09604066\n",
            " -0.57285804  0.26571018  0.43274835  0.19915207 -0.4815969   0.19120543\n",
            "  0.06091124  0.381497   -0.155061    0.18331501 -0.6704014   0.3859066\n",
            "  0.08402716  0.29345518 -0.65951085  0.5110535  -0.21730019  0.06260721\n",
            "  0.4299781  -0.4010384   0.5571703   0.24821563 -0.10244319  0.04410516\n",
            " -0.5278675   0.3311016  -0.11637681 -0.12379638 -0.62160444  0.8063842\n",
            "  0.04502703  0.29034486 -0.03620352  0.7027634   0.52869296  0.08379135\n",
            "  0.6839755   0.19432493  0.03141201  0.09304123  1.0764033   0.4493388\n",
            "  0.2531661  -0.5436962   0.14515343 -0.21450466]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vector obtained from the Word2Vec model for the word \"dragons\" is a numerical representation of the word in a 100-dimensional vector space. Each of the 100 values in this vector corresponds to a feature that captures some aspect of the semantic and contextual meaning of \"dragons\" based on how it appears in the training data. This vector encodes the relationships and nuances of \"dragons\" relative to other words in the dataset, allowing the model to recognize similarities, differences, and various linguistic patterns. By analyzing these values, algorithms can perform tasks like finding similar words, categorizing content, or even understanding complex language structures where \"dragons\" plays a role."
      ],
      "metadata": {
        "id": "qBa5JVjQjsEm"
      },
      "id": "qBa5JVjQjsEm"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}