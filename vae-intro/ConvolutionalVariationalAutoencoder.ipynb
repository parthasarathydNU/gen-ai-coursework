{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCVFqDonFASu9KNCIAWcdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthasarathydNU/gen-ai-coursework/blob/main/vae-intro/ConvolutionalVariationalAutoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://www.tensorflow.org/tutorials/generative/cvae#define_the_loss_function_and_the_optimizer)\n",
        "\n",
        "# Convolutional Variational Auto Encoder\n",
        "\n",
        "In this notebook we explore how to build a Variational Auto Encoder and the various concepts around it.\n",
        "\n",
        "Unlike a traditional auto encoder that maps the input vector into a latent space, the Variational Auto encoder tries to map the data into inputs for a known probability distribution such as mean and variance. This approach produces a continuous, structured latent space that is useful for image generation."
      ],
      "metadata": {
        "id": "diWzRlDZiQ-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "WXmFmxBIjVxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUfi86fjiBz2",
        "outputId": "ac6558b6-3094-4520-a563-4a771a4a11e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.25.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.1.8)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.25.2)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Collecting git+https://github.com/tensorflow/docs\n",
            "  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-t7eg3a1f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs /tmp/pip-req-build-t7eg3a1f\n",
            "  Resolved https://github.com/tensorflow/docs to commit 940d94cb568bcdd4e82402eff3403ddcf5f874f2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting astor (from tensorflow-docs==2024.5.3.31743)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.5.3.31743) (1.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.5.3.31743) (3.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.5.3.31743) (5.10.4)\n",
            "Requirement already satisfied: protobuf>=3.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.5.3.31743) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.5.3.31743) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->tensorflow-docs==2024.5.3.31743) (2.1.5)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.5.3.31743) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.5.3.31743) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.5.3.31743) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.5.3.31743) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.5.3.31743) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.5.3.31743) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.5.3.31743) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.5.3.31743) (0.18.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow-docs==2024.5.3.31743) (4.2.2)\n",
            "Building wheels for collected packages: tensorflow-docs\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-docs: filename=tensorflow_docs-2024.5.3.31743-py3-none-any.whl size=182531 sha256=ccd9f9d4d9807aae7c336d4fd61c198f48158a3208fa56d3c910064bfb03063a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qdy9p26f/wheels/86/0f/1e/3b62293c8ffd0fd5a49508e6871cdb7554abe9c62afd35ec53\n",
            "Successfully built tensorflow-docs\n",
            "Installing collected packages: astor, tensorflow-docs\n",
            "Successfully installed astor-0.8.1 tensorflow-docs-2024.5.3.31743\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-probability\n",
        "\n",
        "# to generate gifs\n",
        "!pip install imageio\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time"
      ],
      "metadata": {
        "id": "bnkifLeBjmWo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the MNIST dataset\n",
        "\n",
        "Each MNIST image is a vector of 784 integers, each between 0 - 255 and represents the intensity of a pixel."
      ],
      "metadata": {
        "id": "Lj5SW88ujrGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "pdR8bsyHj5a1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting data to a bernoulli distribution"
      ],
      "metadata": {
        "id": "gGRsJnCek2Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(images):\n",
        "  \"\"\"\n",
        "  In this function we convert the images to a binary format\n",
        "  Based on a bernoulli distribution\n",
        "  We first normalize the immage values between 0 and 1\n",
        "  Then we binarize it to 0 or 1 based on the threshold value\n",
        "  \"\"\"\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')"
      ],
      "metadata": {
        "id": "ehSSwuAak44M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hvWmonxlpj3",
        "outputId": "1fb3d588-a02c-4864-f4f1-c2034e1dbace"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "GYgDJ4xBl6s6",
        "outputId": "b7ff7717-e83b-4e13-c18e-174f6eb5380a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-1d0add19-112c-418d-969c-1c531459ae47\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-1d0add19-112c-418d-969c-1c531459ae47 button').onclick = (e) => {\n",
              "        document.querySelector('#id-1d0add19-112c-418d-969c-1c531459ae47').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-1d0add19-112c-418d-969c-1c531459ae47 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuwZXM69p-V-",
        "outputId": "c645051e-5810-4aec-fdae-b3dffa8909c4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting to binary distribution"
      ],
      "metadata": {
        "id": "TNmg2h-jrWoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)"
      ],
      "metadata": {
        "id": "GnNki_q2l2ys"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbn6RujfmGFw",
        "outputId": "b8df48cd-1b11-49e2-fba4-b20156ed56bc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqOp4_TgmJtQ",
        "outputId": "24abe2e4-a36e-4a79-e828-13f9c6ae81a1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwfdjN29px-D",
        "outputId": "2229b8c9-aadc-4693-9d5c-0c2922993f25"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshaping the images and converting the Bernoulli Distribution : WHY ?\n",
        "\n",
        "The function `preprocess_images(images)` is designed to prepare image data for use in a machine learning model, specifically by processing the images into a format that is suitable for a neural network. The function does two main operations on the input images: reshaping and binarizing. Here's a breakdown of each step:\n",
        "\n",
        "1. **Reshaping the Images:**\n",
        "   - **`images.reshape((images.shape[0], 28, 28, 1))`:** This line changes the shape of the input array `images`. The reason for this reshaping involves several factors:\n",
        "     - **Original Shape:** MNIST dataset images are typically stored in a flat array of 784 elements per image (since 28x28 pixels = 784). For use in most convolutional neural networks (CNNs), it is necessary for the input data to be in the form of matrices (or more precisely, tensors) that represent the 2D structure of the images.\n",
        "     - **Target Shape:**\n",
        "       - **`images.shape[0]`** is the number of images in the batch. This part remains dynamic, accommodating whatever batch size is being processed.\n",
        "       - **`(28, 28)`** converts each flat 784 element vector into a 28x28 matrix which represents the original 2D structure of the images.\n",
        "       - **`1`** at the end of the shape tuple adds a single channel to the images, making it compatible with the expected input format for CNNs which typically expect images to have dimensions [batch_size, height, width, channels]. For grayscale images like those in the MNIST dataset, the channel dimension is 1.\n",
        "     - **Normalization:** The division by 255 is a normalization step, converting pixel values from the range [0, 255] to [0, 1]. This step is important for neural network models as it helps in faster convergence during training by maintaining numerical stability.\n",
        "\n",
        "2. **Binarizing the Images:**\n",
        "   - **`np.where(images > .5, 1.0, 0.0).astype('float32')`:** After the images are reshaped and normalized, this line binarizes the image data:\n",
        "     - **Thresholding at 0.5:** This uses a threshold of 0.5 on the normalized pixel values. If a pixel's value is greater than 0.5, it is set to 1.0; otherwise, it is set to 0.0. This step is effectively turning the grayscale images into black and white images, where each pixel is either fully \"on\" (1.0) or fully \"off\" (0.0). The choice of 0.5 as a threshold works well in practice for images where pixel values are normalized between 0 and 1.\n",
        "     - **Type Conversion:** `.astype('float32')` converts the data type of the numpy array to `float32`, which is a common data type used in neural networks due to a good balance between precision and memory requirements.\n",
        "\n",
        "Overall, the `preprocess_images` function not only adjusts the format and shape of the image data to make it suitable for processing with convolutional neural networks but also simplifies the data representation by converting it to binary. This can lead to more efficient learning when dealing with binary or nearly-binary images such as handwritten digits, which often don't require the full range of grayscale to be accurately represented and recognized.\n",
        "\n",
        "# Why do we convert it to a bernoulli distribution ?\n",
        "\n",
        "Variational Autoencoders (VAEs) are a type of generative model that often use specific assumptions about the distribution of input data to simplify the training process and improve the model's performance. When working with image data like the MNIST dataset, these assumptions can play a crucial role. Here's why pixels are converted to a Bernoulli distribution and statistically binarized:\n",
        "\n",
        "1. **Binarization of Images:**\n",
        "   - **Data Simplification:** MNIST images, which are grayscale, contain pixel values ranging from 0 to 255. Binarizing these values (i.e., converting them to 0s and 1s) simplifies the model by reducing the complexity of the input space. Instead of modeling 256 possible intensities for each pixel, the model only needs to consider two states: on or off (pixel is white or black).\n",
        "   - **Match to Output Activation:** VAEs often use a sigmoid activation function in the output layer, which constrains the output values between 0 and 1. Binarizing the inputs to also be 0 or 1 makes it easier for the model to learn the appropriate mappings since the output directly corresponds to the probability of a pixel being on (1) or off (0).\n",
        "\n",
        "2. **Modeling Pixels with Bernoulli Distribution:**\n",
        "   - **Probabilistic Interpretation:** The Bernoulli distribution is a simple discrete distribution, which has two possible outcomes: 0 or 1. This matches the binarized nature of the input data. By modeling each pixel as a Bernoulli distributed variable, the model learns to output the probability that a given pixel should be 1 (pixel on) based on the latent space representation.\n",
        "   - **Training Objective:** In VAEs, the objective includes a reconstruction loss that measures how well the output of the decoder matches the original input. For binarized data, this is typically measured using the binary cross-entropy between the input images and the reconstructed probabilities. The cross-entropy is a natural choice for loss when dealing with probabilities output from a Bernoulli distribution, effectively encouraging the model to adjust its parameters to minimize the difference between the predicted probabilities and the actual binary values of the input pixels.\n",
        "\n",
        "3. **Statistical Efficiency and Stability:**\n",
        "   - **Reduction of Variability:** By converting pixel values to a binary format, you reduce the intra-class variability among the pixels, which can lead to more stable and efficient learning. This is because the model no longer needs to account for varying degrees of intensity within the same class of objects (digits in the case of MNIST).\n",
        "   - **Robustness:** Models trained on binary data tend to be less sensitive to small variations and noise in pixel values, which can be beneficial, especially in simpler models or when computational resources are limited.\n",
        "\n",
        "In summary, converting pixel values from grayscale to a binary format and modeling these binary values with a Bernoulli distribution simplifies the VAE's learning process, aligns with the nature of the output layer's activation function, and is suitable for the kind of reconstruction loss used (binary cross-entropy). This approach makes training more straightforward and often more effective for generating or reconstructing binary images."
      ],
      "metadata": {
        "id": "y201ScYckpG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Parameters"
      ],
      "metadata": {
        "id": "UTltkC4wqvjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBqAoNzqqyPX",
        "outputId": "4070572d-e5bf-4140-a66a-23406f5bc3fa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEalZ56zq1JB",
        "outputId": "574e1f93-682f-45a4-e012-08e8dc4a6608"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 60000\n",
        "batch_size = 32\n",
        "test_size = 10000"
      ],
      "metadata": {
        "id": "m-isEjjWqvNF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use tf.data to batch and shuffle the data"
      ],
      "metadata": {
        "id": "8aAyegu4q6zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
        "                 .shuffle(train_size).batch(batch_size))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
        "                .shuffle(test_size).batch(batch_size))"
      ],
      "metadata": {
        "id": "qi4Kk_OPq6kY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the encoder and decoder networks with tf.keras\n",
        "\n",
        "In this VAE we use two small Convolutional Neural Networks for the encoder and decoder networks. These models are referred to as inference/recognition and generation models respectively.\n",
        "\n",
        "Let `x` and `z` denote the observation and latent variable respectively in the following descriptions.\n",
        "\n",
        "## Encoder Network\n",
        "\n",
        "This defines the approximate posterior distribution `q(z|x)`, which takes as input an observation and outputs a set of parameters for specifying the conditional distribution of the latent representation `z`. In this example, simply model the distribution as a diagonal Gaussian, and the network outputs the mean and log-variance parameters of a factorized Gaussian. Output log-variance instead of the variance directly for numerical stability.\n",
        "\n",
        "> Let's break down the concepts mentioned:\n",
        "\n",
        "### Diagonal Gaussian\n",
        "In a VAE, the encoder network models the distribution of latent variables as a Gaussian (normal) distribution. The term **\"diagonal Gaussian\"** refers to the type of covariance matrix used in this Gaussian distribution. A diagonal covariance matrix means that all off-diagonal entries are zero, implying that there are no correlations between different latent variables. Each latent variable has its variance, but these variances don't interact. This simplification makes it easier to compute and less parameter-intensive than a full covariance matrix.\n",
        "\n",
        "### Mean and Log-Variance Outputs\n",
        "The encoder network does two key things:\n",
        "1. **Outputs the Mean:** This is the mean of the Gaussian distribution of the latent variables. It represents the central point around which the latent variable values are distributed, essentially capturing the \"average\" state of the latent factors for the given input.\n",
        "2. **Outputs the Log-Variance:** Instead of outputting the variance directly, the encoder outputs the logarithm of the variance. This step is crucial for numerical stability:\n",
        "   - **Avoid Negative Variance:** Variance must be non-negative, but if the network were to output variance directly, there could be computational issues or optimization difficulties that might lead to negative values, which are not valid. By using the log-variance, the network ensures that the variance is always positive, as the exponential of any real number (which is the inverse operation to logarithm) is positive.\n",
        "   - **Easier Optimization:** Variance values can span several orders of magnitude (very small to very large). Working in the log space normalizes this scale, potentially leading to smoother and more stable optimization landscapes.\n",
        "\n",
        "### Why Log-Variance and Not Variance\n",
        "Outputting the log-variance instead of the variance directly has advantages in terms of numerical stability and computational efficiency:\n",
        "- **Numerical Stability:** Calculating the variance directly can lead to numerical issues, such as floating-point underflow or overflow, especially when the variance is extremely low (close to zero) or high. Using the logarithm mitigates these risks.\n",
        "- **Ease of Optimization:** In optimization, especially with gradient descent methods, dealing with parameters that can vary over several orders of magnitude (as variances can) is challenging. Using the logarithm transforms the variance into a scale that's more manageable for the optimizer.\n",
        "\n",
        "In summary, in a VAE, the encoder network approximates the posterior distribution of the latent variables given the input data. By outputting parameters (mean and log-variance) for a diagonal Gaussian distribution, the encoder simplifies the representation of the input data into a latent space where each dimension is independent and normally distributed. This structured approach allows the VAE not only to efficiently encode variations in the data but also to handle the intricacies of probabilistic generative modeling."
      ],
      "metadata": {
        "id": "Iq12E8NQ108Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Network\n",
        "\n",
        "This defines the conditional distribution of the observation `p(x|z)`, which takes a latent sample `z` as an input and outputs the parameters for a conditional distribution of the observation. Model the latent distribution priod `p(z)` as a unit Gaussian.\n",
        "\n",
        ">  My understandign was that the decoder network was supposed to take in a distribution and output back an image. But here I see that it outputs the parameters for a distribution, isn't that the job of the encoder ? ðŸ¤”\n",
        "\n",
        "ðŸ¤– ðŸ’¬ Your understanding of the roles of the encoder and decoder in a Variational Autoencoder (VAE) is on the right track, but there's a bit more nuance to how these components interact and what they output, particularly in the case of the decoder.\n",
        "\n",
        "### Encoder vs. Decoder Roles\n",
        "\n",
        "- **Encoder Network:** The encoder network takes in the original data (like images) and transforms it into a latent space. As you noted, it outputs parameters for a probability distributionâ€”typically the mean and log-variance of a Gaussian distribution. These parameters define how we think about the latent variables given the input data, thus approximating the posterior distribution of the latent variables.\n",
        "\n",
        "- **Decoder Network:** The decoder, on the other hand, aims to reconstruct the original input data from the latent space. It takes a sample \\( z \\) from the latent spaceâ€”drawn from the distribution defined by the encoder's outputsâ€”and attempts to reconstruct the original input.\n",
        "\n",
        "### Output of the Decoder\n",
        "\n",
        "The decoder does not simply spit out a reconstructed image directly. Instead, it outputs parameters that define a probability distribution of the possible outputs. Hereâ€™s how this works and why:\n",
        "\n",
        "- **Conditional Distribution:** When the description states that the decoder outputs parameters for a \"conditional distribution of the observation,\" it means that the decoder specifies how likely certain outputs are, given the latent sample \\( z \\). The decoder is essentially modeling \\( p(x|z) \\), the probability of the data \\( x \\) given the latent variables \\( z \\).\n",
        "\n",
        "- **Probabilistic Reconstruction:** The actual output data (like an image) is then sampled from this distribution. This approach allows the model to express uncertainty about the reconstructions and to generate multiple possible outputs from the same latent variables, reflecting variations that could plausibly lead to the observed inputs.\n",
        "\n",
        "- **Example with Images:** If the input data are images, and the VAE is trained on these, the decoder might output the mean and variance for a Gaussian distribution for each pixel. Alternatively, in cases like binarized images, it might output the probability that each pixel should be 1 (if using a Bernoulli distribution, as often done with MNIST).\n",
        "\n",
        "### Why Use Distributions at Both Ends?\n",
        "\n",
        "Using distributions both at the encoder and decoder ends makes VAEs powerful for a couple of reasons:\n",
        "- **Flexibility in Data Generation:** By handling distributions rather than fixed outputs, VAEs can generate new data samples that are variations on the learned data, useful for tasks like data augmentation, anomaly detection, or generative models.\n",
        "- **Better Handling of Uncertainty:** This approach inherently allows the model to handle and express uncertainty about the data it processes and generates, a key advantage in probabilistic modeling.\n",
        "\n",
        "Thus, while it might initially seem like the job of outputting distributions should be confined to the encoder, having the decoder also output parameters for distributions enables more robust, flexible, and realistic modeling of data in various conditions.\n",
        "\n",
        "> It will be an interesting exersice for us to have a look at the outputs that were generated by the decoder\n",
        "\n"
      ],
      "metadata": {
        "id": "FZRWP2NP3mDU"
      }
    }
  ]
}